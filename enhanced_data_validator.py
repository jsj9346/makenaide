# """
# Enhanced Data Validation System for Makenaide
# í†µí•© ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ
# 
# ì£¼ìš” ê¸°ëŠ¥:
# 1. OHLCV ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# 2. Static Indicators ë¬´ê²°ì„± ê²€ì¦  
# 3. ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
# 4. ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
# 5. í¬ê´„ì ì¸ ë¦¬í¬íŠ¸ ìƒì„±
# """
# 
# import pandas as pd
# import numpy as np
# import psycopg2
# from psycopg2.extras import RealDictCursor
# import logging
# from datetime import datetime, timedelta
# from typing import Dict, List, Tuple, Optional, Any
# import json
# from dataclasses import dataclass, asdict
# from utils import get_db_connection
# 
# # ë¡œê¹… ì„¤ì •
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# 
# @dataclass
# class ValidationResult:
#     """ê²€ì¦ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
#     is_valid: bool
#     severity: str  # 'INFO', 'WARNING', 'ERROR', 'CRITICAL'
#     message: str
#     details: Dict[str, Any]
#     timestamp: datetime
#     affected_records: int = 0
#     
# class EnhancedDataValidator:
#     """ê°•í™”ëœ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ"""
#     
#     def __init__(self):
#         self.validation_results = []
#         self.critical_thresholds = {
#             'null_ratio_threshold': 0.1,  # 10% ì´ìƒ NULLê°’
#             'zero_ratio_threshold': 0.05,  # 5% ì´ìƒ 0ê°’
#             'logic_error_threshold': 0.01,  # 1% ì´ìƒ ë…¼ë¦¬ ì˜¤ë¥˜
#             'staleness_hours': 24,  # 24ì‹œê°„ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ
#             'price_anomaly_factor': 10,  # ì¼ë°˜ì  ê°€ê²© ë²”ìœ„ ë²—ì–´ë‚¨
#             'volume_anomaly_factor': 100  # ì¼ë°˜ì  ê±°ë˜ëŸ‰ ë²”ìœ„ ë²—ì–´ë‚¨
#         }
#         
#     def add_result(self, result: ValidationResult):
#         """ê²€ì¦ ê²°ê³¼ ì¶”ê°€"""
#         self.validation_results.append(result)
#         
#         # ì‹¬ê°í•œ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ë¡œê¹…
#         if result.severity in ['ERROR', 'CRITICAL']:
#             logger.error(f"âŒ {result.message}")
#         elif result.severity == 'WARNING':
#             logger.warning(f"âš ï¸ {result.message}")
#         else:
#             logger.info(f"âœ… {result.message}")
#     
#     def validate_ohlcv_data_quality(self, days: int = 7) -> List[ValidationResult]:
#         """OHLCV ë°ì´í„° í’ˆì§ˆ ì¢…í•© ê²€ì¦"""
#         logger.info(f"ğŸ” OHLCV ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
#         results = []
#         
#         try:
#             conn = get_db_connection()
#             cursor = conn.cursor(cursor_factory=RealDictCursor)
#             
#             # 1. ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘
#             cursor.execute(f"""
#                 SELECT 
#                     COUNT(*) as total_records,
#                     COUNT(DISTINCT ticker) as unique_tickers,
#                     MIN(date) as earliest_date,
#                     MAX(date) as latest_date
#                 FROM ohlcv 
#                 WHERE date >= CURRENT_DATE - INTERVAL '{days} days'
#             """)
#             basic_stats = cursor.fetchone()
#             
#             if basic_stats['total_records'] == 0:
#                 results.append(ValidationResult(
#                     is_valid=False,
#                     severity='CRITICAL',
#                     message=f"ìµœê·¼ {days}ì¼ê°„ OHLCV ë°ì´í„°ê°€ ì—†ìŒ",
#                     details=dict(basic_stats),
#                     timestamp=datetime.now()
#                 ))
#                 return results
#             
#             # 2. NULL ê°’ ê²€ì¦
#             cursor.execute(f"""
#                 SELECT 
#                     SUM(CASE WHEN open IS NULL THEN 1 ELSE 0 END) as null_open,
#                     SUM(CASE WHEN high IS NULL THEN 1 ELSE 0 END) as null_high,
#                     SUM(CASE WHEN low IS NULL THEN 1 ELSE 0 END) as null_low,
#                     SUM(CASE WHEN close IS NULL THEN 1 ELSE 0 END) as null_close,
#                     SUM(CASE WHEN volume IS NULL THEN 1 ELSE 0 END) as null_volume,
#                     COUNT(*) as total_count
#                 FROM ohlcv 
#                 WHERE date >= CURRENT_DATE - INTERVAL '{days} days'
#             """)
#             null_stats = cursor.fetchone()
#             
#             total_count = null_stats['total_count']
#             for column in ['open', 'high', 'low', 'close', 'volume']:
#                 null_count = null_stats[f'null_{column}']
#                 null_ratio = null_count / total_count if total_count > 0 else 0
#                 
#                 if null_ratio > self.critical_thresholds['null_ratio_threshold']:
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"OHLCV {column} ì»¬ëŸ¼ì— ê³¼ë„í•œ NULLê°’ ({null_ratio:.1%})",
#                         details={'column': column, 'null_count': null_count, 'null_ratio': null_ratio},
#                         timestamp=datetime.now(),
#                         affected_records=null_count
#                     ))
#                 elif null_ratio > 0:
#                     results.append(ValidationResult(
#                         is_valid=True,
#                         severity='WARNING',
#                         message=f"OHLCV {column} ì»¬ëŸ¼ì— ì†ŒëŸ‰ì˜ NULLê°’ ({null_ratio:.1%})",
#                         details={'column': column, 'null_count': null_count, 'null_ratio': null_ratio},
#                         timestamp=datetime.now(),
#                         affected_records=null_count
#                     ))
#             
#             # 3. 0ê°’ ê²€ì¦
#             cursor.execute(f"""
#                 SELECT 
#                     SUM(CASE WHEN open = 0 THEN 1 ELSE 0 END) as zero_open,
#                     SUM(CASE WHEN high = 0 THEN 1 ELSE 0 END) as zero_high,
#                     SUM(CASE WHEN low = 0 THEN 1 ELSE 0 END) as zero_low,
#                     SUM(CASE WHEN close = 0 THEN 1 ELSE 0 END) as zero_close,
#                     COUNT(*) as total_count
#                 FROM ohlcv 
#                 WHERE date >= CURRENT_DATE - INTERVAL '{days} days'
#             """)
#             zero_stats = cursor.fetchone()
#             
#             for column in ['open', 'high', 'low', 'close']:
#                 zero_count = zero_stats[f'zero_{column}']
#                 zero_ratio = zero_count / total_count if total_count > 0 else 0
#                 
#                 if zero_ratio > self.critical_thresholds['zero_ratio_threshold']:
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"OHLCV {column} ì»¬ëŸ¼ì— ê³¼ë„í•œ 0ê°’ ({zero_ratio:.1%})",
#                         details={'column': column, 'zero_count': zero_count, 'zero_ratio': zero_ratio},
#                         timestamp=datetime.now(),
#                         affected_records=zero_count
#                     ))
#                 elif zero_count > 0:
#                     results.append(ValidationResult(
#                         is_valid=True,
#                         severity='WARNING',
#                         message=f"OHLCV {column} ì»¬ëŸ¼ì— 0ê°’ ë°œê²¬ ({zero_count}ê±´)",
#                         details={'column': column, 'zero_count': zero_count, 'zero_ratio': zero_ratio},
#                         timestamp=datetime.now(),
#                         affected_records=zero_count
#                     ))
#             
#             # 4. ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦
#             cursor.execute(f"""
#                 SELECT COUNT(*) as logic_errors
#                 FROM ohlcv 
#                 WHERE date >= CURRENT_DATE - INTERVAL '{days} days'
#                   AND (high < low OR close > high OR close < low OR open > high OR open < low)
#             """)
#             logic_errors = cursor.fetchone()['logic_errors']
#             logic_error_ratio = logic_errors / total_count if total_count > 0 else 0
#             
#             if logic_error_ratio > self.critical_thresholds['logic_error_threshold']:
#                 results.append(ValidationResult(
#                     is_valid=False,
#                     severity='ERROR',
#                     message=f"OHLCV ê°€ê²© ë…¼ë¦¬ ì˜¤ë¥˜ ê³¼ë‹¤ ({logic_error_ratio:.1%})",
#                     details={'logic_errors': logic_errors, 'error_ratio': logic_error_ratio},
#                     timestamp=datetime.now(),
#                     affected_records=logic_errors
#                 ))
#             elif logic_errors > 0:
#                 results.append(ValidationResult(
#                     is_valid=True,
#                     severity='WARNING',
#                     message=f"OHLCV ê°€ê²© ë…¼ë¦¬ ì˜¤ë¥˜ ë°œê²¬ ({logic_errors}ê±´)",
#                     details={'logic_errors': logic_errors, 'error_ratio': logic_error_ratio},
#                     timestamp=datetime.now(),
#                     affected_records=logic_errors
#                 ))
#             
#             # 5. ê°€ê²© ì´ìƒê°’ ê²€ì¦
#             cursor.execute(f"""
#                 SELECT ticker, date, close
#                 FROM ohlcv 
#                 WHERE date >= CURRENT_DATE - INTERVAL '{days} days'
#                   AND (close > 100000000 OR close < 0.00000001)
#                 ORDER BY close DESC
#                 LIMIT 10
#             """)
#             extreme_prices = cursor.fetchall()
#             
#             if extreme_prices:
#                 results.append(ValidationResult(
#                     is_valid=True,
#                     severity='WARNING',
#                     message=f"ê·¹ê°’ ê°€ê²© ë°ì´í„° ë°œê²¬ ({len(extreme_prices)}ê±´)",
#                     details={'extreme_prices': [dict(row) for row in extreme_prices]},
#                     timestamp=datetime.now(),
#                     affected_records=len(extreme_prices)
#                 ))
#             
#             # 6. ë°ì´í„° ìµœì‹ ì„± ê²€ì¦
#             cursor.execute("SELECT MAX(date) as latest_date FROM ohlcv")
#             latest_date = cursor.fetchone()['latest_date']
#             if latest_date:
#                 staleness_days = (datetime.now().date() - latest_date).days
#                 if staleness_days > 1:
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"OHLCV ë°ì´í„°ê°€ {staleness_days}ì¼ ì§€ì—°ë¨ (ìµœì‹ : {latest_date})",
#                         details={'latest_date': str(latest_date), 'staleness_days': staleness_days},
#                         timestamp=datetime.now()
#                     ))
#             
#             cursor.close()
#             conn.close()
#             
#         except Exception as e:
#             results.append(ValidationResult(
#                 is_valid=False,
#                 severity='CRITICAL',
#                 message=f"OHLCV ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}",
#                 details={'error': str(e)},
#                 timestamp=datetime.now()
#             ))
#         
#         return results
#     
#     def validate_static_indicators_integrity(self, days: int = 7) -> List[ValidationResult]:
#         """Static Indicators ë¬´ê²°ì„± ê²€ì¦"""
#         logger.info(f"ğŸ” Static Indicators ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
#         results = []
#         
#         try:
#             conn = get_db_connection()
#             cursor = conn.cursor(cursor_factory=RealDictCursor)
#             
#             # 1. ê¸°ë³¸ í†µê³„
#             cursor.execute("""
#                 SELECT 
#                     COUNT(*) as total_records,
#                     COUNT(DISTINCT ticker) as unique_tickers,
#                     MAX(updated_at) as latest_update
#                 FROM static_indicators
#             """)
#             basic_stats = cursor.fetchone()
#             
#             if basic_stats['total_records'] == 0:
#                 results.append(ValidationResult(
#                     is_valid=False,
#                     severity='CRITICAL',
#                     message="Static indicators í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŒ",
#                     details=dict(basic_stats),
#                     timestamp=datetime.now()
#                 ))
#                 return results
#             
#             # 2. ê° ì§€í‘œë³„ ê³ ìœ ê°’ ë¶„í¬ í™•ì¸
#             critical_indicators = ['ma200_slope', 'nvt_relative', 'volume_change_7_30', 'adx', 'supertrend_signal']
#             
#             for indicator in critical_indicators:
#                 cursor.execute(f"""
#                     SELECT 
#                         COUNT(DISTINCT {indicator}) as unique_values,
#                         COUNT(*) as total_records,
#                         SUM(CASE WHEN {indicator} IS NULL THEN 1 ELSE 0 END) as null_count
#                     FROM static_indicators
#                 """)
#                 indicator_stats = cursor.fetchone()
#                 
#                 unique_values = indicator_stats['unique_values']
#                 total_records = indicator_stats['total_records']
#                 null_count = indicator_stats['null_count']
#                 
#                 # ëª¨ë“  ê°’ì´ ë™ì¼í•œ ê²½ìš° (ê³ ìœ ê°’ì´ 1ê°œ)
#                 if unique_values <= 1:
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"Static indicator '{indicator}' ëª¨ë“  ê°’ì´ ë™ì¼í•¨ (ê³ ìœ ê°’: {unique_values}ê°œ)",
#                         details={'indicator': indicator, 'unique_values': unique_values, 'total_records': total_records},
#                         timestamp=datetime.now(),
#                         affected_records=total_records
#                     ))
#                 elif unique_values < 5:  # ê³ ìœ ê°’ì´ ë„ˆë¬´ ì ì€ ê²½ìš°
#                     results.append(ValidationResult(
#                         is_valid=True,
#                         severity='WARNING',
#                         message=f"Static indicator '{indicator}' ê³ ìœ ê°’ì´ ì ìŒ ({unique_values}ê°œ)",
#                         details={'indicator': indicator, 'unique_values': unique_values, 'total_records': total_records},
#                         timestamp=datetime.now()
#                     ))
#                 
#                 # NULL ê°’ í™•ì¸
#                 null_ratio = null_count / total_records if total_records > 0 else 0
#                 if null_ratio > 0.1:  # 10% ì´ìƒ NULL
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"Static indicator '{indicator}' ê³¼ë„í•œ NULLê°’ ({null_ratio:.1%})",
#                         details={'indicator': indicator, 'null_count': null_count, 'null_ratio': null_ratio},
#                         timestamp=datetime.now(),
#                         affected_records=null_count
#                     ))
#             
#             # 3. ì—…ë°ì´íŠ¸ ìµœì‹ ì„± í™•ì¸
#             latest_update = basic_stats['latest_update']
#             if latest_update:
#                 staleness_hours = (datetime.now() - latest_update).total_seconds() / 3600
#                 if staleness_hours > self.critical_thresholds['staleness_hours']:
#                     results.append(ValidationResult(
#                         is_valid=False,
#                         severity='ERROR',
#                         message=f"Static indicators ì—…ë°ì´íŠ¸ ì§€ì—° ({staleness_hours:.1f}ì‹œê°„)",
#                         details={'latest_update': str(latest_update), 'staleness_hours': staleness_hours},
#                         timestamp=datetime.now()
#                     ))
#             
#             # 4. í‹°ì»¤ë³„ ì¼ê´€ì„± í™•ì¸
#             cursor.execute("""
#                 SELECT ticker, COUNT(*) as count
#                 FROM static_indicators
#                 GROUP BY ticker
#                 HAVING COUNT(*) > 1
#             """)
#             duplicate_tickers = cursor.fetchall()
#             
#             if duplicate_tickers:
#                 results.append(ValidationResult(
#                     is_valid=False,
#                     severity='WARNING',
#                     message=f"ì¤‘ë³µ í‹°ì»¤ ë°œê²¬ ({len(duplicate_tickers)}ê°œ)",
#                     details={'duplicate_tickers': [dict(row) for row in duplicate_tickers]},
#                     timestamp=datetime.now(),
#                     affected_records=len(duplicate_tickers)
#                 ))
#             
#             cursor.close()
#             conn.close()
#             
#         except Exception as e:
#             results.append(ValidationResult(
#                 is_valid=False,
#                 severity='CRITICAL',
#                 message=f"Static indicators ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}",
#                 details={'error': str(e)},
#                 timestamp=datetime.now()
#             ))
#         
#         return results
#     
#     def validate_data_synchronization(self) -> List[ValidationResult]:
#         """OHLCVì™€ Static Indicators ê°„ ë°ì´í„° ë™ê¸°í™” ê²€ì¦"""
#         logger.info("ğŸ” ë°ì´í„° ë™ê¸°í™” ê²€ì¦ ì‹œì‘")
#         results = []
#         
#         try:
#             conn = get_db_connection()
#             cursor = conn.cursor(cursor_factory=RealDictCursor)
#             
#             # 1. í‹°ì»¤ ì¼ì¹˜ì„± í™•ì¸
#             cursor.execute("""
#                 SELECT 
#                     (SELECT COUNT(DISTINCT ticker) FROM ohlcv) as ohlcv_tickers,
#                     (SELECT COUNT(DISTINCT ticker) FROM static_indicators) as static_tickers,
#                     (SELECT COUNT(DISTINCT o.ticker) 
#                      FROM ohlcv o 
#                      INNER JOIN static_indicators s ON o.ticker = s.ticker) as common_tickers
#             """)
#             sync_stats = cursor.fetchone()
#             
#             ohlcv_tickers = sync_stats['ohlcv_tickers']
#             static_tickers = sync_stats['static_tickers']
#             common_tickers = sync_stats['common_tickers']
#             
#             coverage_ratio = common_tickers / static_tickers if static_tickers > 0 else 0
#             
#             if coverage_ratio < 0.9:  # 90% ë¯¸ë§Œ ì»¤ë²„ë¦¬ì§€
#                 results.append(ValidationResult(
#                     is_valid=False,
#                     severity='ERROR',
#                     message=f"ë°ì´í„° ë™ê¸°í™” ë¬¸ì œ: ì»¤ë²„ë¦¬ì§€ {coverage_ratio:.1%}",
#                     details={
#                         'ohlcv_tickers': ohlcv_tickers,
#                         'static_tickers': static_tickers,
#                         'common_tickers': common_tickers,
#                         'coverage_ratio': coverage_ratio
#                     },
#                     timestamp=datetime.now()
#                 ))
#             elif coverage_ratio < 1.0:
#                 results.append(ValidationResult(
#                     is_valid=True,
#                     severity='WARNING',
#                     message=f"ì¼ë¶€ í‹°ì»¤ ë™ê¸°í™” ëˆ„ë½: ì»¤ë²„ë¦¬ì§€ {coverage_ratio:.1%}",
#                     details={
#                         'ohlcv_tickers': ohlcv_tickers,
#                         'static_tickers': static_tickers,
#                         'common_tickers': common_tickers,
#                         'coverage_ratio': coverage_ratio
#                     },
#                     timestamp=datetime.now()
#                 ))
#             else:
#                 results.append(ValidationResult(
#                     is_valid=True,
#                     severity='INFO',
#                     message="ë°ì´í„° ë™ê¸°í™” ì •ìƒ",
#                     details={
#                         'ohlcv_tickers': ohlcv_tickers,
#                         'static_tickers': static_tickers,
#                         'common_tickers': common_tickers,
#                         'coverage_ratio': coverage_ratio
#                     },
#                     timestamp=datetime.now()
#                 ))
#             
#             cursor.close()
#             conn.close()
#             
#         except Exception as e:
#             results.append(ValidationResult(
#                 is_valid=False,
#                 severity='CRITICAL',
#                 message=f"ë™ê¸°í™” ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}",
#                 details={'error': str(e)},
#                 timestamp=datetime.now()
#             ))
#         
#         return results
#     
#     def run_comprehensive_validation(self, days: int = 7) -> Dict[str, Any]:
#         """ì¢…í•© ë°ì´í„° ê²€ì¦ ì‹¤í–‰"""
#         logger.info(f"ğŸ” ì¢…í•© ë°ì´í„° ê²€ì¦ ì‹œì‘ (ìµœê·¼ {days}ì¼)")
#         
#         start_time = datetime.now()
#         self.validation_results = []
#         
#         # 1. OHLCV ë°ì´í„° í’ˆì§ˆ ê²€ì¦
#         ohlcv_results = self.validate_ohlcv_data_quality(days)
#         for result in ohlcv_results:
#             self.add_result(result)
#         
#         # 2. Static Indicators ë¬´ê²°ì„± ê²€ì¦
#         static_results = self.validate_static_indicators_integrity(days)
#         for result in static_results:
#             self.add_result(result)
#         
#         # 3. ë°ì´í„° ë™ê¸°í™” ê²€ì¦
#         sync_results = self.validate_data_synchronization()
#         for result in sync_results:
#             self.add_result(result)
#         
#         end_time = datetime.now()
#         duration = (end_time - start_time).total_seconds()
#         
#         # ê²°ê³¼ ìš”ì•½
#         summary = self._generate_summary()
#         summary['validation_duration'] = duration
#         summary['timestamp'] = start_time.isoformat()
#         
#         logger.info(f"ğŸ” ì¢…í•© ê²€ì¦ ì™„ë£Œ ({duration:.2f}ì´ˆ)")
#         
#         return summary
#     
#     def _generate_summary(self) -> Dict[str, Any]:
#         """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
#         total_results = len(self.validation_results)
#         
#         severity_counts = {
#             'INFO': 0,
#             'WARNING': 0,
#             'ERROR': 0,
#             'CRITICAL': 0
#         }
#         
#         valid_count = 0
#         total_affected_records = 0
#         
#         for result in self.validation_results:
#             severity_counts[result.severity] += 1
#             if result.is_valid:
#                 valid_count += 1
#             total_affected_records += result.affected_records
#         
#         overall_health = "HEALTHY"
#         if severity_counts['CRITICAL'] > 0:
#             overall_health = "CRITICAL"
#         elif severity_counts['ERROR'] > 0:
#             overall_health = "UNHEALTHY"
#         elif severity_counts['WARNING'] > 0:
#             overall_health = "WARNING"
#         
#         return {
#             'overall_health': overall_health,
#             'total_checks': total_results,
#             'valid_checks': valid_count,
#             'validation_rate': valid_count / total_results if total_results > 0 else 0,
#             'severity_breakdown': severity_counts,
#             'total_affected_records': total_affected_records,
#             'detailed_results': [asdict(result) for result in self.validation_results]
#         }
#     
#     def generate_report(self, include_details: bool = True) -> str:
#         """ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
#         if not self.validation_results:
#             return "ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. run_comprehensive_validation()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
#         
#         summary = self._generate_summary()
#         
#         report_lines = []
#         report_lines.append("=" * 80)
#         report_lines.append("ğŸ” Enhanced Data Validation Report")
#         report_lines.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
#         report_lines.append("=" * 80)
#         
#         # ì „ì²´ ìƒíƒœ
#         health_emoji = {
#             'HEALTHY': 'âœ…',
#             'WARNING': 'âš ï¸',
#             'UNHEALTHY': 'âŒ',
#             'CRITICAL': 'ğŸš¨'
#         }
#         
#         report_lines.append(f"\nğŸ“Š ì „ì²´ ìƒíƒœ: {health_emoji.get(summary['overall_health'], 'â“')} {summary['overall_health']}")
#         report_lines.append(f"ğŸ“ˆ ê²€ì¦ë¥ : {summary['validation_rate']:.1%} ({summary['valid_checks']}/{summary['total_checks']})")
#         report_lines.append(f"ğŸ“ ì˜í–¥ë°›ì€ ë ˆì½”ë“œ: {summary['total_affected_records']:,}ê°œ")
#         
#         # ì‹¬ê°ë„ë³„ ìš”ì•½
#         report_lines.append(f"\nğŸ“‹ ì‹¬ê°ë„ë³„ ìš”ì•½:")
#         for severity, count in summary['severity_breakdown'].items():
#             if count > 0:
#                 emoji = {'INFO': 'âœ…', 'WARNING': 'âš ï¸', 'ERROR': 'âŒ', 'CRITICAL': 'ğŸš¨'}[severity]
#                 report_lines.append(f"   {emoji} {severity}: {count}ê±´")
#         
#         # ìƒì„¸ ê²°ê³¼
#         if include_details:
#             report_lines.append(f"\nğŸ“‹ ìƒì„¸ ê²€ì¦ ê²°ê³¼:")
#             
#             for result in self.validation_results:
#                 emoji = {'INFO': 'âœ…', 'WARNING': 'âš ï¸', 'ERROR': 'âŒ', 'CRITICAL': 'ğŸš¨'}[result.severity]
#                 report_lines.append(f"\n{emoji} [{result.severity}] {result.message}")
#                 
#                 if result.affected_records > 0:
#                     report_lines.append(f"   ğŸ“Š ì˜í–¥ë°›ì€ ë ˆì½”ë“œ: {result.affected_records:,}ê°œ")
#                 
#                 if result.details:
#                     key_details = []
#                     for key, value in result.details.items():
#                         if key not in ['error'] and not isinstance(value, (list, dict)):
#                             key_details.append(f"{key}={value}")
#                     if key_details:
#                         report_lines.append(f"   ğŸ“ ì„¸ë¶€ì‚¬í•­: {', '.join(key_details)}")
#         
#         # ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­
#         report_lines.append(f"\nğŸ”§ ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­:")
#         
#         if summary['severity_breakdown']['CRITICAL'] > 0:
#             report_lines.append("   ğŸš¨ CRITICAL ì´ìŠˆ ì¦‰ì‹œ í•´ê²° í•„ìš”")
#         if summary['severity_breakdown']['ERROR'] > 0:
#             report_lines.append("   âŒ ERROR ì´ìŠˆ ìš°ì„  í•´ê²° í•„ìš”")
#         if summary['severity_breakdown']['WARNING'] > 0:
#             report_lines.append("   âš ï¸ WARNING ì´ìŠˆ ê²€í†  ë° ê°œì„  ê³ ë ¤")
#         if summary['overall_health'] == 'HEALTHY':
#             report_lines.append("   âœ… ëª¨ë“  ê²€ì¦ í†µê³¼ - ì •ê¸° ëª¨ë‹ˆí„°ë§ ì§€ì†")
#         
#         report_lines.append("=" * 80)
#         
#         return "\n".join(report_lines)
#     
#     def save_report(self, filename: Optional[str] = None) -> str:
#         """ê²€ì¦ ë³´ê³ ì„œ íŒŒì¼ë¡œ ì €ì¥"""
#         if filename is None:
#             filename = f"data_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
#         
#         report = self.generate_report()
#         
#         with open(filename, 'w', encoding='utf-8') as f:
#             f.write(report)
#         
#         logger.info(f"ğŸ“ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {filename}")
#         return filename
# 
# def main():
#     """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
#     validator = EnhancedDataValidator()
#     
#     # ì¢…í•© ê²€ì¦ ì‹¤í–‰
#     results = validator.run_comprehensive_validation(days=7)
#     
#     # ë¦¬í¬íŠ¸ ìƒì„± ë° ì €ì¥
#     report = validator.generate_report(include_details=True)
#     filename = validator.save_report()
#     
#     print(f"âœ… ê²€ì¦ ì™„ë£Œ: {filename}")
#     print(f"ğŸ“Š ê²€ì¦ ê²°ê³¼: {results['summary']}")
# 
# 
# if __name__ == "__main__":
#     main()
