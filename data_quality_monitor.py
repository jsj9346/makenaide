"""
ğŸ“Š ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (data_quality_monitor.py)

Makenaide í”„ë¡œì íŠ¸ì˜ ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ í’ˆì§ˆ ì§€í‘œ ì¶”ì 
2. ë™ì¼ê°’ íŒ¨í„´ íƒì§€ ë° ì•Œë¦¼
3. ë°ì´í„° ì´ìƒê°’ ê°ì§€
4. í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ìƒì„±
5. ìë™ ì•Œë¦¼ ë° ë³´ê³ ì„œ

ì‘ì„±ì: Makenaide Development Team
ì‘ì„±ì¼: 2025-01-27
ë²„ì „: 1.0.0
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import json
import os
from collections import defaultdict, deque
import threading
import time
from utils import setup_logger, get_db_connection

# ë¡œê±° ì„¤ì •
logger = setup_logger()

class DataQualityMonitor:
    """
    ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    
    ì£¼ìš” ëª¨ë‹ˆí„°ë§ í•­ëª©:
    1. ë™ì¼ê°’ ê²€ì¶œë¥ 
    2. ë°ì´í„° ì™„ì„±ë„
    3. ê°’ ë¶„í¬ ì´ìƒ íƒì§€
    4. ì‹œê°„ë³„ í’ˆì§ˆ ë³€í™”
    5. í‹°ì»¤ë³„ í’ˆì§ˆ ì ìˆ˜
    """
    
    def __init__(self, monitor_interval: int = 300):  # 5ë¶„ ê°„ê²©
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.monitor_interval = monitor_interval
        self.is_running = False
        self.monitor_thread = None
        
        # í’ˆì§ˆ ì§€í‘œ ì €ì¥ì†Œ
        self.quality_metrics = {
            'static_indicators': defaultdict(lambda: deque(maxlen=100)),
            'ohlcv': defaultdict(lambda: deque(maxlen=100))
        }
        
        # ì§€í‘œ ì„±ëŠ¥ ì¶”ì  ì €ì¥ì†Œ ì¶”ê°€
        self.indicator_performance = defaultdict(lambda: {
            'success_count': 0,
            'failure_count': 0,
            'total_time': 0.0,
            'avg_time': 0.0,
            'last_updated': None
        })
        
        # ì•Œë¦¼ ì„¤ì •
        self.alert_thresholds = {
            'duplicate_rate': 0.3,      # 30% ì´ìƒ ë™ì¼ê°’
            'completion_rate': 0.7,     # 70% ë¯¸ë§Œ ì™„ì„±ë„
            'quality_score': 5.0,       # 5.0 ë¯¸ë§Œ í’ˆì§ˆ ì ìˆ˜
            'consecutive_failures': 3    # ì—°ì† 3íšŒ ì‹¤íŒ¨
        }
        
        # ì´ìƒ íŒ¨í„´ íƒì§€ê¸°
        self.anomaly_detectors = {}
        self.setup_anomaly_detection()
        
        # ë³´ê³ ì„œ ì„¤ì •
        self.reports_dir = "reports/quality"
        os.makedirs(self.reports_dir, exist_ok=True)
        
        logger.info("âœ… Data Quality Monitor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def setup_anomaly_detection(self):
        """ì´ìƒ íŒ¨í„´ íƒì§€ ì„¤ì •"""
        
        # static_indicators ì§€í‘œë³„ ì •ìƒ ë²”ìœ„ ì •ì˜
        self.normal_ranges = {
            'ma200_slope': (-50.0, 50.0),
            'nvt_relative': (0.1, 100.0),
            'volume_change_7_30': (0.01, 50.0),
            'adx': (0.0, 100.0),
            'supertrend_signal': (0.0, 1.0)
        }
        
        # ë™ì¼ê°’ íŒ¨í„´ íƒì§€ê¸°
        self.duplicate_patterns = {
            'exact_match': {},      # ì •í™•í•œ ë™ì¼ê°’
            'cluster_match': {},    # ìœ ì‚¬ê°’ í´ëŸ¬ìŠ¤í„°
            'sequence_match': {}    # ì—°ì†ëœ ë™ì¼ê°’
        }
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_running:
            logger.warning("âš ï¸ ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return
        
        self.is_running = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info(f"âœ… ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {self.monitor_interval}ì´ˆ)")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        
        logger.info("â¹ï¸ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë©”ì¸ ë£¨í”„"""
        while self.is_running:
            try:
                start_time = time.time()
                
                # 1. static_indicators í’ˆì§ˆ ê²€ì‚¬
                static_metrics = self._check_static_indicators_quality()
                
                # 2. ohlcv í’ˆì§ˆ ê²€ì‚¬  
                ohlcv_metrics = self._check_ohlcv_quality()
                
                # 3. ì´ìƒ íŒ¨í„´ íƒì§€
                anomalies = self._detect_anomalies(static_metrics, ohlcv_metrics)
                
                # 4. ì•Œë¦¼ ì²˜ë¦¬
                self._process_alerts(static_metrics, ohlcv_metrics, anomalies)
                
                # 5. í’ˆì§ˆ ì§€í‘œ ì €ì¥
                self._store_quality_metrics(static_metrics, ohlcv_metrics)
                
                # 6. ë³´ê³ ì„œ ìƒì„± (ë§¤ì‹œê°„)
                if datetime.now().minute == 0:
                    self._generate_hourly_report()
                
                # ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
                execution_time = time.time() - start_time
                logger.debug(f"ğŸ“Š í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
                
                # ë‹¤ìŒ ì‚¬ì´í´ê¹Œì§€ ëŒ€ê¸°
                time.sleep(max(0, self.monitor_interval - execution_time))
                
            except Exception as e:
                logger.error(f"âŒ ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°
    
    def _check_static_indicators_quality(self) -> Dict[str, Any]:
        """static_indicators í…Œì´ë¸” í’ˆì§ˆ ê²€ì‚¬"""
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # 1. ì „ì²´ ë°ì´í„° ê°œìˆ˜ ë° ì™„ì„±ë„
                cursor.execute("SELECT COUNT(*) FROM static_indicators")
                total_count = cursor.fetchone()[0]
                
                # 2. ê° ì»¬ëŸ¼ë³„ NULL ë¹„ìœ¨
                columns = ['ma200_slope', 'nvt_relative', 'volume_change_7_30', 'adx', 'supertrend_signal']
                null_rates = {}
                
                for col in columns:
                    cursor.execute(f"SELECT COUNT(*) FROM static_indicators WHERE {col} IS NULL")
                    null_count = cursor.fetchone()[0]
                    null_rates[col] = null_count / total_count if total_count > 0 else 0
                
                # 3. ë™ì¼ê°’ ê²€ì¶œ
                duplicate_stats = self._detect_duplicates_in_db('static_indicators', columns)
                
                # 4. ê°’ ë¶„í¬ ì´ìƒ íƒì§€
                distribution_anomalies = self._check_value_distributions('static_indicators', columns)
                
                # 5. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_table_quality_score(
                    null_rates, duplicate_stats, distribution_anomalies
                )
                
                metrics = {
                    'timestamp': datetime.now(),
                    'table': 'static_indicators',
                    'total_records': total_count,
                    'null_rates': null_rates,
                    'duplicate_stats': duplicate_stats,
                    'distribution_anomalies': distribution_anomalies,
                    'quality_score': quality_score,
                    'completion_rate': 1.0 - sum(null_rates.values()) / len(null_rates)
                }
                
                logger.debug(f"ğŸ“Š static_indicators í’ˆì§ˆ: {quality_score:.1f}/10 (ì™„ì„±ë„: {metrics['completion_rate']:.1%})")
                return metrics
                
        except Exception as e:
            logger.error(f"âŒ static_indicators í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return {'timestamp': datetime.now(), 'table': 'static_indicators', 'error': str(e)}
    
    def _check_ohlcv_quality(self) -> Dict[str, Any]:
        """ohlcv í…Œì´ë¸” í’ˆì§ˆ ê²€ì‚¬"""
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ìµœê·¼ 7ì¼ ë°ì´í„° ëŒ€ìƒ
                cursor.execute("""
                    SELECT COUNT(*) FROM ohlcv 
                    WHERE date >= CURRENT_DATE - INTERVAL '7 days'
                """)
                total_count = cursor.fetchone()[0]
                
                # OHLCV ê¸°ë³¸ ì»¬ëŸ¼ ê²€ì‚¬
                columns = ['open', 'high', 'low', 'close', 'volume']
                null_rates = {}
                
                for col in columns:
                    cursor.execute(f"""
                        SELECT COUNT(*) FROM ohlcv 
                        WHERE {col} IS NULL OR {col} = 0
                        AND date >= CURRENT_DATE - INTERVAL '7 days'
                    """)
                    null_count = cursor.fetchone()[0]
                    null_rates[col] = null_count / total_count if total_count > 0 else 0
                
                # ê°€ê²© ë…¼ë¦¬ ê²€ì¦ (high >= low, close >= 0 ë“±)
                logic_errors = self._check_ohlcv_logic_errors()
                
                quality_score = self._calculate_ohlcv_quality_score(null_rates, logic_errors)
                
                metrics = {
                    'timestamp': datetime.now(),
                    'table': 'ohlcv',
                    'total_records': total_count,
                    'null_rates': null_rates,
                    'logic_errors': logic_errors,
                    'quality_score': quality_score,
                    'completion_rate': 1.0 - sum(null_rates.values()) / len(null_rates)
                }
                
                logger.debug(f"ğŸ“Š ohlcv í’ˆì§ˆ: {quality_score:.1f}/10 (ì™„ì„±ë„: {metrics['completion_rate']:.1%})")
                return metrics
                
        except Exception as e:
            logger.error(f"âŒ ohlcv í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return {'timestamp': datetime.now(), 'table': 'ohlcv', 'error': str(e)}
    
    def _detect_duplicates_in_db(self, table: str, columns: List[str]) -> Dict[str, Any]:
        """DBì—ì„œ ë™ì¼ê°’ íŒ¨í„´ íƒì§€"""
        duplicate_stats = {}
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                for col in columns:
                    # ë™ì¼ê°’ ë¹ˆë„ ìƒìœ„ 10ê°œ ì¡°íšŒ
                    cursor.execute(f"""
                        SELECT {col}, COUNT(*) as count 
                        FROM {table} 
                        WHERE {col} IS NOT NULL
                        GROUP BY {col}
                        HAVING COUNT(*) > 1
                        ORDER BY count DESC
                        LIMIT 10
                    """)
                    
                    results = cursor.fetchall()
                    
                    if results:
                        total_duplicates = sum(count for _, count in results)
                        cursor.execute(f"SELECT COUNT(*) FROM {table} WHERE {col} IS NOT NULL")
                        total_records = cursor.fetchone()[0]
                        
                        duplicate_stats[col] = {
                            'duplicate_rate': total_duplicates / total_records if total_records > 0 else 0,
                            'top_duplicates': results[:5],  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
                            'total_duplicate_records': total_duplicates
                        }
                    else:
                        duplicate_stats[col] = {
                            'duplicate_rate': 0.0,
                            'top_duplicates': [],
                            'total_duplicate_records': 0
                        }
        
        except Exception as e:
            logger.error(f"âŒ {table} ë™ì¼ê°’ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return duplicate_stats
    
    def _check_value_distributions(self, table: str, columns: List[str]) -> Dict[str, Any]:
        """ê°’ ë¶„í¬ ì´ìƒ íƒì§€"""
        anomalies = {}
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                for col in columns:
                    if col not in self.normal_ranges:
                        continue
                    
                    # ê¸°ë³¸ í†µê³„
                    cursor.execute(f"""
                        SELECT 
                            MIN({col}) as min_val,
                            MAX({col}) as max_val,
                            AVG({col}) as avg_val,
                            STDDEV({col}) as std_val,
                            COUNT(*) as count
                        FROM {table}
                        WHERE {col} IS NOT NULL
                    """)
                    
                    stats = cursor.fetchone()
                    if not stats or stats[4] == 0:  # count = 0
                        continue
                    
                    min_val, max_val, avg_val, std_val, count = stats
                    normal_min, normal_max = self.normal_ranges[col]
                    
                    # ì´ìƒ ê°ì§€
                    issues = []
                    
                    if min_val < normal_min:
                        issues.append(f"ìµœì†Œê°’ ì´ìƒ: {min_val} < {normal_min}")
                    
                    if max_val > normal_max:
                        issues.append(f"ìµœëŒ€ê°’ ì´ìƒ: {max_val} > {normal_max}")
                    
                    if std_val and std_val > abs(avg_val) * 2:  # í‘œì¤€í¸ì°¨ê°€ í‰ê· ì˜ 2ë°° ì´ìƒ
                        issues.append(f"ë†’ì€ ë³€ë™ì„±: std={std_val:.3f}, avg={avg_val:.3f}")
                    
                    anomalies[col] = {
                        'stats': {'min': min_val, 'max': max_val, 'avg': avg_val, 'std': std_val},
                        'issues': issues,
                        'severity': len(issues)
                    }
        
        except Exception as e:
            logger.error(f"âŒ {table} ë¶„í¬ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        
        return anomalies
    
    def _check_ohlcv_logic_errors(self) -> Dict[str, int]:
        """OHLCV ë…¼ë¦¬ ì˜¤ë¥˜ ê²€ì‚¬"""
        errors = {}
        
        try:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                
                # High < Low ì˜¤ë¥˜
                cursor.execute("""
                    SELECT COUNT(*) FROM ohlcv 
                    WHERE high < low AND date >= CURRENT_DATE - INTERVAL '7 days'
                """)
                errors['high_less_than_low'] = cursor.fetchone()[0]
                
                # Closeê°€ High/Low ë²”ìœ„ ë°–
                cursor.execute("""
                    SELECT COUNT(*) FROM ohlcv 
                    WHERE (close > high OR close < low) 
                    AND date >= CURRENT_DATE - INTERVAL '7 days'
                """)
                errors['close_out_of_range'] = cursor.fetchone()[0]
                
                # ìŒìˆ˜ ê°€ê²©
                cursor.execute("""
                    SELECT COUNT(*) FROM ohlcv 
                    WHERE (open <= 0 OR high <= 0 OR low <= 0 OR close <= 0)
                    AND date >= CURRENT_DATE - INTERVAL '7 days'
                """)
                errors['negative_prices'] = cursor.fetchone()[0]
                
                # ê·¹ë‹¨ì  ê°€ê²© ë³€í™” (ì „ì¼ ëŒ€ë¹„ 50% ì´ìƒ)
                cursor.execute("""
                    WITH price_changes AS (
                        SELECT ticker, date, close,
                               LAG(close) OVER (PARTITION BY ticker ORDER BY date) as prev_close
                        FROM ohlcv 
                        WHERE date >= CURRENT_DATE - INTERVAL '7 days'
                    )
                    SELECT COUNT(*) FROM price_changes
                    WHERE prev_close IS NOT NULL 
                    AND ABS(close - prev_close) / prev_close > 0.5
                """)
                errors['extreme_price_changes'] = cursor.fetchone()[0]
        
        except Exception as e:
            logger.error(f"âŒ OHLCV ë…¼ë¦¬ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        
        return errors
    
    def _calculate_table_quality_score(self, null_rates: Dict[str, float], 
                                     duplicate_stats: Dict[str, Any], 
                                     distribution_anomalies: Dict[str, Any]) -> float:
        """í…Œì´ë¸” í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            base_score = 10.0
            
            # NULL ë¹„ìœ¨ ê°ì  (ê° ì»¬ëŸ¼ë‹¹ ìµœëŒ€ 1ì  ê°ì )
            null_penalty = sum(min(rate * 2, 1.0) for rate in null_rates.values())
            
            # ë™ì¼ê°’ ë¹„ìœ¨ ê°ì 
            duplicate_penalty = 0
            for col, stats in duplicate_stats.items():
                duplicate_penalty += min(stats['duplicate_rate'] * 3, 2.0)
            
            # ë¶„í¬ ì´ìƒ ê°ì 
            distribution_penalty = sum(min(anomaly['severity'] * 0.5, 1.0) 
                                     for anomaly in distribution_anomalies.values())
            
            final_score = max(0.0, base_score - null_penalty - duplicate_penalty - distribution_penalty)
            return min(final_score, 10.0)
            
        except:
            return 5.0  # ê¸°ë³¸ ì ìˆ˜
    
    def _calculate_ohlcv_quality_score(self, null_rates: Dict[str, float], 
                                     logic_errors: Dict[str, int]) -> float:
        """OHLCV í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            base_score = 10.0
            
            # NULL/0 ê°’ ê°ì 
            null_penalty = sum(min(rate * 3, 2.0) for rate in null_rates.values())
            
            # ë…¼ë¦¬ ì˜¤ë¥˜ ê°ì 
            logic_penalty = min(sum(logic_errors.values()) * 0.1, 3.0)
            
            final_score = max(0.0, base_score - null_penalty - logic_penalty)
            return min(final_score, 10.0)
            
        except:
            return 5.0
    
    def _detect_anomalies(self, static_metrics: Dict[str, Any], 
                         ohlcv_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì´ìƒ íŒ¨í„´ íƒì§€"""
        anomalies = []
        
        try:
            # static_indicators ì´ìƒ íƒì§€
            if 'error' not in static_metrics:
                if static_metrics['quality_score'] < self.alert_thresholds['quality_score']:
                    anomalies.append({
                        'type': 'low_quality',
                        'table': 'static_indicators',
                        'score': static_metrics['quality_score'],
                        'threshold': self.alert_thresholds['quality_score'],
                        'severity': 'high' if static_metrics['quality_score'] < 3.0 else 'medium'
                    })
                
                # ë†’ì€ ë™ì¼ê°’ ë¹„ìœ¨ íƒì§€
                for col, stats in static_metrics.get('duplicate_stats', {}).items():
                    if stats['duplicate_rate'] > self.alert_thresholds['duplicate_rate']:
                        anomalies.append({
                            'type': 'high_duplicates',
                            'table': 'static_indicators',
                            'column': col,
                            'rate': stats['duplicate_rate'],
                            'threshold': self.alert_thresholds['duplicate_rate'],
                            'severity': 'high' if stats['duplicate_rate'] > 0.5 else 'medium'
                        })
            
            # ohlcv ì´ìƒ íƒì§€
            if 'error' not in ohlcv_metrics:
                logic_errors = ohlcv_metrics.get('logic_errors', {})
                total_errors = sum(logic_errors.values())
                
                if total_errors > 10:  # ì„ê³„ê°’: 10ê°œ ì´ìƒ ë…¼ë¦¬ ì˜¤ë¥˜
                    anomalies.append({
                        'type': 'logic_errors',
                        'table': 'ohlcv',
                        'error_count': total_errors,
                        'errors': logic_errors,
                        'severity': 'high' if total_errors > 50 else 'medium'
                    })
        
        except Exception as e:
            logger.error(f"âŒ ì´ìƒ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return anomalies
    
    def _process_alerts(self, static_metrics: Dict[str, Any], 
                       ohlcv_metrics: Dict[str, Any], 
                       anomalies: List[Dict[str, Any]]):
        """ì•Œë¦¼ ì²˜ë¦¬"""
        try:
            if not anomalies:
                return
            
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            high_severity = [a for a in anomalies if a.get('severity') == 'high']
            medium_severity = [a for a in anomalies if a.get('severity') == 'medium']
            
            # ê³ ì‹¬ê°ë„ ì•Œë¦¼
            if high_severity:
                self._send_alert('HIGH', high_severity, static_metrics, ohlcv_metrics)
            
            # ì¤‘ê°„ ì‹¬ê°ë„ ì•Œë¦¼ (1ì‹œê°„ì— í•œ ë²ˆë§Œ)
            if medium_severity and datetime.now().minute == 0:
                self._send_alert('MEDIUM', medium_severity, static_metrics, ohlcv_metrics)
        
        except Exception as e:
            logger.error(f"âŒ ì•Œë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _send_alert(self, severity: str, anomalies: List[Dict[str, Any]], static_metrics: Dict[str, Any], ohlcv_metrics: Dict[str, Any]):
        """ì•Œë¦¼ ë°œì†¡"""
        try:
            alert_message = f"ğŸš¨ [{severity}] ë°ì´í„° í’ˆì§ˆ ì•Œë¦¼\n\n"
            alert_message += f"ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            alert_message += f"íƒì§€ëœ ì´ìƒ: {len(anomalies)}ê°œ\n\n"
            
            for anomaly in anomalies:
                alert_message += f"â€¢ {anomaly['type']}: {anomaly.get('table', 'unknown')}\n"
                if 'column' in anomaly:
                    alert_message += f"  ì»¬ëŸ¼: {anomaly['column']}\n"
                if 'rate' in anomaly:
                    alert_message += f"  ë¹„ìœ¨: {anomaly['rate']:.1%}\n"
                if 'score' in anomaly:
                    alert_message += f"  ì ìˆ˜: {anomaly['score']:.1f}/10\n"
                alert_message += "\n"
            
            # í’ˆì§ˆ ìš”ì•½
            static_score = static_metrics.get('quality_score', 0)
            ohlcv_score = ohlcv_metrics.get('quality_score', 0)
            alert_message += f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜:\n"
            alert_message += f"â€¢ static_indicators: {static_score:.1f}/10\n"
            alert_message += f"â€¢ ohlcv: {ohlcv_score:.1f}/10\n"
            
            # ë¡œê·¸ë¡œ ì•Œë¦¼ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥)
            logger.warning(alert_message)
            
            # ì•Œë¦¼ íŒŒì¼ë¡œ ì €ì¥
            alert_file = os.path.join(self.reports_dir, f"alert_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
            with open(alert_file, 'w', encoding='utf-8') as f:
                f.write(alert_message)
        
        except Exception as e:
            logger.error(f"âŒ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def _store_quality_metrics(self, static_metrics: Dict[str, Any], ohlcv_metrics: Dict[str, Any]):
        """í’ˆì§ˆ ì§€í‘œ ì €ì¥"""
        try:
            # ë©”ëª¨ë¦¬ ì €ì¥
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            self.quality_metrics['static_indicators']['scores'].append({
                'timestamp': timestamp,
                'quality_score': static_metrics.get('quality_score', 0),
                'completion_rate': static_metrics.get('completion_rate', 0)
            })
            
            self.quality_metrics['ohlcv']['scores'].append({
                'timestamp': timestamp,
                'quality_score': ohlcv_metrics.get('quality_score', 0),
                'completion_rate': ohlcv_metrics.get('completion_rate', 0)
            })
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ì§€í‘œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _generate_hourly_report(self):
        """ì‹œê°„ë³„ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        try:
            report_time = datetime.now()
            report_file = os.path.join(
                self.reports_dir, 
                f"quality_report_{report_time.strftime('%Y%m%d_%H00')}.json"
            )
            
            # ìµœê·¼ 1ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
            recent_static = list(self.quality_metrics['static_indicators']['scores'])[-12:]  # 5ë¶„*12 = 1ì‹œê°„
            recent_ohlcv = list(self.quality_metrics['ohlcv']['scores'])[-12:]
            
            report = {
                'timestamp': report_time.isoformat(),
                'period': '1_hour',
                'static_indicators': {
                    'avg_quality_score': np.mean([s['quality_score'] for s in recent_static]) if recent_static else 0,
                    'avg_completion_rate': np.mean([s['completion_rate'] for s in recent_static]) if recent_static else 0,
                    'trend': 'improving' if len(recent_static) >= 2 and recent_static[-1]['quality_score'] > recent_static[0]['quality_score'] else 'declining'
                },
                'ohlcv': {
                    'avg_quality_score': np.mean([s['quality_score'] for s in recent_ohlcv]) if recent_ohlcv else 0,
                    'avg_completion_rate': np.mean([s['completion_rate'] for s in recent_ohlcv]) if recent_ohlcv else 0,
                    'trend': 'improving' if len(recent_ohlcv) >= 2 and recent_ohlcv[-1]['quality_score'] > recent_ohlcv[0]['quality_score'] else 'declining'
                }
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“Š ì‹œê°„ë³„ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê°„ë³„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def get_current_status(self) -> Dict[str, Any]:
        """í˜„ì¬ í’ˆì§ˆ ìƒíƒœ ì¡°íšŒ"""
        try:
            static_recent = list(self.quality_metrics['static_indicators']['scores'])
            ohlcv_recent = list(self.quality_metrics['ohlcv']['scores'])
            
            status = {
                'monitoring_active': self.is_running,
                'last_update': datetime.now().isoformat(),
                'static_indicators': {
                    'latest_score': static_recent[-1]['quality_score'] if static_recent else 0,
                    'latest_completion': static_recent[-1]['completion_rate'] if static_recent else 0,
                    'data_points': len(static_recent)
                },
                'ohlcv': {
                    'latest_score': ohlcv_recent[-1]['quality_score'] if ohlcv_recent else 0,
                    'latest_completion': ohlcv_recent[-1]['completion_rate'] if ohlcv_recent else 0,
                    'data_points': len(ohlcv_recent)
                }
            }
            
            return status
            
        except Exception as e:
            logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def generate_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ìƒì„±"""
        try:
            # ìµœê·¼ 24ì‹œê°„ ë°ì´í„° (5ë¶„ ê°„ê²© * 288 = 24ì‹œê°„)
            static_data = list(self.quality_metrics['static_indicators']['scores'])[-288:]
            ohlcv_data = list(self.quality_metrics['ohlcv']['scores'])[-288:]
            
            dashboard = {
                'generated_at': datetime.now().isoformat(),
                'charts': {
                    'quality_scores': {
                        'static_indicators': [
                            {'x': item['timestamp'], 'y': item['quality_score']} 
                            for item in static_data
                        ],
                        'ohlcv': [
                            {'x': item['timestamp'], 'y': item['quality_score']} 
                            for item in ohlcv_data
                        ]
                    },
                    'completion_rates': {
                        'static_indicators': [
                            {'x': item['timestamp'], 'y': item['completion_rate']} 
                            for item in static_data
                        ],
                        'ohlcv': [
                            {'x': item['timestamp'], 'y': item['completion_rate']} 
                            for item in ohlcv_data
                        ]
                    }
                },
                'summary': {
                    'avg_quality_24h': {
                        'static_indicators': np.mean([s['quality_score'] for s in static_data]) if static_data else 0,
                        'ohlcv': np.mean([s['quality_score'] for s in ohlcv_data]) if ohlcv_data else 0
                    },
                    'avg_completion_24h': {
                        'static_indicators': np.mean([s['completion_rate'] for s in static_data]) if static_data else 0,
                        'ohlcv': np.mean([s['completion_rate'] for s in ohlcv_data]) if ohlcv_data else 0
                    }
                }
            }
            
            return dashboard
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def track_indicator_performance(self, indicator_name: str, calculation_time: float, success: bool = True):
        """
        ì§€í‘œ ê³„ì‚° ì„±ëŠ¥ì„ ì¶”ì í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            indicator_name (str): ì§€í‘œ ì´ë¦„
            calculation_time (float): ê³„ì‚° ì†Œìš” ì‹œê°„ (ì´ˆ)
            success (bool): ê³„ì‚° ì„±ê³µ ì—¬ë¶€
        """
        try:
            if indicator_name not in self.indicator_performance:
                self.indicator_performance[indicator_name] = {
                    'success_count': 0,
                    'failure_count': 0,
                    'total_time': 0.0,
                    'avg_time': 0.0,
                    'last_updated': datetime.now()
                }
            
            perf = self.indicator_performance[indicator_name]
            
            if success:
                perf['success_count'] += 1
            else:
                perf['failure_count'] += 1
            
            perf['total_time'] += calculation_time
            perf['avg_time'] = perf['total_time'] / (perf['success_count'] + perf['failure_count'])
            perf['last_updated'] = datetime.now()
            
            # ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼
            self._check_indicator_performance_thresholds(indicator_name, perf)
            
        except Exception as e:
            logger.error(f"âŒ ì§€í‘œ ì„±ëŠ¥ ì¶”ì  ì‹¤íŒ¨ ({indicator_name}): {e}")
    
    def _check_indicator_performance_thresholds(self, indicator_name: str, performance: Dict[str, Any]):
        """ì§€í‘œ ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼"""
        try:
            total_attempts = performance['success_count'] + performance['failure_count']
            if total_attempts < 10:  # ìµœì†Œ 10íšŒ ì‹œë„ í›„ ì²´í¬
                return
            
            failure_rate = performance['failure_count'] / total_attempts
            avg_time = performance['avg_time']
            
            # ì‹¤íŒ¨ìœ¨ì´ 20% ì´ìƒì´ê±°ë‚˜ í‰ê·  ê³„ì‚° ì‹œê°„ì´ 1ì´ˆ ì´ìƒì¸ ê²½ìš° ì•Œë¦¼
            if failure_rate > 0.2 or avg_time > 1.0:
                alert_msg = f"âš ï¸ ì§€í‘œ ì„±ëŠ¥ ê²½ê³ : {indicator_name}\n"
                alert_msg += f"   ì‹¤íŒ¨ìœ¨: {failure_rate:.1%}\n"
                alert_msg += f"   í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ\n"
                alert_msg += f"   ì´ ì‹œë„: {total_attempts}íšŒ"
                
                logger.warning(alert_msg)
                
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def get_indicator_performance_summary(self) -> Dict[str, Any]:
        """ì§€í‘œ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        try:
            summary = {
                'total_indicators': len(self.indicator_performance),
                'indicators': {},
                'overall_stats': {
                    'total_success': 0,
                    'total_failure': 0,
                    'avg_calculation_time': 0.0
                }
            }
            
            total_success = 0
            total_failure = 0
            total_time = 0.0
            
            for indicator_name, perf in self.indicator_performance.items():
                total_attempts = perf['success_count'] + perf['failure_count']
                if total_attempts > 0:
                    success_rate = perf['success_count'] / total_attempts
                    failure_rate = perf['failure_count'] / total_attempts
                else:
                    success_rate = 0.0
                    failure_rate = 0.0
                
                summary['indicators'][indicator_name] = {
                    'success_count': perf['success_count'],
                    'failure_count': perf['failure_count'],
                    'success_rate': success_rate,
                    'failure_rate': failure_rate,
                    'avg_time': perf['avg_time'],
                    'last_updated': perf['last_updated'].isoformat() if perf['last_updated'] else None
                }
                
                total_success += perf['success_count']
                total_failure += perf['failure_count']
                total_time += perf['total_time']
            
            total_attempts = total_success + total_failure
            if total_attempts > 0:
                summary['overall_stats']['total_success'] = total_success
                summary['overall_stats']['total_failure'] = total_failure
                summary['overall_stats']['avg_calculation_time'] = total_time / total_attempts
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ì§€í‘œ ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def log_indicator_calculation_quality(self, ticker: str, df: pd.DataFrame, available_indicators: list):
        """
        ì§€í‘œ ê³„ì‚° í’ˆì§ˆì„ ë¡œê¹…í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            df (pd.DataFrame): ê³„ì‚°ëœ ì§€í‘œê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„
            available_indicators (list): ì‚¬ìš© ê°€ëŠ¥í•œ ì§€í‘œ ëª©ë¡
        """
        try:
            if df is None or df.empty:
                logger.warning(f"âš ï¸ {ticker} ì§€í‘œ í’ˆì§ˆ ë¡œê¹…: ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŒ")
                return
            
            # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
            total_indicators = len(available_indicators)
            valid_indicators = 0
            null_counts = {}
            
            for indicator in available_indicators:
                if indicator in df.columns:
                    null_count = df[indicator].isna().sum()
                    total_count = len(df[indicator])
                    
                    if total_count > 0:
                        null_ratio = null_count / total_count
                        null_counts[indicator] = {
                            'null_count': null_count,
                            'total_count': total_count,
                            'null_ratio': null_ratio
                        }
                        
                        # ìœ íš¨í•œ ì§€í‘œ íŒë‹¨ (null ë¹„ìœ¨ì´ 50% ë¯¸ë§Œ)
                        if null_ratio < 0.5:
                            valid_indicators += 1
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-10)
            quality_score = (valid_indicators / total_indicators) * 10 if total_indicators > 0 else 0
            
            # ë¡œê¹…
            logger.info(f"ğŸ“Š {ticker} ì§€í‘œ ê³„ì‚° í’ˆì§ˆ: {quality_score:.1f}/10")
            logger.info(f"   - ì´ ì§€í‘œ: {total_indicators}ê°œ")
            logger.info(f"   - ìœ íš¨ ì§€í‘œ: {valid_indicators}ê°œ")
            logger.info(f"   - ìœ íš¨ìœ¨: {(valid_indicators/total_indicators*100):.1f}%" if total_indicators > 0 else "0%")
            
            # ë¬¸ì œê°€ ìˆëŠ” ì§€í‘œë“¤ ë¡œê¹…
            problematic_indicators = []
            for indicator, stats in null_counts.items():
                if stats['null_ratio'] > 0.3:  # 30% ì´ìƒ null
                    problematic_indicators.append(f"{indicator}({stats['null_ratio']:.1%})")
            
            if problematic_indicators:
                logger.warning(f"   - ë¬¸ì œ ì§€í‘œ: {', '.join(problematic_indicators)}")
            
            # í’ˆì§ˆ ì§€í‘œ ì €ì¥
            self._store_calculation_quality(ticker, quality_score, valid_indicators, total_indicators)
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ì§€í‘œ ê³„ì‚° í’ˆì§ˆ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    def _store_calculation_quality(self, ticker: str, quality_score: float, valid_count: int, total_count: int):
        """ê³„ì‚° í’ˆì§ˆ ì •ë³´ ì €ì¥"""
        try:
            # ë©”ëª¨ë¦¬ì— í’ˆì§ˆ ì •ë³´ ì €ì¥
            if not hasattr(self, 'calculation_quality'):
                self.calculation_quality = {}
            
            self.calculation_quality[ticker] = {
                'timestamp': datetime.now(),
                'quality_score': quality_score,
                'valid_indicators': valid_count,
                'total_indicators': total_count,
                'validity_ratio': valid_count / total_count if total_count > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ ê³„ì‚° í’ˆì§ˆ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_calculation_quality_summary(self) -> Dict[str, Any]:
        """ê³„ì‚° í’ˆì§ˆ ìš”ì•½ ì¡°íšŒ"""
        try:
            if not hasattr(self, 'calculation_quality') or not self.calculation_quality:
                return {'message': 'í’ˆì§ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            total_tickers = len(self.calculation_quality)
            avg_quality = sum(data['quality_score'] for data in self.calculation_quality.values()) / total_tickers
            avg_validity = sum(data['validity_ratio'] for data in self.calculation_quality.values()) / total_tickers
            
            summary = {
                'total_tickers': total_tickers,
                'average_quality_score': avg_quality,
                'average_validity_ratio': avg_validity,
                'ticker_details': {}
            }
            
            # í‹°ì»¤ë³„ ìƒì„¸ ì •ë³´
            for ticker, data in self.calculation_quality.items():
                summary['ticker_details'][ticker] = {
                    'quality_score': data['quality_score'],
                    'valid_indicators': data['valid_indicators'],
                    'total_indicators': data['total_indicators'],
                    'validity_ratio': data['validity_ratio'],
                    'timestamp': data['timestamp'].isoformat()
                }
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ê³„ì‚° í’ˆì§ˆ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def log_api_response_quality(self, ticker: str, df: Any, api_params: Dict):
        """
        API ì‘ë‹µ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            df (Any): API ì‘ë‹µ ë°ì´í„°í”„ë ˆì„
            api_params (Dict): API í˜¸ì¶œ íŒŒë¼ë¯¸í„°
            
        Returns:
            bool: í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ ì—¬ë¶€
        """
        try:
            # API í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸
            if not hasattr(self, 'api_call_stats'):
                self.api_call_stats = {
                    'total_calls': 0,
                    'successful_calls': 0,
                    'failed_calls': 0,
                    'empty_responses': 0,
                    'invalid_responses': 0
                }
            
            self.api_call_stats['total_calls'] += 1
            
            # ê¸°ë³¸ ì‘ë‹µ ê²€ì¦
            if df is None:
                self.api_call_stats['failed_calls'] += 1
                logger.warning(f"âš ï¸ {ticker} API ì‘ë‹µ None")
                return False
                
            if hasattr(df, 'empty') and df.empty:
                self.api_call_stats['empty_responses'] += 1
                logger.warning(f"âš ï¸ {ticker} API ì‘ë‹µ ë¹ˆ DataFrame")
                return False
            
            # 1970-01-01 ì˜¤ë¥˜ ê°ì§€
            if hasattr(df, 'index') and len(df) > 0:
                try:
                    first_date = df.index[0]
                    if hasattr(first_date, 'year') and first_date.year == 1970:
                        self.api_call_stats['invalid_responses'] += 1
                        logger.error(f"ğŸš¨ {ticker} 1970-01-01 ì˜¤ë¥˜ ê°ì§€")
                        return False
                except Exception as e:
                    logger.debug(f"API ë‚ ì§œ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì„±ê³µì ì¸ ì‘ë‹µ
            self.api_call_stats['successful_calls'] += 1
            return True
            
        except Exception as e:
            logger.error(f"âŒ API í’ˆì§ˆ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def get_quality_summary(self) -> Dict[str, Any]:
        """ì „ì²´ í’ˆì§ˆ ìš”ì•½ ì¡°íšŒ"""
        try:
            summary = {
                'total_api_calls': 0,
                'api_1970_error_rate': 0.0,
                'total_indicator_calculations': 0,
                'indicator_failure_rate': 0.0,
                'total_db_updates': 0,
                'db_failure_rate': 0.0
            }
            
            # API í†µê³„
            if hasattr(self, 'api_call_stats'):
                total_api = self.api_call_stats['total_calls']
                invalid_api = self.api_call_stats['invalid_responses']
                
                summary['total_api_calls'] = total_api
                if total_api > 0:
                    summary['api_1970_error_rate'] = (invalid_api / total_api) * 100
            
            # ì§€í‘œ ê³„ì‚° í†µê³„
            if hasattr(self, 'indicator_performance'):
                total_calcs = sum(perf['success_count'] + perf['failure_count'] 
                                for perf in self.indicator_performance.values())
                failed_calcs = sum(perf['failure_count'] 
                                 for perf in self.indicator_performance.values())
                
                summary['total_indicator_calculations'] = total_calcs
                if total_calcs > 0:
                    summary['indicator_failure_rate'] = (failed_calcs / total_calcs) * 100
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

# ì „ì—­ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
quality_monitor = DataQualityMonitor()

def start_quality_monitoring(interval: int = 300):
    """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    global quality_monitor
    quality_monitor.monitor_interval = interval
    quality_monitor.start_monitoring()

def stop_quality_monitoring():
    """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
    global quality_monitor
    quality_monitor.stop_monitoring()

def get_quality_status():
    """í˜„ì¬ í’ˆì§ˆ ìƒíƒœ ì¡°íšŒ"""
    return quality_monitor.get_current_status()

def generate_quality_dashboard():
    """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
    return quality_monitor.generate_dashboard_data()

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë¡œê·¸
logger.info("âœ… Data Quality Monitoring System ì´ˆê¸°í™” ì™„ë£Œ")
logger.info("   ğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§, ì´ìƒ íƒì§€, ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ í™œì„±í™”")
logger.info("   ğŸ”§ ì‚¬ìš©ë²•: start_quality_monitoring() í˜¸ì¶œë¡œ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
