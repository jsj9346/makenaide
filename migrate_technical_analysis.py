#!/usr/bin/env python3
"""
ê¸°ìˆ ì  ë¶„ì„ í…Œì´ë¸” í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
technical_analysis + makenaide_technical_analysis â†’ í†µí•© technical_analysis

ğŸ¯ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ:
1. ìƒˆë¡œìš´ í†µí•© ìŠ¤í‚¤ë§ˆë¡œ technical_analysis_unified í…Œì´ë¸” ìƒì„±
2. ë‘ ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë³‘í•© (ticker ê¸°ì¤€ LEFT JOIN)
3. ì»¬ëŸ¼ ë§¤í•‘ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
4. ê¸°ì¡´ í…Œì´ë¸” ë°±ì—… í›„ ìƒˆ í…Œì´ë¸”ë¡œ êµì²´

ğŸ“Š ë°ì´í„° ì†ŒìŠ¤:
- technical_analysis: Weinstein Stage ë¶„ì„ (205ê°œ, 2025-09-17~19)
- makenaide_technical_analysis: LayeredScoring (201ê°œ, 2025-09-21)
"""

import sqlite3
import os
import logging
from datetime import datetime
from typing import Optional, Dict, Any
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TechnicalAnalysisMigrator:
    """ê¸°ìˆ ì  ë¶„ì„ í…Œì´ë¸” í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, db_path: str = "makenaide_local.db"):
        """
        ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤ ì´ˆê¸°í™”

        Args:
            db_path: SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        """
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None
        self.migration_log = {
            "start_time": None,
            "end_time": None,
            "source_counts": {},
            "target_count": 0,
            "errors": [],
            "success": False
        }

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.close()

    def connect(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row  # ë”•ì…”ë„ˆë¦¬ ìŠ¤íƒ€ì¼ ì ‘ê·¼
            logger.info(f"ğŸ“± ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°: {self.db_path}")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    def close(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info("ğŸ”’ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

    def analyze_source_tables(self) -> Dict[str, Any]:
        """ì†ŒìŠ¤ í…Œì´ë¸” ë¶„ì„"""
        logger.info("ğŸ” ì†ŒìŠ¤ í…Œì´ë¸” ë¶„ì„ ì¤‘...")

        analysis = {
            "technical_analysis": {},
            "makenaide_technical_analysis": {}
        }

        # technical_analysis í…Œì´ë¸” ë¶„ì„
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total_count,
                COUNT(DISTINCT ticker) as unique_tickers,
                MIN(analysis_date) as oldest_date,
                MAX(analysis_date) as newest_date,
                COUNT(CASE WHEN current_stage IS NOT NULL THEN 1 END) as stage_data_count,
                COUNT(CASE WHEN supertrend IS NOT NULL AND supertrend != '' THEN 1 END) as supertrend_data_count,
                COUNT(CASE WHEN atr IS NOT NULL THEN 1 END) as atr_data_count
            FROM technical_analysis
        """)

        row = cursor.fetchone()
        analysis["technical_analysis"] = {
            "total_count": row["total_count"],
            "unique_tickers": row["unique_tickers"],
            "date_range": f"{row['oldest_date']} ~ {row['newest_date']}",
            "stage_data_coverage": f"{row['stage_data_count']}/{row['total_count']} ({row['stage_data_count']/row['total_count']*100:.1f}%)",
            "supertrend_coverage": f"{row['supertrend_data_count']}/{row['total_count']} ({row['supertrend_data_count']/row['total_count']*100:.1f}%)",
            "atr_coverage": f"{row['atr_data_count']}/{row['total_count']} ({row['atr_data_count']/row['total_count']*100:.1f}%)"
        }

        # makenaide_technical_analysis í…Œì´ë¸” ë¶„ì„
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total_count,
                COUNT(DISTINCT ticker) as unique_tickers,
                MIN(analysis_timestamp) as oldest_timestamp,
                MAX(analysis_timestamp) as newest_timestamp,
                COUNT(CASE WHEN total_score IS NOT NULL THEN 1 END) as score_data_count,
                COUNT(CASE WHEN macro_score IS NOT NULL THEN 1 END) as macro_score_count
            FROM makenaide_technical_analysis
        """)

        row = cursor.fetchone()
        analysis["makenaide_technical_analysis"] = {
            "total_count": row["total_count"],
            "unique_tickers": row["unique_tickers"],
            "timestamp_range": f"{row['oldest_timestamp']} ~ {row['newest_timestamp']}",
            "score_coverage": f"{row['score_data_count']}/{row['total_count']} ({row['score_data_count']/row['total_count']*100:.1f}%)",
            "macro_score_coverage": f"{row['macro_score_count']}/{row['total_count']} ({row['macro_score_count']/row['total_count']*100:.1f}%)"
        }

        # ê³µí†µ ticker ë¶„ì„
        cursor = self.conn.execute("""
            SELECT COUNT(DISTINCT t1.ticker) as common_tickers
            FROM technical_analysis t1
            INNER JOIN makenaide_technical_analysis t2 ON t1.ticker = t2.ticker
        """)
        analysis["common_tickers"] = cursor.fetchone()["common_tickers"]

        self.migration_log["source_counts"] = analysis
        return analysis

    def create_unified_table(self) -> None:
        """í†µí•© technical_analysis í…Œì´ë¸” ìƒì„±"""
        logger.info("ğŸ—ï¸ í†µí•© í…Œì´ë¸” ìƒì„± ì¤‘...")

        # ê¸°ì¡´ unified í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ì‚­ì œ
        self.conn.execute("DROP TABLE IF EXISTS technical_analysis_unified")

        # í†µí•© ìŠ¤í‚¤ë§ˆë¡œ ìƒˆ í…Œì´ë¸” ìƒì„±
        unified_schema = """
        CREATE TABLE technical_analysis_unified (
            -- ê¸°ë³¸ ì •ë³´
            ticker TEXT NOT NULL,
            analysis_date TEXT NOT NULL,

            -- Weinstein Stage analysis (HybridTechnicalFilter í˜¸í™˜)
            current_stage INTEGER,
            stage_confidence REAL,
            stage_trend TEXT,
            ma200_trend TEXT,
            ma200_slope REAL,
            price_vs_ma200 REAL,
            breakout_strength REAL,
            volume_surge REAL,
            days_in_stage INTEGER,

            -- 4-Gate í•„í„°ë§ ê²°ê³¼
            gate1_stage2 INTEGER,
            gate2_volume INTEGER,
            gate3_momentum INTEGER,
            gate4_quality INTEGER,
            total_gates_passed INTEGER,
            quality_score REAL,
            recommendation TEXT,

            -- ê¸°ìˆ ì  ì§€í‘œ (trading_engine.py í˜¸í™˜, íƒ€ì… ìµœì í™”)
            close_price REAL,
            ma5 REAL,
            ma20 REAL,
            ma60 REAL,
            ma120 REAL,
            ma200 REAL,
            rsi REAL,
            atr REAL,                    -- TEXT â†’ REAL ë³€ê²½
            supertrend REAL,             -- TEXT â†’ REAL ë³€ê²½
            macd_histogram REAL,         -- TEXT â†’ REAL ë³€ê²½
            adx REAL,
            support_level REAL,          -- TEXT â†’ REAL ë³€ê²½
            resistance_level REAL,
            volume_surge_factor REAL,

            -- LayeredScoringEngine í™•ì¥ (IntegratedScoringSystem í˜¸í™˜)
            macro_score REAL,
            structural_score REAL,
            micro_score REAL,
            total_score REAL,
            quality_gates_passed BOOLEAN,
            analysis_details TEXT,

            -- ë©”íƒ€ë°ì´í„°
            source_table TEXT,           -- ë°ì´í„° ì¶œì²˜ ì¶”ì 
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

            -- ë³µí•© UNIQUE ì œì•½ì¡°ê±´
            UNIQUE(ticker, analysis_date)
        )
        """

        self.conn.execute(unified_schema)

        # ì¸ë±ìŠ¤ ìƒì„±
        indexes = [
            "CREATE INDEX idx_unified_ticker ON technical_analysis_unified(ticker)",
            "CREATE INDEX idx_unified_date ON technical_analysis_unified(analysis_date)",
            "CREATE INDEX idx_unified_stage ON technical_analysis_unified(current_stage)",
            "CREATE INDEX idx_unified_recommendation ON technical_analysis_unified(recommendation)",
            "CREATE INDEX idx_unified_total_score ON technical_analysis_unified(total_score)"
        ]

        for index_sql in indexes:
            self.conn.execute(index_sql)

        logger.info("âœ… í†µí•© í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

    def migrate_data(self) -> None:
        """ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        logger.info("ğŸ”„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")

        # ìµœì‹  ë°ì´í„° ìš°ì„  ì •ì±…ìœ¼ë¡œ ë‘ í…Œì´ë¸” ë°ì´í„° ë³‘í•©
        # Step 1: technical_analysisì—ì„œ ê° tickerì˜ ìµœì‹  ë°ì´í„° ì„ íƒ
        latest_technical_query = """
        CREATE TEMPORARY TABLE latest_technical AS
        SELECT t1.*
        FROM technical_analysis t1
        WHERE t1.analysis_date = (
            SELECT MAX(t2.analysis_date)
            FROM technical_analysis t2
            WHERE t2.ticker = t1.ticker
        )
        """
        self.conn.execute(latest_technical_query)

        # Step 2: í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜
        migration_query = """
        INSERT INTO technical_analysis_unified (
            ticker, analysis_date,
            -- Weinstein Stage ë°ì´í„° (technical_analysisì—ì„œ)
            current_stage, stage_confidence, ma200_trend, ma200_slope,
            price_vs_ma200, breakout_strength, volume_surge, days_in_stage,
            gate1_stage2, gate2_volume, gate3_momentum, gate4_quality,
            total_gates_passed, quality_score, recommendation,
            -- ê¸°ìˆ ì  ì§€í‘œ (technical_analysisì—ì„œ, íƒ€ì… ë³€í™˜)
            atr, supertrend, macd_histogram, adx, support_level,
            -- LayeredScoring ë°ì´í„° (makenaide_technical_analysisì—ì„œ)
            macro_score, structural_score, micro_score, total_score,
            quality_gates_passed, analysis_details,
            -- ë©”íƒ€ë°ì´í„°
            source_table
        )
        SELECT
            all_tickers.ticker as ticker,
            COALESCE(t1.analysis_date, DATE(t2.analysis_timestamp)) as analysis_date,
            -- Weinstein Stage ë°ì´í„°
            t1.current_stage,
            t1.stage_confidence,
            t1.ma200_trend,
            t1.ma200_slope,
            t1.price_vs_ma200,
            t1.breakout_strength,
            t1.volume_surge,
            t1.days_in_stage,
            t1.gate1_stage2,
            t1.gate2_volume,
            t1.gate3_momentum,
            t1.gate4_quality,
            t1.total_gates_passed,
            COALESCE(t1.quality_score, t2.quality_score) as quality_score,
            COALESCE(t1.recommendation, t2.recommendation) as recommendation,
            -- ê¸°ìˆ ì  ì§€í‘œ (íƒ€ì… ë³€í™˜)
            CASE
                WHEN t1.atr IS NOT NULL AND t1.atr != ''
                THEN CAST(t1.atr AS REAL)
                ELSE NULL
            END as atr,
            CASE
                WHEN t1.supertrend IS NOT NULL AND t1.supertrend != ''
                THEN CAST(t1.supertrend AS REAL)
                ELSE NULL
            END as supertrend,
            CASE
                WHEN t1.macd_histogram IS NOT NULL AND t1.macd_histogram != ''
                THEN CAST(t1.macd_histogram AS REAL)
                ELSE NULL
            END as macd_histogram,
            t1.adx,
            CASE
                WHEN t1.support_level IS NOT NULL AND t1.support_level != ''
                THEN CAST(t1.support_level AS REAL)
                ELSE NULL
            END as support_level,
            -- LayeredScoring ë°ì´í„°
            t2.macro_score,
            t2.structural_score,
            t2.micro_score,
            t2.total_score,
            t2.quality_gates_passed,
            t2.analysis_details,
            -- ì†ŒìŠ¤ ì¶”ì 
            CASE
                WHEN t1.ticker IS NOT NULL AND t2.ticker IS NOT NULL THEN 'both'
                WHEN t1.ticker IS NOT NULL THEN 'technical_analysis'
                WHEN t2.ticker IS NOT NULL THEN 'makenaide_technical_analysis'
                ELSE 'unknown'
            END as source_table
        FROM (
            SELECT DISTINCT ticker FROM latest_technical
            UNION
            SELECT DISTINCT ticker FROM makenaide_technical_analysis
        ) all_tickers
        LEFT JOIN latest_technical t1 ON all_tickers.ticker = t1.ticker
        LEFT JOIN makenaide_technical_analysis t2 ON all_tickers.ticker = t2.ticker
        """

        cursor = self.conn.execute(migration_query)
        migrated_count = cursor.rowcount

        # Step 3: ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
        self.conn.execute("DROP TABLE latest_technical")

        self.conn.commit()

        # ìµœì¢… ì¹´ìš´íŠ¸ í™•ì¸
        final_count = self.conn.execute("SELECT COUNT(*) FROM technical_analysis_unified").fetchone()[0]
        self.migration_log["target_count"] = final_count

        logger.info(f"âœ… ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {final_count}ê°œ ë ˆì½”ë“œ")

    def validate_migration(self) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
        logger.info("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì¤‘...")

        validation = {}

        # ê¸°ë³¸ í†µê³„
        cursor = self.conn.execute("""
            SELECT
                COUNT(*) as total_count,
                COUNT(DISTINCT ticker) as unique_tickers,
                COUNT(CASE WHEN source_table = 'both' THEN 1 END) as both_sources,
                COUNT(CASE WHEN source_table = 'technical_analysis' THEN 1 END) as technical_only,
                COUNT(CASE WHEN source_table = 'makenaide_technical_analysis' THEN 1 END) as makenaide_only,
                COUNT(CASE WHEN current_stage IS NOT NULL THEN 1 END) as stage_data,
                COUNT(CASE WHEN total_score IS NOT NULL THEN 1 END) as score_data
            FROM technical_analysis_unified
        """)

        row = cursor.fetchone()
        validation["summary"] = {
            "total_records": row["total_count"],
            "unique_tickers": row["unique_tickers"],
            "both_sources": row["both_sources"],
            "technical_only": row["technical_only"],
            "makenaide_only": row["makenaide_only"],
            "stage_data_coverage": f"{row['stage_data']}/{row['total_count']} ({row['stage_data']/row['total_count']*100:.1f}%)",
            "score_data_coverage": f"{row['score_data']}/{row['total_count']} ({row['score_data']/row['total_count']*100:.1f}%)"
        }

        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        cursor = self.conn.execute("""
            SELECT ticker, current_stage, total_score, source_table
            FROM technical_analysis_unified
            ORDER BY total_score DESC NULLS LAST
            LIMIT 5
        """)
        validation["sample_data"] = [dict(row) for row in cursor.fetchall()]

        return validation

    def backup_existing_tables(self) -> None:
        """ê¸°ì¡´ í…Œì´ë¸” ë°±ì—…"""
        logger.info("ğŸ’¾ ê¸°ì¡´ í…Œì´ë¸” ë°±ì—… ì¤‘...")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # technical_analysis ë°±ì—…
        self.conn.execute(f"""
            CREATE TABLE technical_analysis_backup_{timestamp} AS
            SELECT * FROM technical_analysis
        """)

        # makenaide_technical_analysis ë°±ì—…
        self.conn.execute(f"""
            CREATE TABLE makenaide_technical_analysis_backup_{timestamp} AS
            SELECT * FROM makenaide_technical_analysis
        """)

        self.conn.commit()
        logger.info(f"âœ… ë°±ì—… ì™„ë£Œ: *_backup_{timestamp}")

    def replace_tables(self) -> None:
        """í…Œì´ë¸” êµì²´"""
        logger.info("ğŸ”„ í…Œì´ë¸” êµì²´ ì¤‘...")

        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
        self.conn.execute("DROP TABLE technical_analysis")
        self.conn.execute("DROP TABLE makenaide_technical_analysis")

        # í†µí•© í…Œì´ë¸”ì„ technical_analysisë¡œ ë¦¬ë„¤ì„
        self.conn.execute("ALTER TABLE technical_analysis_unified RENAME TO technical_analysis")

        self.conn.commit()
        logger.info("âœ… í…Œì´ë¸” êµì²´ ì™„ë£Œ")

    def run_migration(self) -> Dict[str, Any]:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        self.migration_log["start_time"] = datetime.now().isoformat()

        try:
            logger.info("ğŸš€ ê¸°ìˆ ì  ë¶„ì„ í…Œì´ë¸” í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")

            # 1. ì†ŒìŠ¤ í…Œì´ë¸” ë¶„ì„
            source_analysis = self.analyze_source_tables()
            logger.info(f"ğŸ“Š ì†ŒìŠ¤ ë¶„ì„: {source_analysis}")

            # 2. í†µí•© í…Œì´ë¸” ìƒì„±
            self.create_unified_table()

            # 3. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
            self.migrate_data()

            # 4. ê²€ì¦
            validation = self.validate_migration()
            logger.info(f"âœ… ê²€ì¦ ê²°ê³¼: {validation}")

            # 5. ë°±ì—… ë° êµì²´
            self.backup_existing_tables()
            self.replace_tables()

            self.migration_log["success"] = True
            self.migration_log["validation"] = validation

            logger.info("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µì  ì™„ë£Œ!")

        except Exception as e:
            error_msg = f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}"
            logger.error(f"âŒ {error_msg}")
            self.migration_log["errors"].append(error_msg)
            raise

        finally:
            self.migration_log["end_time"] = datetime.now().isoformat()

        return self.migration_log

    def save_migration_log(self) -> None:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ì €ì¥"""
        log_filename = f"migration_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(log_filename, 'w', encoding='utf-8') as f:
            json.dump(self.migration_log, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ì €ì¥: {log_filename}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ ê¸°ìˆ ì  ë¶„ì„ í…Œì´ë¸” í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 50)

    try:
        with TechnicalAnalysisMigrator() as migrator:
            # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
            result = migrator.run_migration()

            # ë¡œê·¸ ì €ì¥
            migrator.save_migration_log()

            # ê²°ê³¼ ì¶œë ¥
            print("\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
            print(f"â° ì‹¤í–‰ ì‹œê°„: {result['start_time']} ~ {result['end_time']}")
            print(f"ğŸ“ˆ ì„±ê³µ ì—¬ë¶€: {'âœ… ì„±ê³µ' if result['success'] else 'âŒ ì‹¤íŒ¨'}")
            print(f"ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë ˆì½”ë“œ: {result['target_count']}ê°œ")

            if result.get('validation'):
                validation = result['validation']['summary']
                print(f"ğŸ¯ ê²€ì¦ ê²°ê³¼:")
                print(f"  - ì´ ë ˆì½”ë“œ: {validation['total_records']}")
                print(f"  - ê³ ìœ  ì¢…ëª©: {validation['unique_tickers']}")
                print(f"  - ì–‘ìª½ ì†ŒìŠ¤: {validation['both_sources']}")
                print(f"  - Stage ë°ì´í„°: {validation['stage_data_coverage']}")
                print(f"  - Score ë°ì´í„°: {validation['score_data_coverage']}")

    except Exception as e:
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())