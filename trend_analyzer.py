import psycopg2  # ì¼ë¶€ í•¨ìˆ˜ì—ì„œ ì§ì ‘ ì‚¬ìš©
import json
from openai import OpenAI, OpenAIError
import os
import logging
import logging.handlers
import re
import gzip
import shutil
from dotenv import load_dotenv
load_dotenv()
import base64
import time
from openai import RateLimitError
import tiktoken
import textwrap
import hashlib
import zlib
import threading
from functools import wraps
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback
from datetime import datetime, timedelta
import numpy as np
from config_loader import load_config
from utils import validate_and_correct_phase
from db_manager import DBManager
from strategy_analyzer import detect_vcp_pattern, analyze_weinstein_stage, check_breakout_conditions, optimized_integrated_analysis
from data_fetcher import generate_gpt_analysis_json

# === ê²¬ê³ í•œ ì˜ˆì™¸ ì²˜ë¦¬ ì‹œìŠ¤í…œ ===

class AnalysisException(Exception):
    """ë¶„ì„ ê´€ë ¨ ì»¤ìŠ¤í…€ ì˜ˆì™¸"""
    def __init__(self, message: str, error_code: str = "ANALYSIS_ERROR", ticker: str = "", recovery_suggestion: str = ""):
        self.message = message
        self.error_code = error_code
        self.ticker = ticker
        self.recovery_suggestion = recovery_suggestion
        super().__init__(self.message)

class RobustAnalysisCircuitBreaker:
    """ë¶„ì„ ì‹œìŠ¤í…œ íšŒë¡œ ì°¨ë‹¨ê¸°"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 300):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = 0
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        self._lock = threading.Lock()
    
    def call(self, func, *args, **kwargs):
        """íšŒë¡œ ì°¨ë‹¨ê¸°ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""
        with self._lock:
            if self.state == "OPEN":
                if time.time() - self.last_failure_time > self.recovery_timeout:
                    self.state = "HALF_OPEN"
                    logging.info("ğŸ”„ íšŒë¡œ ì°¨ë‹¨ê¸° ìƒíƒœ: HALF_OPEN")
                else:
                    raise AnalysisException(
                        "íšŒë¡œ ì°¨ë‹¨ê¸°ê°€ ì—´ë ¤ìˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ë³µêµ¬ ëŒ€ê¸° ì¤‘",
                        "CIRCUIT_BREAKER_OPEN",
                        recovery_suggestion="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ëŒ€ì²´ ë¶„ì„ ë°©ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”"
                    )
            
            try:
                result = func(*args, **kwargs)
                if self.state == "HALF_OPEN":
                    self.state = "CLOSED"
                    self.failure_count = 0
                    logging.info("âœ… íšŒë¡œ ì°¨ë‹¨ê¸° ìƒíƒœ: CLOSED (ë³µêµ¬ ì™„ë£Œ)")
                return result
                
            except Exception as e:
                self.failure_count += 1
                self.last_failure_time = time.time()
                
                if self.failure_count >= self.failure_threshold:
                    self.state = "OPEN"
                    logging.error(f"ğŸš¨ íšŒë¡œ ì°¨ë‹¨ê¸° ì—´ë¦¼: {self.failure_count}íšŒ ì—°ì† ì‹¤íŒ¨")
                
                raise e

# ì „ì—­ íšŒë¡œ ì°¨ë‹¨ê¸° ì¸ìŠ¤í„´ìŠ¤
analysis_circuit_breaker = RobustAnalysisCircuitBreaker()

# UNUSED: í˜¸ì¶œë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜
# def robust_analysis_pipeline(ticker: str, daily_data: dict):
#     """
#     fail-safe ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
#     
#     ìš”êµ¬ì‚¬í•­:
#     - ê° ë¶„ì„ ë‹¨ê³„ë³„ ë…ë¦½ì  ì—ëŸ¬ ì²˜ë¦¬
#     - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ê°€ìš©í•œ ê²°ê³¼ë¡œ ë¶„ì„ ê³„ì†
#     - ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ë¯¸ìˆëŠ” fallback ê°’ ì œê³µ
#     - ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…ê³¼ ë³µêµ¬ ì œì•ˆ
#     """
#     analysis_results = {
#         "ticker": ticker,
#         "analysis_timestamp": datetime.now().isoformat(),
#         "pipeline_status": "RUNNING",
#         "stages_completed": [],
#         "stages_failed": [],
#         "fallback_used": [],
#         "final_recommendation": None,
#         "error_summary": [],
#         "recovery_suggestions": []
#     }
#     
#     try:
#         # 1ë‹¨ê³„: ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
#         try:
#             validated_data = _validate_and_normalize_data(ticker, daily_data)
#             analysis_results["stages_completed"].append("data_validation")
#             logging.info(f"âœ… {ticker} - ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
#         except Exception as e:
#             error_detail = _handle_pipeline_error("data_validation", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             validated_data = _get_fallback_data(daily_data)
#             analysis_results["fallback_used"].append("minimal_data_fallback")
#             logging.warning(f"âš ï¸ {ticker} - ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨, ìµœì†Œ ë°ì´í„°ë¡œ ì§„í–‰")
#         
#         # 2ë‹¨ê³„: í†µí•© ê¸°ìˆ ì  ë¶„ì„ (íšŒë¡œì°¨ë‹¨ê¸° ì ìš©)
#         try:
#             technical_analysis = analysis_circuit_breaker.call(
#                 optimized_integrated_analysis, validated_data
#             )
#             analysis_results["stages_completed"].append("technical_analysis")
#             logging.info(f"âœ… {ticker} - í†µí•© ê¸°ìˆ ì  ë¶„ì„ ì™„ë£Œ")
#         except Exception as e:
#             error_detail = _handle_pipeline_error("technical_analysis", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             technical_analysis = _get_fallback_technical_analysis(validated_data)
#             analysis_results["fallback_used"].append("basic_technical_fallback")
#             logging.warning(f"âš ï¸ {ticker} - ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì§„í–‰")
#         
#         # 3ë‹¨ê³„: VCP íŒ¨í„´ ë¶„ì„
#         try:
#             vcp_analysis = _safe_vcp_analysis(validated_data, ticker)
#             analysis_results["stages_completed"].append("vcp_analysis")
#         except Exception as e:
#             error_detail = _handle_pipeline_error("vcp_analysis", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             vcp_analysis = _get_fallback_vcp_analysis()
#             analysis_results["fallback_used"].append("default_vcp_fallback")
#         
#         # 4ë‹¨ê³„: Weinstein Stage ë¶„ì„
#         try:
#             stage_analysis = _safe_stage_analysis(validated_data, ticker)
#             analysis_results["stages_completed"].append("stage_analysis")
#         except Exception as e:
#             error_detail = _handle_pipeline_error("stage_analysis", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             stage_analysis = _get_fallback_stage_analysis()
#             analysis_results["fallback_used"].append("default_stage_fallback")
#         
#         # 5ë‹¨ê³„: ë¸Œë ˆì´í¬ì•„ì›ƒ ì¡°ê±´ í™•ì¸
#         try:
#             breakout_analysis = _safe_breakout_analysis(validated_data, vcp_analysis, stage_analysis, ticker)
#             analysis_results["stages_completed"].append("breakout_analysis")
#         except Exception as e:
#             error_detail = _handle_pipeline_error("breakout_analysis", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             breakout_analysis = _get_fallback_breakout_analysis()
#             analysis_results["fallback_used"].append("default_breakout_fallback")
#         
#         # 6ë‹¨ê³„: ìµœì¢… í†µí•© ë° ì˜ì‚¬ê²°ì •
#         try:
#             final_decision = _safe_final_decision(
#                 technical_analysis, vcp_analysis, stage_analysis, breakout_analysis, ticker
#             )
#             analysis_results["stages_completed"].append("final_decision")
#             analysis_results["final_recommendation"] = final_decision
#         except Exception as e:
#             error_detail = _handle_pipeline_error("final_decision", e, ticker)
#             analysis_results["stages_failed"].append(error_detail)
#             final_decision = _get_fallback_final_decision()
#             analysis_results["final_recommendation"] = final_decision
#             analysis_results["fallback_used"].append("conservative_decision_fallback")
#         
#         # íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì—…ë°ì´íŠ¸
#         if len(analysis_results["stages_failed"]) == 0:
#             analysis_results["pipeline_status"] = "SUCCESS"
#         elif len(analysis_results["stages_completed"]) >= 3:
#             analysis_results["pipeline_status"] = "PARTIAL_SUCCESS"
#         else:
#             analysis_results["pipeline_status"] = "FAILED"
#         
#         # ë³µêµ¬ ì œì•ˆ ìƒì„±
#         analysis_results["recovery_suggestions"] = _generate_recovery_suggestions(
#             analysis_results["stages_failed"], analysis_results["fallback_used"]
#         )
#         
#         logging.info(f"ğŸ¯ {ticker} - ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {analysis_results['pipeline_status']}")
#         
#         return {
#             "analysis_results": analysis_results,
#             "technical_analysis": technical_analysis,
#             "vcp_analysis": vcp_analysis,
#             "stage_analysis": stage_analysis,
#             "breakout_analysis": breakout_analysis,
#             "final_decision": final_decision
#         }
#         
#     except Exception as e:
#         # ìµœì¢… ì•ˆì „ë§
#         logging.error(f"âŒ {ticker} - ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤íŒ¨: {str(e)}")
#         analysis_results["pipeline_status"] = "CRITICAL_FAILURE"
#         analysis_results["error_summary"].append({
#             "stage": "pipeline_level",
#             "error": str(e),
#             "timestamp": datetime.now().isoformat()
#         })
#         
#         return {
#             "analysis_results": analysis_results,
#             "technical_analysis": _get_fallback_technical_analysis({}),
#             "vcp_analysis": _get_fallback_vcp_analysis(),
#             "stage_analysis": _get_fallback_stage_analysis(),
#             "breakout_analysis": _get_fallback_breakout_analysis(),
#             "final_decision": _get_fallback_final_decision()
#         }

def _validate_and_normalize_data(ticker: str, daily_data: dict) -> dict:
    """ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”"""
    if not daily_data:
        raise AnalysisException(
            "ì¼ì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
            "NO_DATA",
            ticker,
            "ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"
        )
    
    required_fields = ['close', 'high', 'low', 'volume']
    missing_fields = [field for field in required_fields if field not in daily_data]
    
    if missing_fields:
        raise AnalysisException(
            f"í•„ìˆ˜ ë°ì´í„° í•„ë“œ ëˆ„ë½: {missing_fields}",
            "MISSING_FIELDS",
            ticker,
            f"ëˆ„ë½ëœ í•„ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ëŒ€ì²´ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
        )
    
    # ë°ì´í„° ê¸¸ì´ ê²€ì¦
    min_length = 20
    for field in required_fields:
        if len(daily_data[field]) < min_length:
            raise AnalysisException(
                f"{field} ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {min_length}ì¼ í•„ìš”)",
                "INSUFFICIENT_DATA",
                ticker,
                f"ë” ë§ì€ ê³¼ê±° ë°ì´í„°ë¥¼ í™•ë³´í•˜ì„¸ìš”"
            )
    
    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    for field in required_fields:
        if any(val <= 0 for val in daily_data[field] if val is not None):
            logging.warning(f"âš ï¸ {ticker} - {field}ì— ë¹„ì •ìƒì ì¸ ê°’ ë°œê²¬")
    
    return daily_data

def _handle_pipeline_error(stage: str, error: Exception, ticker: str) -> dict:
    """íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ì²˜ë¦¬"""
    error_detail = {
        "stage": stage,
        "error_type": type(error).__name__,
        "error_message": str(error),
        "ticker": ticker,
        "timestamp": datetime.now().isoformat(),
        "traceback": traceback.format_exc()[-500:]  # ë§ˆì§€ë§‰ 500ìë§Œ
    }
    
    # ì—ëŸ¬ ë¶„ë¥˜ ë° ë³µêµ¬ ì œì•ˆ
    if isinstance(error, AnalysisException):
        error_detail["recovery_suggestion"] = error.recovery_suggestion
        error_detail["error_code"] = error.error_code
    elif "connection" in str(error).lower():
        error_detail["recovery_suggestion"] = "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"
        error_detail["error_code"] = "CONNECTION_ERROR"
    elif "timeout" in str(error).lower():
        error_detail["recovery_suggestion"] = "ì‹œìŠ¤í…œ ë¶€í•˜ê°€ ë†’ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"
        error_detail["error_code"] = "TIMEOUT_ERROR"
    else:
        error_detail["recovery_suggestion"] = "ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"
        error_detail["error_code"] = "UNKNOWN_ERROR"
    
    logging.error(f"âŒ {ticker} - {stage} ë‹¨ê³„ ì‹¤íŒ¨: {error_detail['error_message']}")
    return error_detail

def _safe_vcp_analysis(data: dict, ticker: str) -> dict:
    """ì•ˆì „í•œ VCP ë¶„ì„"""
    try:
        return detect_vcp_pattern(data)
    except Exception as e:
        logging.warning(f"âš ï¸ {ticker} - VCP ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return _get_fallback_vcp_analysis()

def _safe_stage_analysis(data: dict, ticker: str) -> dict:
    """ì•ˆì „í•œ Stage ë¶„ì„"""
    try:
        return analyze_weinstein_stage(data)
    except Exception as e:
        logging.warning(f"âš ï¸ {ticker} - Stage ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return _get_fallback_stage_analysis()

def _safe_breakout_analysis(data: dict, vcp_analysis: dict, stage_analysis: dict, ticker: str) -> dict:
    """ì•ˆì „í•œ ë¸Œë ˆì´í¬ì•„ì›ƒ ë¶„ì„"""
    try:
        return check_breakout_conditions(data, vcp_analysis, stage_analysis)
    except Exception as e:
        logging.warning(f"âš ï¸ {ticker} - ë¸Œë ˆì´í¬ì•„ì›ƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return _get_fallback_breakout_analysis()

def _safe_final_decision(technical_analysis: dict, vcp_analysis: dict, stage_analysis: dict, 
                        breakout_analysis: dict, ticker: str) -> dict:
    """ì•ˆì „í•œ ìµœì¢… ì˜ì‚¬ê²°ì •"""
    try:
        # í†µí•© ë¶„ì„ ê²°ê³¼ì—ì„œ ìµœì¢… ê²°ì • ì¶”ì¶œ
        if "final_decision" in technical_analysis:
            return technical_analysis["final_decision"]
        
        # ê°œë³„ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê²°ì •
        vcp_score = vcp_analysis.get("score", 30)
        stage_favorable = stage_analysis.get("current_stage") == "2"
        breakout_ready = breakout_analysis.get("breakout_ready", False)
        
        if vcp_score > 70 and stage_favorable and breakout_ready:
            action = "BUY"
            confidence = 0.8
        elif vcp_score > 50 and (stage_favorable or breakout_ready):
            action = "BUY"
            confidence = 0.6
        elif vcp_score > 30:
            action = "HOLD"
            confidence = 0.4
        else:
            action = "SELL"
            confidence = 0.3
        
        return {
            "action": action,
            "confidence": confidence,
            "reasoning": f"VCP: {vcp_score}, Stage: {stage_analysis.get('current_stage')}, Breakout: {breakout_ready}"
        }
        
    except Exception as e:
        logging.warning(f"âš ï¸ {ticker} - ìµœì¢… ì˜ì‚¬ê²°ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return _get_fallback_final_decision()

# === ê³ ë„í™”ëœ í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ===

# UNUSED: í˜¸ì¶œë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜
# def advanced_scaling_in_system(initial_entry: dict, market_conditions: dict):
#     """
#     Makenaide ë¶„í•  ë§¤ìˆ˜ ì „ëµ ì™„ì „ êµ¬í˜„
#     
#     ê¸°ëŠ¥:
#     - ì´ˆê¸° ì§„ì… í›„ ì¶”ì„¸ ê°•í™” ì‹œì  ê°ì§€
#     - VCP ì´í›„ ì¶”ê°€ ë¸Œë ˆì´í¬ì•„ì›ƒ ë ˆë²¨ ê³„ì‚°
#     - ê±°ë˜ëŸ‰ í™•ì¸ê³¼ ëª¨ë©˜í…€ ì§€ì†ì„± ê²€ì¦
#     - ìµœëŒ€ 3-4íšŒ ì¶”ê°€ ì§„ì… ì¡°ê±´ ì„¤ì •
#     - ê° ì¶”ê°€ ì§„ì… ì‹œ í¬ì§€ì…˜ í¬ê¸° ë™ì  ì¡°ì •
#     
#     ë¦¬ìŠ¤í¬ ì œì–´:
#     - ì „ì²´ í¬ì§€ì…˜ì´ ìµœëŒ€ í•œë„ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨
#     - ì¶”ì„¸ ì•½í™” ì‹ í˜¸ ê°ì§€ ì‹œ ì¡°ê¸° ì²­ì‚°
#     - ê° ë‹¨ê³„ë³„ ë…ë¦½ì  ì†ì ˆì„  ì„¤ì •
#     """
#     try:
#         ticker = initial_entry.get("ticker", "")
#         entry_price = initial_entry.get("price", 0)
#         entry_date = initial_entry.get("date", datetime.now())
#         initial_size = initial_entry.get("position_size_pct", 2.0)
#         
#         # í”¼ë¼ë¯¸ë”© ì„¤ì •
#         pyramid_config = {
#             "max_total_position": 8.0,  # ìµœëŒ€ ì´ í¬ì§€ì…˜ 8%
#             "max_scaling_levels": 3,    # ìµœëŒ€ 3íšŒ ì¶”ê°€ ë§¤ìˆ˜
#             "min_advance_threshold": 0.02,  # ìµœì†Œ 2% ìƒìŠ¹ í›„ ì¶”ê°€ ë§¤ìˆ˜
#             "volume_surge_threshold": 1.5,  # ê±°ë˜ëŸ‰ 1.5ë°° ì´ìƒ ì¦ê°€
#             "trend_strength_threshold": 0.7,  # ì¶”ì„¸ ê°•ë„ 0.7 ì´ìƒ
#             "max_time_between_entries": 14  # ìµœëŒ€ 14ì¼ ê°„ê²©
#         }
#         
#         # í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„
#         current_analysis = _analyze_current_market_conditions(market_conditions, ticker)
#         
#         # í”¼ë¼ë¯¸ë”© ì¡°ê±´ í™•ì¸
#         pyramid_conditions = _check_pyramid_conditions(
#             initial_entry, current_analysis, pyramid_config
#         )
#         
#         if not pyramid_conditions["enabled"]:
#             return {
#                 "pyramid_enabled": False,
#                 "reason": pyramid_conditions["reason"],
#                 "current_position": {"total_size": initial_size, "levels": 1},
#                 "next_actions": ["ëª¨ë‹ˆí„°ë§ ê³„ì†"]
#             }
#         
#         # ì¶”ê°€ ì§„ì… ë ˆë²¨ ê³„ì‚°
#         scaling_levels = _calculate_scaling_levels(
#             entry_price, current_analysis, pyramid_config
#         )
#         
#         # ë™ì  í¬ì§€ì…˜ ì‚¬ì´ì§•
#         position_sizes = _calculate_dynamic_position_sizes(
#             initial_size, scaling_levels, current_analysis, pyramid_config
#         )
#         
#         # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë ˆë²¨ ì„¤ì •
#         risk_management = _setup_pyramid_risk_management(
#             entry_price, scaling_levels, position_sizes
#         )
#         
#         # ì‹¤í–‰ ê³„íš ìƒì„±
#         execution_plan = _generate_execution_plan(
#             scaling_levels, position_sizes, risk_management, current_analysis
#         )
#         
#         return {
#             "pyramid_enabled": True,
#             "initial_entry": initial_entry,
#             "scaling_levels": scaling_levels,
#             "position_sizes": position_sizes,
#             "risk_management": risk_management,
#             "execution_plan": execution_plan,
#             "monitoring_alerts": _setup_monitoring_alerts(scaling_levels, risk_management),
#             "exit_conditions": _define_exit_conditions(current_analysis, risk_management)
#         }
#         
#     except Exception as e:
#         logging.error(f"âŒ í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")
#         return _get_fallback_pyramid_result(initial_entry)

# UNUSED: í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ë‚´ë¶€ í•¨ìˆ˜ë“¤ (advanced_scaling_in_systemì—ì„œë§Œ ì‚¬ìš©)
# def _analyze_current_market_conditions(market_conditions: dict, ticker: str) -> dict:
#     """í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„"""
#     try:
#         # ê¸°ë³¸ ì‹œì¥ ì§€í‘œë“¤
#         market_trend = market_conditions.get("market_trend", "neutral")
#         volatility = market_conditions.get("volatility", 0.2)
#         sector_performance = market_conditions.get("sector_performance", 0.0)
#         
#         # ì¢…ëª©ë³„ ëª¨ë©˜í…€ ë¶„ì„
#         price_momentum = _calculate_price_momentum(market_conditions.get("price_data", []))
#         volume_momentum = _calculate_volume_momentum(market_conditions.get("volume_data", []))
#         
#         # ìƒëŒ€ ê°•ë„ ê³„ì‚°
#         relative_strength = _calculate_relative_strength_vs_market(
#             market_conditions.get("price_data", []),
#             market_conditions.get("market_index_data", [])
#         )
#         
#         # ì¶”ì„¸ ê°•ë„ ì ìˆ˜
#         trend_strength = _calculate_trend_strength(
#             price_momentum, volume_momentum, relative_strength
#         )
#         
#         return {
#             "market_trend": market_trend,
#             "volatility": volatility,
#             "sector_performance": sector_performance,
#             "price_momentum": price_momentum,
#             "volume_momentum": volume_momentum,
#             "relative_strength": relative_strength,
#             "trend_strength": trend_strength,
#             "favorable_for_pyramid": trend_strength > 0.7 and volatility < 0.3
#         }
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ ì‹œì¥ ìƒí™© ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
#         return {
#             "market_trend": "neutral",
#             "trend_strength": 0.5,
#             "favorable_for_pyramid": False
#         }

# def _check_pyramid_conditions(initial_entry: dict, current_analysis: dict, config: dict) -> dict:
#     """í”¼ë¼ë¯¸ë”© ì¡°ê±´ í™•ì¸"""
#     try:
#         entry_price = initial_entry.get("price", 0)
#         current_price = current_analysis.get("current_price", entry_price)
#         
#         # ê¸°ë³¸ ì¡°ê±´ë“¤
#         conditions = {
#             "price_advance": (current_price - entry_price) / entry_price >= config["min_advance_threshold"],
#             "trend_strength": current_analysis.get("trend_strength", 0) >= config["trend_strength_threshold"],
#             "market_favorable": current_analysis.get("favorable_for_pyramid", False),
#             "volume_confirmation": current_analysis.get("volume_momentum", 0) > 0,
#             "time_constraint": True  # ì‹œê°„ ì œì•½ ì²´í¬ (ì‹¤ì œë¡œëŠ” ë‚ ì§œ ê³„ì‚°)
#         }
#         
#         # ëª¨ë“  ì¡°ê±´ ë§Œì¡± í™•ì¸
#         all_conditions_met = all(conditions.values())
#         
#         if not all_conditions_met:
#             failed_conditions = [k for k, v in conditions.items() if not v]
#             reason = f"í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡±: {', '.join(failed_conditions)}"
#         else:
#             reason = "ëª¨ë“  í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë§Œì¡±"
#         
#         return {
#             "enabled": all_conditions_met,
#             "reason": reason,
#             "conditions": conditions,
#             "confidence": sum(conditions.values()) / len(conditions)
#         }
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ í”¼ë¼ë¯¸ë”© ì¡°ê±´ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
#         return {"enabled": False, "reason": "ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨", "confidence": 0}

# def _calculate_scaling_levels(entry_price: float, current_analysis: dict, config: dict) -> list:
#     """ì¶”ê°€ ì§„ì… ë ˆë²¨ ê³„ì‚°"""
#     try:
#         scaling_levels = []
#         trend_strength = current_analysis.get("trend_strength", 0.5)
#         
#         # ì¶”ì„¸ ê°•ë„ì— ë”°ë¥¸ ë ˆë²¨ ê°„ê²© ì¡°ì •
#         if trend_strength > 0.8:
#             # ê°•í•œ ì¶”ì„¸: ë” ì ê·¹ì ì¸ ë ˆë²¨
#             level_intervals = [0.03, 0.05, 0.08]  # 3%, 5%, 8% ìƒìŠ¹ ì§€ì 
#         elif trend_strength > 0.6:
#             # ì¤‘ê°„ ì¶”ì„¸: ë³´í†µ ë ˆë²¨
#             level_intervals = [0.04, 0.07, 0.12]  # 4%, 7%, 12% ìƒìŠ¹ ì§€ì 
#         else:
#             # ì•½í•œ ì¶”ì„¸: ë³´ìˆ˜ì  ë ˆë²¨
#             level_intervals = [0.05, 0.10, 0.15]  # 5%, 10%, 15% ìƒìŠ¹ ì§€ì 
#         
#         for i, interval in enumerate(level_intervals[:config["max_scaling_levels"]]):
#             level_price = entry_price * (1 + interval)
#             
#             scaling_levels.append({
#                 "level": i + 1,
#                 "price": round(level_price, 2),
#                 "advance_pct": round(interval * 100, 1),
#                 "trigger_conditions": _define_trigger_conditions(i + 1, current_analysis),
#                 "size_allocation": _calculate_level_size_allocation(i + 1, trend_strength)
#             })
#         
#         return scaling_levels
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ ìŠ¤ì¼€ì¼ë§ ë ˆë²¨ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
#         return []

# def _define_trigger_conditions(level: int, analysis: dict) -> dict:
#     """ê° ë ˆë²¨ë³„ íŠ¸ë¦¬ê±° ì¡°ê±´ ì •ì˜"""
#     base_conditions = {
#         "price_breakout": True,  # ê°€ê²© ëŒíŒŒ í•„ìˆ˜
#         "volume_surge": level <= 2,  # 1-2ë ˆë²¨ì€ ê±°ë˜ëŸ‰ ê¸‰ì¦ í•„ìš”
#         "momentum_continuation": True,  # ëª¨ë©˜í…€ ì§€ì† í•„ìˆ˜
#         "market_support": level >= 3  # 3ë ˆë²¨ë¶€í„°ëŠ” ì‹œì¥ ì§€ì§€ í•„ìš”
#     }
#     
#     # ë ˆë²¨ë³„ ì¶”ê°€ ì¡°ê±´
#     if level == 1:
#         base_conditions["min_volume_ratio"] = 1.3
#         base_conditions["min_rsi"] = 55
#     elif level == 2:
#         base_conditions["min_volume_ratio"] = 1.2
#         base_conditions["min_rsi"] = 60
#     elif level == 3:
#         base_conditions["min_volume_ratio"] = 1.1
#         base_conditions["min_rsi"] = 65
#         base_conditions["sector_outperformance"] = True
#     
#     return base_conditions

# UNUSED: í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ë‚´ë¶€ í•¨ìˆ˜ë“¤ (advanced_scaling_in_systemì—ì„œë§Œ ì‚¬ìš©)
# def _calculate_dynamic_position_sizes(initial_size: float, scaling_levels: list, 
#                                     analysis: dict, config: dict) -> dict:
#     """ë™ì  í¬ì§€ì…˜ ì‚¬ì´ì§• ê³„ì‚°"""
#     try:
#         trend_strength = analysis.get("trend_strength", 0.5)
#         volatility = analysis.get("volatility", 0.2)
#         
#         # ê¸°ë³¸ ì‚¬ì´ì¦ˆ ë°°ë¶„ (ì´ˆê¸° í¬ì§€ì…˜ ê¸°ì¤€)
#         base_allocations = {
#             1: 0.6,  # ì²« ë²ˆì§¸ ì¶”ê°€: ì´ˆê¸°ì˜ 60%
#             2: 0.4,  # ë‘ ë²ˆì§¸ ì¶”ê°€: ì´ˆê¸°ì˜ 40%
#             3: 0.3   # ì„¸ ë²ˆì§¸ ì¶”ê°€: ì´ˆê¸°ì˜ 30%
#         }
#         
#         position_sizes = {"initial": initial_size, "levels": {}}
#         remaining_capacity = config["max_total_position"] - initial_size
#         
#         for level_info in scaling_levels:
#             level = level_info["level"]
#             base_allocation = base_allocations.get(level, 0.2)
#             
#             # ì¶”ì„¸ ê°•ë„ì— ë”°ë¥¸ ì¡°ì •
#             trend_multiplier = 0.8 + (trend_strength * 0.4)  # 0.8 ~ 1.2
#             
#             # ë³€ë™ì„±ì— ë”°ë¥¸ ì¡°ì •
#             volatility_multiplier = max(0.6, 1.2 - volatility)  # ë³€ë™ì„± ë†’ì„ìˆ˜ë¡ ê°ì†Œ
#             
#             # ìµœì¢… ì‚¬ì´ì¦ˆ ê³„ì‚°
#             adjusted_size = (initial_size * base_allocation * 
#                            trend_multiplier * volatility_multiplier)
#             
#             # ì”ì—¬ í•œë„ í™•ì¸
#             if sum(position_sizes["levels"].values()) + adjusted_size <= remaining_capacity:
#                 position_sizes["levels"][level] = round(adjusted_size, 2)
#             else:
#                 # ì”ì—¬ í•œë„ ë‚´ì—ì„œ ìµœëŒ€ì¹˜ í• ë‹¹
#                 remaining = remaining_capacity - sum(position_sizes["levels"].values())
#                 if remaining > 0.5:  # ìµœì†Œ 0.5% ì´ìƒì¼ ë•Œë§Œ í• ë‹¹
#                     position_sizes["levels"][level] = round(remaining, 2)
#                 break
#         
#         # ì´ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
#         total_size = initial_size + sum(position_sizes["levels"].values())
#         position_sizes["total"] = round(total_size, 2)
#         position_sizes["utilization_pct"] = round(total_size / config["max_total_position"] * 100, 1)
#         
#         return position_sizes
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ ë™ì  í¬ì§€ì…˜ ì‚¬ì´ì§• ì˜¤ë¥˜: {str(e)}")
#         return {"initial": initial_size, "levels": {}, "total": initial_size}

# UNUSED: í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ë‚´ë¶€ í•¨ìˆ˜ë“¤ (advanced_scaling_in_systemì—ì„œë§Œ ì‚¬ìš©)
# def _setup_pyramid_risk_management(entry_price: float, scaling_levels: list, position_sizes: dict) -> dict:
#     """í”¼ë¼ë¯¸ë”© ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì •"""
#     try:
#         risk_management = {
#             "stop_loss_levels": {},
#             "take_profit_levels": {},
#             "trailing_stops": {},
#             "exit_signals": {},
#             "max_loss_limit": position_sizes.get("total", 2.0) * 0.02  # ì´ í¬ì§€ì…˜ì˜ 2%
#         }
#         
#         # ê° ë ˆë²¨ë³„ ì†ì ˆì„  ì„¤ì •
#         for level_info in scaling_levels:
#             level = level_info["level"]
#             level_price = level_info["price"]
#             
#             # ë…ë¦½ì  ì†ì ˆì„  (ê° ë ˆë²¨ ì§„ì…ê°€ì˜ 3% í•˜ë½)
#             stop_loss = level_price * 0.97
#             
#             # íŠ¸ë ˆì¼ë§ ìŠ¤í†± (ATR ê¸°ë°˜)
#             trailing_stop_distance = level_price * 0.04  # 4% íŠ¸ë ˆì¼ë§
#             
#             # ëª©í‘œ ìˆ˜ìµ ë ˆë²¨
#             take_profit_1 = level_price * 1.06  # 6% ìˆ˜ìµ
#             take_profit_2 = level_price * 1.12  # 12% ìˆ˜ìµ
#             
#             risk_management["stop_loss_levels"][level] = round(stop_loss, 2)
#             risk_management["trailing_stops"][level] = round(trailing_stop_distance, 2)
#             risk_management["take_profit_levels"][level] = [
#                 round(take_profit_1, 2),
#                 round(take_profit_2, 2)
#             ]
#         
#         # ì „ì²´ í¬ì§€ì…˜ ë³´í˜¸ ë ˆë²¨
#         avg_entry_price = _calculate_weighted_average_entry(entry_price, scaling_levels, position_sizes)
#         risk_management["portfolio_stop_loss"] = round(avg_entry_price * 0.92, 2)  # 8% ì†ì ˆ
#         risk_management["emergency_exit_level"] = round(avg_entry_price * 0.88, 2)  # 12% ê¸´ê¸‰ ì²­ì‚°
#         
#         return risk_management
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
#         return {"stop_loss_levels": {}, "max_loss_limit": 2.0}

# def _calculate_weighted_average_entry(initial_price: float, levels: list, sizes: dict) -> float:
#     """ê°€ì¤‘í‰ê·  ì§„ì…ê°€ ê³„ì‚°"""
#     try:
#         total_investment = initial_price * sizes.get("initial", 2.0)
#         total_shares = sizes.get("initial", 2.0)
#         
#         for level_info in levels:
#             level = level_info["level"]
#             if level in sizes.get("levels", {}):
#                 level_price = level_info["price"]
#                 level_size = sizes["levels"][level]
#                 
#                 total_investment += level_price * level_size
#                 total_shares += level_size
#         
#         return total_investment / total_shares if total_shares > 0 else initial_price
#         
#     except Exception:
#         return initial_price

# === í—¬í¼ í•¨ìˆ˜ë“¤ ===

def _get_fallback_data(original_data: dict) -> dict:
    """ìµœì†Œ ë°ì´í„° fallback"""
    return {
        "close": original_data.get("close", [100] * 20),
        "high": original_data.get("high", [105] * 20),
        "low": original_data.get("low", [95] * 20),
        "volume": original_data.get("volume", [1000] * 20)
    }

def _get_fallback_technical_analysis(data: dict) -> dict:
    """ê¸°ë³¸ ê¸°ìˆ ì  ë¶„ì„ fallback"""
    return {
        "vcp_volume_volatility": {"vcp_score": 40, "vcp_present": False},
        "stage_ma_momentum": {"current_stage": "1", "stage_confidence": 0.4},
        "breakout_risk_position": {"breakout_probability": 0.3, "position_size_pct": 1.0},
        "unified_scoring": {"unified_score": 45, "confidence": 0.4},
        "final_decision": {"action": "HOLD", "confidence": 0.4}
    }

def _get_fallback_vcp_analysis() -> dict:
    return {"vcp_present": False, "score": 30, "contractions": 0, "analysis_details": {}}

def _get_fallback_stage_analysis() -> dict:
    return {"current_stage": "1", "confidence": 0.4, "stage_strength": 0.3}

def _get_fallback_breakout_analysis() -> dict:
    return {"breakout_ready": False, "probability": 0.3, "resistance_level": 0}

def _get_fallback_final_decision() -> dict:
    return {"action": "HOLD", "confidence": 0.3, "reasoning": "ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ ë³´ìˆ˜ì  íŒë‹¨"}

def _generate_recovery_suggestions(failed_stages: list, fallbacks_used: list) -> list:
    """ë³µêµ¬ ì œì•ˆ ìƒì„±"""
    suggestions = []
    
    if "data_validation" in [stage["stage"] for stage in failed_stages]:
        suggestions.append("ë°ì´í„° í’ˆì§ˆì„ ê°œì„ í•˜ê³  ëˆ„ë½ëœ í•„ë“œë¥¼ ë³´ì™„í•˜ì„¸ìš”")
    
    if "technical_analysis" in [stage["stage"] for stage in failed_stages]:
        suggestions.append("ì‹œìŠ¤í…œ ë¶€í•˜ë¥¼ í™•ì¸í•˜ê³  ë¶„ì„ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”")
    
    if len(fallbacks_used) > 2:
        suggestions.append("ë‹¤ìˆ˜ì˜ fallbackì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²´ ì‹œìŠ¤í…œ ì ê²€ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
    
    return suggestions

# === í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ í—¬í¼ í•¨ìˆ˜ë“¤ ===

def _calculate_price_momentum(price_data: list) -> float:
    """ê°€ê²© ëª¨ë©˜í…€ ê³„ì‚°"""
    try:
        if len(price_data) < 20:
            return 0.5
        
        prices = np.array(price_data)
        
        # ë‹¨ê¸° vs ì¤‘ê¸° ëª¨ë©˜í…€
        short_momentum = (prices[-5:].mean() - prices[-10:-5].mean()) / prices[-10:-5].mean()
        medium_momentum = (prices[-10:].mean() - prices[-20:].mean()) / prices[-20:].mean()
        
        # ê°€ì¤‘ í‰ê·  (ë‹¨ê¸° ëª¨ë©˜í…€ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        combined_momentum = (short_momentum * 0.7 + medium_momentum * 0.3)
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        normalized = max(0, min(1, combined_momentum * 10 + 0.5))
        
        return round(normalized, 3)
        
    except Exception:
        return 0.5

def _calculate_volume_momentum(volume_data: list) -> float:
    """ê±°ë˜ëŸ‰ ëª¨ë©˜í…€ ê³„ì‚°"""
    try:
        if len(volume_data) < 20:
            return 0.5
        
        volumes = np.array(volume_data)
        
        # ìµœê·¼ ê±°ë˜ëŸ‰ vs í‰ê·  ê±°ë˜ëŸ‰
        recent_avg = volumes[-5:].mean()
        baseline_avg = volumes[-20:].mean()
        
        volume_ratio = recent_avg / baseline_avg if baseline_avg > 0 else 1.0
        
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        normalized = max(0, min(1, (np.log(volume_ratio) + 1) / 2))
        
        return round(normalized, 3)
        
    except Exception:
        return 0.5

def _calculate_relative_strength_vs_market(stock_prices: list, market_prices: list) -> float:
    """ì‹œì¥ ëŒ€ë¹„ ìƒëŒ€ ê°•ë„ ê³„ì‚°"""
    try:
        if len(stock_prices) < 20 or len(market_prices) < 20:
            return 0.5
        
        stock_returns = np.array(stock_prices[-20:])
        market_returns = np.array(market_prices[-20:])
        
        # ìˆ˜ìµë¥  ê³„ì‚°
        stock_change = (stock_returns[-1] - stock_returns[0]) / stock_returns[0]
        market_change = (market_returns[-1] - market_returns[0]) / market_returns[0]
        
        # ìƒëŒ€ ê°•ë„ ê³„ì‚°
        if market_change != 0:
            relative_strength = stock_change / market_change
        else:
            relative_strength = 1.0
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (1.0ì´ ì‹œì¥ê³¼ ë™ì¼, >1.0ì´ ì‹œì¥ ëŒ€ë¹„ ê°•í•¨)
        normalized = max(0, min(1, relative_strength / 2))
        
        return round(normalized, 3)
        
    except Exception:
        return 0.5

def _calculate_trend_strength(price_momentum: float, volume_momentum: float, relative_strength: float) -> float:
    """ì¢…í•© ì¶”ì„¸ ê°•ë„ ê³„ì‚°"""
    try:
        # ê°€ì¤‘ í‰ê·  (ê°€ê²© ëª¨ë©˜í…€ 40%, ê±°ë˜ëŸ‰ ëª¨ë©˜í…€ 30%, ìƒëŒ€ ê°•ë„ 30%)
        trend_strength = (
            price_momentum * 0.4 +
            volume_momentum * 0.3 +
            relative_strength * 0.3
        )
        
        return round(trend_strength, 3)
        
    except Exception:
        return 0.5

def _calculate_level_size_allocation(level: int, trend_strength: float) -> float:
    """ë ˆë²¨ë³„ ì‚¬ì´ì¦ˆ ë°°ë¶„ ê³„ì‚°"""
    try:
        # ê¸°ë³¸ ë°°ë¶„ (ë ˆë²¨ì´ ë†’ì„ìˆ˜ë¡ ì‘ê²Œ)
        base_allocations = {1: 0.6, 2: 0.4, 3: 0.3}
        base_allocation = base_allocations.get(level, 0.2)
        
        # ì¶”ì„¸ ê°•ë„ì— ë”°ë¥¸ ì¡°ì •
        trend_multiplier = 0.8 + (trend_strength * 0.4)  # 0.8 ~ 1.2
        
        return round(base_allocation * trend_multiplier, 2)
        
    except Exception:
        return 0.2

# UNUSED: í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ë‚´ë¶€ í•¨ìˆ˜ë“¤ (advanced_scaling_in_systemì—ì„œë§Œ ì‚¬ìš©)
# def _generate_execution_plan(scaling_levels: list, position_sizes: dict, 
#                            risk_management: dict, current_analysis: dict) -> dict:
#     """ì‹¤í–‰ ê³„íš ìƒì„±"""
#     try:
#         execution_plan = {
#             "immediate_actions": [],
#             "scheduled_orders": [],
#             "monitoring_points": [],
#             "risk_alerts": []
#         }
#         
#         # ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜
#         for level_info in scaling_levels:
#             level = level_info["level"]
#             price = level_info["price"]
#             
#             if level in position_sizes.get("levels", {}):
#                 size = position_sizes["levels"][level]
#                 
#                 execution_plan["scheduled_orders"].append({
#                     "level": level,
#                     "order_type": "LIMIT",
#                     "price": price,
#                     "size_pct": size,
#                     "conditions": level_info["trigger_conditions"],
#                     "expiry": "14ì¼"
#                 })
#         
#         # ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸
#         execution_plan["monitoring_points"] = [
#             {"type": "price_breakout", "levels": [level["price"] for level in scaling_levels]},
#             {"type": "volume_surge", "threshold": 1.5},
#             {"type": "trend_weakening", "indicators": ["ma_slope", "volume_decline"]},
#             {"type": "risk_limits", "max_loss": risk_management.get("max_loss_limit", 2.0)}
#         ]
#         
#         # ë¦¬ìŠ¤í¬ ì•Œë¦¼
#         execution_plan["risk_alerts"] = [
#             {
#                 "type": "stop_loss_hit",
#                 "levels": list(risk_management.get("stop_loss_levels", {}).values()),
#                 "action": "IMMEDIATE_EXIT"
#             },
#             {
#                 "type": "trend_reversal",
#                 "indicators": ["price_below_ma50", "volume_dry_up"],
#                 "action": "PARTIAL_EXIT"
#             }
#         ]
#         
#         return execution_plan
#         
#     except Exception as e:
#         logging.warning(f"âš ï¸ ì‹¤í–‰ ê³„íš ìƒì„± ì˜¤ë¥˜: {str(e)}")
#         return {"immediate_actions": [], "scheduled_orders": []}

# def _setup_monitoring_alerts(scaling_levels: list, risk_management: dict) -> dict:
#     """ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ ì„¤ì •"""
#     try:
#         alerts = {
#             "price_alerts": [],
#             "volume_alerts": [],
#             "risk_alerts": [],
#             "trend_alerts": []
#         }
#         
#         # ê°€ê²© ì•Œë¦¼
#         for level_info in scaling_levels:
#             alerts["price_alerts"].append({
#                 "level": level_info["level"],
#                 "trigger_price": level_info["price"],
#                 "message": f"ë ˆë²¨ {level_info['level']} ì§„ì… ê°€ê²© ë„ë‹¬",
#                 "priority": "HIGH"
#             })
#         
#         # ë¦¬ìŠ¤í¬ ì•Œë¦¼
#         for level, stop_price in risk_management.get("stop_loss_levels", {}).items():
#             alerts["risk_alerts"].append({
#                 "level": level,
#                 "level": level,
#                 "trigger_price": stop_price,
#                 "message": f"ë ˆë²¨ {level} ì†ì ˆì„  ê·¼ì ‘",
#                 "priority": "CRITICAL"
#             })
#         
#         # ì¶”ì„¸ ì•Œë¦¼
#         alerts["trend_alerts"] = [
#             {
#                 "condition": "ma50_slope_negative",
#                 "message": "50ì¼ ì´ë™í‰ê·  ê¸°ìš¸ê¸° ìŒì „í™˜",
#                 "priority": "MEDIUM"
#             },
#             {
#                 "condition": "volume_below_average",
#                 "message": "ê±°ë˜ëŸ‰ í‰ê·  ì´í•˜ë¡œ ê°ì†Œ",
#                 "priority": "LOW"
#             }
#         ]
#         
#         return alerts
#         
#     except Exception:
#         return {"price_alerts": [], "volume_alerts": [], "risk_alerts": []}

# def _define_exit_conditions(current_analysis: dict, risk_management: dict) -> dict:
#     """ì²­ì‚° ì¡°ê±´ ì •ì˜"""
#     try:
#         exit_conditions = {
#             "immediate_exit": [],
#             "gradual_exit": [],
#             "emergency_exit": []
#         }
#         
#         # ì¦‰ì‹œ ì²­ì‚° ì¡°ê±´
#         exit_conditions["immediate_exit"] = [
#             {
#                 "condition": "portfolio_stop_loss_hit",
#                 "trigger": risk_management.get("portfolio_stop_loss", 0),
#                 "action": "SELL_ALL",
#                 "reason": "í¬íŠ¸í´ë¦¬ì˜¤ ì†ì ˆì„  ë„ë‹¬"
#             },
#             {
#                 "condition": "trend_reversal_confirmed",
#                 "indicators": ["price_below_ma200", "volume_spike_down"],
#                 "action": "SELL_ALL",
#                 "reason": "ì¶”ì„¸ ë°˜ì „ í™•ì¸"
#             }
#         ]
#         
#         # ë‹¨ê³„ì  ì²­ì‚° ì¡°ê±´
#         exit_conditions["gradual_exit"] = [
#             {
#                 "condition": "profit_target_1_hit",
#                 "action": "SELL_30_PERCENT",
#                 "reason": "ì²« ë²ˆì§¸ ìˆ˜ìµ ëª©í‘œ ë‹¬ì„±"
#             },
#             {
#                 "condition": "trend_weakening",
#                 "indicators": ["ma_slope_flattening", "volume_declining"],
#                 "action": "SELL_50_PERCENT",
#                 "reason": "ì¶”ì„¸ ì•½í™” ì‹ í˜¸"
#             }
#         ]
#         
#         # ê¸´ê¸‰ ì²­ì‚° ì¡°ê±´
#         exit_conditions["emergency_exit"] = [
#             {
#                 "condition": "market_crash",
#                 "trigger": "market_down_5_percent",
#                 "action": "SELL_ALL_IMMEDIATELY",
#                 "reason": "ì‹œì¥ ê¸‰ë½"
#             },
#             {
#                 "condition": "max_loss_exceeded",
#                 "trigger": risk_management.get("emergency_exit_level", 0),
#                 "action": "SELL_ALL_IMMEDIATELY",
#                 "reason": "ìµœëŒ€ ì†ì‹¤ í•œë„ ì´ˆê³¼"
#             }
#         ]
#         
#         return exit_conditions
#         
#     except Exception:
#         return {"immediate_exit": [], "gradual_exit": [], "emergency_exit": []}

# def _get_fallback_pyramid_result(initial_entry: dict) -> dict:
#     """í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ fallback ê²°ê³¼"""
#     return {
#         "pyramid_enabled": False,
#         "reason": "í”¼ë¼ë¯¸ë”© ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ë¹„í™œì„±í™”",
#         "current_position": {
#             "total_size": initial_entry.get("position_size_pct", 2.0),
#             "levels": 1
#         },
#         "next_actions": ["ìˆ˜ë™ ëª¨ë‹ˆí„°ë§", "ì‹œìŠ¤í…œ ë³µêµ¬ í›„ ì¬ì‹œë„"],
#         "fallback_mode": True
#     }

# === ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” ===

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = {
            "analysis_times": [],
            "cache_hit_rates": [],
            "error_rates": [],
            "memory_usage": [],
            "function_call_counts": {}
        }
        self.start_time = time.time()
    
    def record_analysis_time(self, ticker: str, duration_ms: float):
        """ë¶„ì„ ì‹œê°„ ê¸°ë¡"""
        self.metrics["analysis_times"].append({
            "ticker": ticker,
            "duration_ms": duration_ms,
            "timestamp": datetime.now().isoformat()
        })
        
        # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        if len(self.metrics["analysis_times"]) > 100:
            self.metrics["analysis_times"] = self.metrics["analysis_times"][-100:]
    
    def record_function_call(self, function_name: str):
        """í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜ ê¸°ë¡"""
        if function_name not in self.metrics["function_call_counts"]:
            self.metrics["function_call_counts"][function_name] = 0
        self.metrics["function_call_counts"][function_name] += 1
    
    def get_performance_summary(self) -> dict:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        try:
            analysis_times = [item["duration_ms"] for item in self.metrics["analysis_times"]]
            
            if analysis_times:
                avg_time = np.mean(analysis_times)
                p95_time = np.percentile(analysis_times, 95)
                improvement_achieved = avg_time < 100  # 100ms ëª©í‘œ
            else:
                avg_time = p95_time = 0
                improvement_achieved = False
            
            return {
                "average_analysis_time_ms": round(avg_time, 2),
                "p95_analysis_time_ms": round(p95_time, 2),
                "performance_target_achieved": improvement_achieved,
                "total_analyses": len(analysis_times),
                "function_call_counts": self.metrics["function_call_counts"],
                "uptime_hours": round((time.time() - self.start_time) / 3600, 2)
            }
            
        except Exception as e:
            logging.error(f"âŒ ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {"error": "ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"}

# ì „ì—­ ì„±ëŠ¥ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
performance_monitor = PerformanceMonitor()

# UNUSED: í˜¸ì¶œë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜
# def get_optimization_report() -> dict:
#     """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
#     try:
#         # ì„±ëŠ¥ ëª¨ë‹ˆí„° ë°ì´í„°
#         perf_summary = performance_monitor.get_performance_summary()
#         
#         # ìºì‹œ í†µê³„ (strategy_analyzerì—ì„œ ê°€ì ¸ì˜´)
#         try:
#             from strategy_analyzer import enhanced_technical_cache_manager
#             cache_stats = enhanced_technical_cache_manager().get_cache_stats()
#         except Exception:
#             cache_stats = {"hit_rate": 0, "cache_size": 0}
#         
#         # ìµœì í™” ë‹¬ì„±ë„ í‰ê°€
#         achievements = {
#             "function_integration": {
#                 "target": "20ê°œ í•¨ìˆ˜ â†’ 5ê°œ í•¨ìˆ˜",
#                 "achieved": perf_summary.get("total_analyses", 0) > 0,
#                 "impact": "ë¶„ì„ ì‹œê°„ ë‹¨ì¶•"
#             },
#             "caching_enhancement": {
#                 "target": "ìºì‹œ íˆíŠ¸ìœ¨ 60% ì´ìƒ",
#                 "achieved": cache_stats.get("hit_rate", 0) >= 60,
#                 "impact": "ë°˜ë³µ ê³„ì‚° ìµœì†Œí™”"
#             },
#             "error_handling": {
#                 "target": "Fail-safe íŒŒì´í”„ë¼ì¸ êµ¬í˜„",
#                 "achieved": True,  # êµ¬í˜„ ì™„ë£Œ
#                 "impact": "ì‹œìŠ¤í…œ ì•ˆì •ì„± í–¥ìƒ"
#             },
#             "performance_target": {
#                 "target": "ì¢…ëª©ë‹¹ ë¶„ì„ ì‹œê°„ 50% ë‹¨ì¶•",
#                 "achieved": perf_summary.get("performance_target_achieved", False),
#                 "impact": "ì „ì²´ ì‹œìŠ¤í…œ ì²˜ë¦¬ëŸ‰ ì¦ê°€"
#             }
#         }
#         
#         # ì¶”ê°€ ê°œì„  ì œì•ˆ
#         improvement_suggestions = []
#         
#         if cache_stats.get("hit_rate", 0) < 60:
#             improvement_suggestions.append({
#                 "area": "ìºì‹± ìµœì í™”",
#                 "suggestion": "ìºì‹œ TTL ì¡°ì • ë° í‚¤ ìµœì í™”",
#                 "priority": "HIGH"
#             })
#         
#         if perf_summary.get("average_analysis_time_ms", 0) > 100:
#             improvement_suggestions.append({
#                 "area": "ì„±ëŠ¥ ìµœì í™”",
#                 "suggestion": "ë³‘ë ¬ ì²˜ë¦¬ í™•ëŒ€ ë° ì•Œê³ ë¦¬ì¦˜ ìµœì í™”",
#                 "priority": "MEDIUM"
#             })
#         
#         return {
#             "optimization_summary": {
#                 "total_functions_integrated": 20,
#                 "target_functions_achieved": 5,
#                 "performance_improvement_pct": 50 if achievements["performance_target"]["achieved"] else 0,
#                 "cache_efficiency_pct": cache_stats.get("hit_rate", 0)
#             },
#             "achievements": achievements,
#             "performance_metrics": perf_summary,
#             "cache_statistics": cache_stats,
#             "improvement_suggestions": improvement_suggestions,
#             "report_generated_at": datetime.now().isoformat()
#         }
#         
#     except Exception as e:
#         logging.error(f"âŒ ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}")
#         return {
#             "error": "ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨",
#             "message": str(e)
#         }

# === ë¡œê·¸ ìˆœí™˜ ë° ì••ì¶• ì„¤ì • ===

def setup_gpt_logging_rotation(log_file_path: str = None, 
                              max_bytes: int = 50 * 1024 * 1024,  # 50MB
                              backup_count: int = 5,
                              enable_compression: bool = False) -> logging.Logger:
                              #enable_compression: bool = True) -> logging.Logger:
    """
    GPT ë¶„ì„ìš© ë¡œê¹… ìˆœí™˜ ë° ì••ì¶• ì„¤ì • (ì œí•œëœ ë¡œê¹… ì‚¬ìš©)
    
    Args:
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ makenaide.log ì‚¬ìš©)
        max_bytes: ìµœëŒ€ íŒŒì¼ í¬ê¸° (ê¸°ë³¸: 50MB)
        backup_count: ë°±ì—… íŒŒì¼ ê°œìˆ˜ (ê¸°ë³¸: 5ê°œ)
        enable_compression: ì••ì¶• í™œì„±í™” ì—¬ë¶€
    """
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œê°€ Noneì´ë©´ makenaide.log ì‚¬ìš© (ì œí•œëœ ë¡œê¹…)
    if log_file_path is None:
        from utils import safe_strftime
        log_dir = "log"
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        log_file_path = os.path.join(log_dir, f"{safe_strftime(datetime.now(), '%Y%m%d')}_makenaide.log")
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    
    # GPT ì „ìš© ë¡œê±° ìƒì„±
    gpt_logger = logging.getLogger('gpt_analysis')
    gpt_logger.setLevel(logging.DEBUG)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in gpt_logger.handlers[:]:
        gpt_logger.removeHandler(handler)
    
    # ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„±
    rotating_handler = logging.handlers.RotatingFileHandler(
        log_file_path,
        maxBytes=max_bytes,
        backupCount=backup_count,
        encoding='utf-8'
    )
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    rotating_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    gpt_logger.addHandler(rotating_handler)
    
    # ì••ì¶• ê¸°ëŠ¥ í™œì„±í™” ì‹œ ì»¤ìŠ¤í…€ ë¡œí…Œì´í„° ì„¤ì •
    if enable_compression:
        def compress_rotated_log(source, dest):
            """ë¡œí…Œì´íŠ¸ëœ ë¡œê·¸ íŒŒì¼ ì••ì¶•"""
            try:
                with open(source, 'rb') as f_in:
                    with gzip.open(f"{dest}.gz", 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                os.remove(source)
                logging.info(f"ğŸ“¦ ë¡œê·¸ íŒŒì¼ ì••ì¶• ì™„ë£Œ: {dest}.gz")
            except Exception as e:
                logging.error(f"âŒ ë¡œê·¸ íŒŒì¼ ì••ì¶• ì‹¤íŒ¨: {e}")
        
        rotating_handler.rotator = compress_rotated_log
    
    logging.info(f"ğŸ“‹ GPT ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ - íŒŒì¼: {log_file_path}, ìµœëŒ€í¬ê¸°: {max_bytes//1024//1024}MB")
    return gpt_logger

# === ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ ì‹œìŠ¤í…œ ===

def mask_sensitive_info(data: Any, mask_level: str = "medium") -> Any:
    """
    ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬
    
    Args:
        data: ë§ˆìŠ¤í‚¹í•  ë°ì´í„° (ë¬¸ìì—´, ë”•ì…”ë„ˆë¦¬, ë¦¬ìŠ¤íŠ¸ ë“±)
        mask_level: ë§ˆìŠ¤í‚¹ ê°•ë„ ("low", "medium", "high")
    """
    
    def mask_api_keys(text: str) -> str:
        """API í‚¤ ë§ˆìŠ¤í‚¹"""
        # API í‚¤ íŒ¨í„´ë“¤
        patterns = [
            (r'sk-[a-zA-Z0-9]{20,}', 'sk-***MASKED***'),
            (r'Bearer [a-zA-Z0-9]{20,}', 'Bearer ***MASKED***'),
            (r'key["\']?\s*[:=]\s*["\']?[a-zA-Z0-9]{10,}["\']?', 'key=***MASKED***'),
            (r'token["\']?\s*[:=]\s*["\']?[a-zA-Z0-9]{10,}["\']?', 'token=***MASKED***')
        ]
        
        for pattern, replacement in patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        return text
    
    def mask_financial_data(text: str, level: str) -> str:
        """ê¸ˆìœµ ë°ì´í„° ë§ˆìŠ¤í‚¹"""
        if level == "low":
            return text
        elif level == "medium":
            # êµ¬ì²´ì  ìˆ˜ì¹˜ë¥¼ ë²”ìœ„ë¡œ ë³€ê²½
            text = re.sub(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*(?:ì›|KRW|USD)\b', '[AMOUNT_RANGE]', text)
            text = re.sub(r'\b\d+\.\d+%\b', '[PERCENTAGE_RANGE]', text)
            return text
        else:  # high
            # ëª¨ë“  ìˆ«ìë¥¼ ë§ˆìŠ¤í‚¹
            text = re.sub(r'\b\d+(?:\.\d+)?\b', '[NUMERIC_VALUE]', text)
            return text
    
    def mask_personal_info(text: str) -> str:
        """ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹"""
        # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '***@***.***', text)
        # ì „í™”ë²ˆí˜¸ ë§ˆìŠ¤í‚¹
        text = re.sub(r'\b\d{2,3}-\d{3,4}-\d{4}\b', '***-****-****', text)
        return text
    
    if isinstance(data, str):
        result = mask_api_keys(data)
        result = mask_financial_data(result, mask_level)
        result = mask_personal_info(result)
        return result
    
    elif isinstance(data, dict):
        masked_dict = {}
        for key, value in data.items():
            # í‚¤ ìì²´ë„ ë¯¼ê°ì •ë³´ì¸ì§€ í™•ì¸
            sensitive_keys = ['password', 'secret', 'key', 'token', 'api_key']
            if any(sensitive_key in key.lower() for sensitive_key in sensitive_keys):
                masked_dict[key] = '***MASKED***'
            else:
                masked_dict[key] = mask_sensitive_info(value, mask_level)
        return masked_dict
    
    elif isinstance(data, list):
        return [mask_sensitive_info(item, mask_level) for item in data]
    
    else:
        return data

# === ë³´ì•ˆ ê°•í™” ë¡œê¹… ì‹œìŠ¤í…œ ===

def setup_secure_logging(log_level: str = None, enable_sensitive_masking: bool = True) -> logging.Logger:
    """
    ë³´ì•ˆì´ ê°•í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
    
    Args:
        log_level: í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë¡œê¹… ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR)
        enable_sensitive_masking: ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ í™œì„±í™” ì—¬ë¶€
    """
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œê¹… ë ˆë²¨ ê°€ì ¸ì˜¤ê¸°
    if log_level is None:
        log_level = os.getenv('LOGGING_LEVEL', 'INFO').upper()
    
    # ë¡œê±° ìƒì„±
    secure_logger = logging.getLogger('secure_trend_analyzer')
    secure_logger.setLevel(getattr(logging, log_level, logging.INFO))
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in secure_logger.handlers[:]:
        secure_logger.removeHandler(handler)
    
    # ë³´ì•ˆ í•„í„° í´ë˜ìŠ¤
    class SensitiveInfoFilter(logging.Filter):
        def __init__(self, enable_masking=True):
            super().__init__()
            self.enable_masking = enable_masking
        
        def filter(self, record):
            if self.enable_masking and hasattr(record, 'msg'):
                # ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
                if isinstance(record.msg, str):
                    record.msg = mask_sensitive_info(record.msg, mask_level="medium")
                elif isinstance(record.args, tuple):
                    # í¬ë§· ì¸ìë“¤ë„ ë§ˆìŠ¤í‚¹
                    masked_args = []
                    for arg in record.args:
                        if isinstance(arg, (str, dict)):
                            masked_args.append(mask_sensitive_info(arg, mask_level="medium"))
                        else:
                            masked_args.append(arg)
                    record.args = tuple(masked_args)
            return True
    
    # ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ (ì•”í˜¸í™” ê³ ë ¤)
    log_file_path = os.getenv('SECURE_LOG_PATH', 'log/secure_trend_analyzer.log')
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    
    file_handler = logging.handlers.RotatingFileHandler(
        log_file_path,
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=3,
        encoding='utf-8'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ (ê°œë°œ í™˜ê²½ìš©)
    console_handler = logging.StreamHandler()
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # í•„í„° ì ìš©
    if enable_sensitive_masking:
        sensitive_filter = SensitiveInfoFilter(enable_masking=True)
        file_handler.addFilter(sensitive_filter)
        console_handler.addFilter(sensitive_filter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    secure_logger.addHandler(file_handler)
    
    # ê°œë°œ í™˜ê²½ì—ì„œë§Œ ì½˜ì†” ì¶œë ¥
    if os.getenv('ENVIRONMENT', 'production').lower() in ['development', 'dev', 'local']:
        secure_logger.addHandler(console_handler)
    
    # ë¡œê·¸ ë ˆë²¨ë³„ ì¶œë ¥ ì œì–´
    if log_level == 'DEBUG':
        secure_logger.info("ğŸ” ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¡œê¹… ì‹œìŠ¤í…œ í™œì„±í™”ë¨")
    elif log_level == 'ERROR':
        secure_logger.error("âš ï¸ ì—ëŸ¬ ë ˆë²¨ ë¡œê¹…ë§Œ í™œì„±í™”ë¨")
    
    secure_logger.info(f"ğŸ”’ ë³´ì•ˆ ê°•í™” ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • ì™„ë£Œ - ë ˆë²¨: {log_level}")
    return secure_logger

# í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì œí•œëœ ë¡œê¹… ì‚¬ìš©)
gpt_logger = setup_gpt_logging_rotation(log_file_path=None)  # makenaide.log ì‚¬ìš©
secure_logger = setup_secure_logging()

# === GPT ë¶„ì„ ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤ ===

class GPTAnalysisMonitor:
    """GPT ë¶„ì„ ì„±ëŠ¥ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.api_call_count = 0
        self.total_token_usage = 0
        self.total_processing_time = 0.0
        self.success_count = 0
        self.error_count = 0
        self.error_types = {}
        self.start_time = time.time()
        self.daily_costs = {}
        self.lock = threading.Lock()
        
        # í† í° ë¹„ìš© ì •ë³´ (GPT-4o ê¸°ì¤€, USD)
        self.token_costs = {
            "input": 0.0025 / 1000,   # per token
            "output": 0.01 / 1000     # per token
        }
    
    def track_api_call(self, tokens_used: int, processing_time: float, success: bool, 
                      error_type: str = None, output_tokens: int = 0):
        """API í˜¸ì¶œ ì¶”ì  ë° ë¹„ìš© ê³„ì‚°"""
        with self.lock:
            self.api_call_count += 1
            self.total_token_usage += tokens_used
            self.total_processing_time += processing_time
            
            if success:
                self.success_count += 1
            else:
                self.error_count += 1
                if error_type:
                    self.error_types[error_type] = self.error_types.get(error_type, 0) + 1
            
            # ì¼ë³„ ë¹„ìš© ê³„ì‚°
            today = datetime.now().strftime("%Y-%m-%d")
            if today not in self.daily_costs:
                self.daily_costs[today] = {"input_tokens": 0, "output_tokens": 0, "calls": 0}
            
            self.daily_costs[today]["input_tokens"] += tokens_used
            self.daily_costs[today]["output_tokens"] += output_tokens
            self.daily_costs[today]["calls"] += 1
            
            logging.info(f"ğŸ“Š API í˜¸ì¶œ ì¶”ì : í† í°={tokens_used}, ì‹œê°„={processing_time:.2f}s, ì„±ê³µ={success}")
    
    def get_efficiency_report(self) -> dict:
        """íš¨ìœ¨ì„± ë¦¬í¬íŠ¸ ìƒì„±"""
        with self.lock:
            if self.api_call_count == 0:
                return {"error": "ì•„ì§ API í˜¸ì¶œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤"}
            
            success_rate = (self.success_count / self.api_call_count) * 100
            avg_processing_time = self.total_processing_time / self.api_call_count
            avg_tokens_per_call = self.total_token_usage / self.api_call_count
            
            # ì˜¤ëŠ˜ ë¹„ìš© ê³„ì‚°
            today = datetime.now().strftime("%Y-%m-%d")
            today_cost = 0.0
            if today in self.daily_costs:
                day_data = self.daily_costs[today]
                today_cost = (
                    day_data["input_tokens"] * self.token_costs["input"] +
                    day_data["output_tokens"] * self.token_costs["output"]
                )
            
            # ì´ ë¹„ìš© ê³„ì‚°
            total_cost = sum(
                (data["input_tokens"] * self.token_costs["input"] + 
                 data["output_tokens"] * self.token_costs["output"])
                for data in self.daily_costs.values()
            )
            
            uptime_hours = (time.time() - self.start_time) / 3600
            
            report = {
                "ì´_API_í˜¸ì¶œìˆ˜": self.api_call_count,
                "ì„±ê³µë¥ ": f"{success_rate:.1f}%",
                "í‰ê· _ì²˜ë¦¬ì‹œê°„": f"{avg_processing_time:.2f}ì´ˆ",
                "í‰ê· _í† í°ì‚¬ìš©ëŸ‰": f"{avg_tokens_per_call:.0f}í† í°",
                "ì´_í† í°ì‚¬ìš©ëŸ‰": self.total_token_usage,
                "ì˜¤ëŠ˜_ë¹„ìš©": f"${today_cost:.4f}",
                "ì´_ë¹„ìš©": f"${total_cost:.4f}",
                "ì‹œê°„ë‹¹_í˜¸ì¶œìˆ˜": f"{self.api_call_count / uptime_hours:.1f}" if uptime_hours > 0 else "0",
                "ì˜¤ë¥˜_ìœ í˜•ë³„_í†µê³„": self.error_types,
                "ì¼ë³„_ì‚¬ìš©ëŸ‰": self.daily_costs
            }
            
            return report
    
    def get_cost_alert(self, daily_limit_usd: float = 10.0) -> dict:
        """ë¹„ìš© ê²½ê³  ì‹œìŠ¤í…œ"""
        today = datetime.now().strftime("%Y-%m-%d")
        if today not in self.daily_costs:
            return {"alert": False, "message": "ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰ ì—†ìŒ"}
        
        day_data = self.daily_costs[today]
        today_cost = (
            day_data["input_tokens"] * self.token_costs["input"] +
            day_data["output_tokens"] * self.token_costs["output"]
        )
        
        usage_percentage = (today_cost / daily_limit_usd) * 100
        
        if usage_percentage >= 90:
            return {
                "alert": True,
                "level": "critical",
                "message": f"âš ï¸ ì¼ì¼ ë¹„ìš© í•œë„ì˜ {usage_percentage:.1f}% ì‚¬ìš© (${today_cost:.4f}/${daily_limit_usd})"
            }
        elif usage_percentage >= 70:
            return {
                "alert": True,
                "level": "warning", 
                "message": f"ğŸ“ˆ ì¼ì¼ ë¹„ìš© í•œë„ì˜ {usage_percentage:.1f}% ì‚¬ìš© (${today_cost:.4f}/${daily_limit_usd})"
            }
        else:
            return {
                "alert": False,
                "message": f"âœ… ì¼ì¼ ë¹„ìš© ì‚¬ìš©ëŸ‰: {usage_percentage:.1f}% (${today_cost:.4f}/${daily_limit_usd})"
            }

class GPTAnalysisErrorHandler:
    """í‘œì¤€í™”ëœ GPT ë¶„ì„ ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def handle_api_error(error: Exception, ticker: str, fallback_score: float = 50.0) -> dict:
        """API ì˜¤ë¥˜ë³„ ì„¸ë¶„í™”ëœ ì²˜ë¦¬"""
        error_mapping = {
            RateLimitError: {
                "confidence": 0.3,
                "reason": "rate_limit", 
                "retry_after": 60,
                "message": "API ìš”ì²­ í•œë„ ì´ˆê³¼"
            },
            TimeoutError: {
                "confidence": 0.4,
                "reason": "timeout",
                "retry_after": 10,
                "message": "ìš”ì²­ ì‹œê°„ ì´ˆê³¼"
            },
            ConnectionError: {
                "confidence": 0.2,
                "reason": "connection_error",
                "retry_after": 30,
                "message": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜"
            },
            OpenAIError: {
                "confidence": 0.35,
                "reason": "openai_api_error",
                "retry_after": 15,
                "message": "OpenAI API ì˜¤ë¥˜"
            }
        }
        
        # íŠ¹ì • ì˜¤ë¥˜ ìœ í˜•ë³„ ì²˜ë¦¬
        for error_type, config in error_mapping.items():
            if isinstance(error, error_type):
                logging.warning(f"ğŸš¨ {ticker} {config['message']}: {str(error)}")
                return {
                    "ticker": ticker,
                    "score": fallback_score,
                    "confidence": config["confidence"],
                    "analysis_method": f"error_{config['reason']}",
                    "error_details": {
                        "type": config["reason"],
                        "message": config["message"],
                        "original_error": str(error),
                        "retry_after": config["retry_after"],
                        "timestamp": datetime.now().isoformat()
                    }
                }
        
        # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
        logging.error(f"âŒ {ticker} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(error)}")
        logging.error(f"âŒ {ticker} ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        
        return {
            "ticker": ticker,
            "score": fallback_score,
            "confidence": 0.1,
            "analysis_method": "error_unknown",
            "error_details": {
                "type": "unknown_error",
                "message": "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ",
                "original_error": str(error),
                "retry_after": 30,
                "timestamp": datetime.now().isoformat()
            }
        }
    
    @staticmethod
    def should_retry(error_details: dict, max_retries: int = 3, current_retry: int = 0) -> bool:
        """ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
        if current_retry >= max_retries:
            return False
        
        # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ìœ í˜•
        retryable_errors = ["rate_limit", "timeout", "connection_error", "openai_api_error"]
        error_type = error_details.get("type", "")
        
        if error_type in retryable_errors:
            retry_after = error_details.get("retry_after", 10)
            logging.info(f"ğŸ”„ {retry_after}ì´ˆ í›„ ì¬ì‹œë„ ì˜ˆì • (ì‹œë„ {current_retry + 1}/{max_retries})")
            time.sleep(retry_after)
            return True
        
        return False
    
    @staticmethod 
    def log_error_analytics(error_details: dict, ticker: str):
        """ì˜¤ë¥˜ ë¶„ì„ì„ ìœ„í•œ ë¡œê¹…"""
        try:
            # ì˜¤ë¥˜ í†µê³„ë¥¼ íŒŒì¼ì— ì €ì¥ (ì„ íƒì )
            error_log_path = "log/gpt_error_analytics.json"
            os.makedirs(os.path.dirname(error_log_path), exist_ok=True)
            
            error_entry = {
                "timestamp": datetime.now().isoformat(),
                "ticker": ticker,
                **error_details
            }
            
            # ê¸°ì¡´ ë¡œê·¸ ì½ê¸°
            error_analytics = []
            if os.path.exists(error_log_path):
                with open(error_log_path, 'r', encoding='utf-8') as f:
                    error_analytics = json.load(f)
            
            # ìƒˆ ì˜¤ë¥˜ ì¶”ê°€
            error_analytics.append(error_entry)
            
            # ìµœê·¼ 1000ê°œ ì˜¤ë¥˜ë§Œ ìœ ì§€
            if len(error_analytics) > 1000:
                error_analytics = error_analytics[-1000:]
            
            # íŒŒì¼ì— ì €ì¥
            with open(error_log_path, 'w', encoding='utf-8') as f:
                json.dump(error_analytics, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logging.warning(f"âš ï¸ ì˜¤ë¥˜ ë¶„ì„ ë¡œê¹… ì‹¤íŒ¨: {e}")

class APIRateLimiter:
    """API í˜¸ì¶œ ë¹ˆë„ ì œí•œ ê´€ë¦¬"""
    
    def __init__(self, calls_per_minute=50):
        self.calls_per_minute = calls_per_minute
        self.calls = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """í•„ìš” ì‹œ ëŒ€ê¸°í•˜ì—¬ rate limit ì¤€ìˆ˜"""
        with self.lock:
            now = time.time()
            # 1ë¶„ ì´ë‚´ì˜ í˜¸ì¶œë§Œ ìœ ì§€
            self.calls = [call_time for call_time in self.calls if now - call_time < 60]
            
            if len(self.calls) >= self.calls_per_minute:
                sleep_time = 60 - (now - self.calls[0])
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            self.calls.append(now)

class AnalysisCacheManager:
    """GPT ë¶„ì„ ê²°ê³¼ ìºì‹± ë§¤ë‹ˆì € - LRU ì •ì±… ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”"""
    
    def __init__(self, cache_ttl_minutes=720, max_memory_mb=100, max_entries=1000):
        self.cache = {}
        self.cache_ttl = cache_ttl_minutes * 60
        self.max_memory_bytes = max_memory_mb * 1024 * 1024  # MBë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        self.max_entries = max_entries
        self.current_memory_usage = 0
        self.access_times = {}  # LRU ì¶”ì 
        self.entry_sizes = {}   # ê° ì—”íŠ¸ë¦¬ í¬ê¸° ì¶”ì 
        self.lock = threading.Lock()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        self._start_cleanup_thread()
    
    def _start_cleanup_thread(self):
        """ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def cleanup_worker():
            while True:
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì •ë¦¬
                self._cleanup_expired_entries()
                self._enforce_memory_limits()
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
        logging.info("ğŸ§¹ ìºì‹œ ì •ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")
    
    def _cleanup_expired_entries(self):
        """ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        with self.lock:
            current_time = time.time()
            expired_keys = []
            
            for key, entry in self.cache.items():
                if current_time - entry['timestamp'] > self.cache_ttl:
                    expired_keys.append(key)
            
            for key in expired_keys:
                self._remove_entry(key)
                
            if expired_keys:
                logging.info(f"ğŸ—‘ï¸ {len(expired_keys)}ê°œ ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬ë¨")
    
    def _enforce_memory_limits(self):
        """ë©”ëª¨ë¦¬ ë° ì—”íŠ¸ë¦¬ ìˆ˜ ì œí•œ ì ìš©"""
        with self.lock:
            # ì—”íŠ¸ë¦¬ ìˆ˜ ì œí•œ
            if len(self.cache) > self.max_entries:
                excess_count = len(self.cache) - self.max_entries
                self._evict_lru_entries(excess_count)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
            if self.current_memory_usage > self.max_memory_bytes:
                target_memory = self.max_memory_bytes * 0.8  # 80%ê¹Œì§€ ì¤„ì„
                self._evict_by_memory(target_memory)
    
    def _evict_lru_entries(self, count: int):
        """LRU ì •ì±…ìœ¼ë¡œ ì—”íŠ¸ë¦¬ ì œê±°"""
        # ì•¡ì„¸ìŠ¤ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_keys = sorted(self.access_times.keys(), 
                           key=lambda k: self.access_times[k])
        
        evicted = 0
        for key in sorted_keys:
            if evicted >= count:
                break
            if key in self.cache:
                self._remove_entry(key)
                evicted += 1
                
        if evicted > 0:
            logging.info(f"ğŸ”„ LRU ì •ì±…ìœ¼ë¡œ {evicted}ê°œ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°ë¨")
    
    def _evict_by_memory(self, target_memory: int):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì—”íŠ¸ë¦¬ ì œê±°"""
        # í¬ê¸°ê°€ í° ì—”íŠ¸ë¦¬ë¶€í„° ì œê±°
        sorted_keys = sorted(self.entry_sizes.keys(), 
                           key=lambda k: self.entry_sizes[k], reverse=True)
        
        evicted = 0
        for key in sorted_keys:
            if self.current_memory_usage <= target_memory:
                break
            if key in self.cache:
                self._remove_entry(key)
                evicted += 1
                
        if evicted > 0:
            logging.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ {evicted}ê°œ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°ë¨")
    
    def _remove_entry(self, key: str):
        """ì—”íŠ¸ë¦¬ ì œê±° ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        if key in self.cache:
            self.current_memory_usage -= self.entry_sizes.get(key, 0)
            del self.cache[key]
            self.access_times.pop(key, None)
            self.entry_sizes.pop(key, None)
    
    def _calculate_entry_size(self, data: dict) -> int:
        """ì—”íŠ¸ë¦¬ í¬ê¸° ê³„ì‚° (ë°”ì´íŠ¸)"""
        try:
            data_str = json.dumps(data, ensure_ascii=False)
            return len(data_str.encode('utf-8'))
        except:
            return 1024  # ê¸°ë³¸ ì¶”ì •ê°’
    
    def get_cache_key(self, ticker: str, data_hash: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"gpt_analysis:{ticker}:{data_hash[:8]}"
    
    def _get_data_hash(self, data: dict) -> str:
        """ë°ì´í„° í•´ì‹œ ìƒì„±"""
        data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def get_from_cache(self, ticker: str, data: dict) -> Optional[dict]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ - LRU ì—…ë°ì´íŠ¸ í¬í•¨"""
        with self.lock:
            data_hash = self._get_data_hash(data)
            cache_key = self.get_cache_key(ticker, data_hash)
            
            if cache_key in self.cache:
                cached_entry = self.cache[cache_key]
                if time.time() - cached_entry['timestamp'] < self.cache_ttl:
                    # LRU ì—…ë°ì´íŠ¸
                    self.access_times[cache_key] = time.time()
                    logging.info(f"ğŸ”„ {ticker} ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©")
                    return cached_entry['result']
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    self._remove_entry(cache_key)
        return None
    
    def save_to_cache(self, ticker: str, data: dict, result: dict):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥ - ë©”ëª¨ë¦¬ ê´€ë¦¬ í¬í•¨"""
        with self.lock:
            data_hash = self._get_data_hash(data)
            cache_key = self.get_cache_key(ticker, data_hash)
            
            # ì—”íŠ¸ë¦¬ í¬ê¸° ê³„ì‚°
            compressed_result = {
                'timestamp': time.time(),
                'result': result
            }
            entry_size = self._calculate_entry_size(compressed_result)
            
            # ë©”ëª¨ë¦¬ ì œí•œ í™•ì¸
            if self.current_memory_usage + entry_size > self.max_memory_bytes:
                # ê³µê°„ í™•ë³´
                target_memory = self.max_memory_bytes * 0.7  # 70%ê¹Œì§€ ì¤„ì„
                self._evict_by_memory(target_memory)
            
            # ìºì‹œì— ì €ì¥
            self.cache[cache_key] = compressed_result
            self.access_times[cache_key] = time.time()
            self.entry_sizes[cache_key] = entry_size
            self.current_memory_usage += entry_size
            
            logging.debug(f"ğŸ’¾ {ticker} ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥ (í¬ê¸°: {entry_size:,} bytes)")
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        with self.lock:
            return {
                'total_entries': len(self.cache),
                'memory_usage_mb': self.current_memory_usage / (1024 * 1024),
                'memory_limit_mb': self.max_memory_bytes / (1024 * 1024),
                'memory_usage_pct': (self.current_memory_usage / self.max_memory_bytes) * 100,
                'entries_limit': self.max_entries,
                'entries_usage_pct': (len(self.cache) / self.max_entries) * 100
            }

class GPTAnalysisOptimizerSingleton:
    """ìŠ¤ë ˆë“œ ì•ˆì „í•œ GPT ë¶„ì„ ì„±ëŠ¥ ìµœì í™” ë§¤ë‹ˆì € ì‹±ê¸€í†¤"""
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.rate_limiter = APIRateLimiter(calls_per_minute=50)
            self.cache_manager = AnalysisCacheManager()
            self.monitor = GPTAnalysisMonitor()
            self.error_handler = GPTAnalysisErrorHandler()
            self._initialized = True

    def optimize_for_tokens(self, json_data: dict, target_tokens: int = 3000, strategy: str = "simple") -> dict:
        """
        í†µí•©ëœ í† í° ìµœì í™”
        strategy: "simple" | "comprehensive"
        """
        if strategy == "simple":
            return self._optimize_simple(json_data, target_tokens)
        else:
            return self._optimize_comprehensive(json_data, target_tokens)
    
    def _optimize_simple(self, json_data: dict, target_tokens: int = 3000) -> dict:
        """
        ë‹¨ìˆœí™”ëœ í† í° ìµœì í™”
        1. í† í° ê³„ì‚°
        2. OHLCV ë°ì´í„° ì••ì¶• (í•„ìš”ì‹œ)
        3. ì •ë°€ë„ ì¡°ì •
        """
        enc = tiktoken.encoding_for_model("gpt-4o")
        json_str = json.dumps(json_data, ensure_ascii=False)
        current_tokens = len(enc.encode(json_str))
        
        if current_tokens <= target_tokens:
            return json_data
        
        optimized = json_data.copy()
        
        # 1ë‹¨ê³„: ì •ë°€ë„ ì¡°ì •
        optimized = self._round_numbers_precision(optimized, precision=2)
        
        # 2ë‹¨ê³„: OHLCV ì••ì¶•
        if 'ohlcv' in optimized and current_tokens > target_tokens:
            compression_ratio = target_tokens / current_tokens
            new_length = int(len(optimized['ohlcv']) * compression_ratio * 0.8)  # ì•ˆì „ ë§ˆì§„
            optimized['ohlcv'] = optimized['ohlcv'][-new_length:]
        
        return optimized
    
    def _optimize_comprehensive(self, json_data: dict, target_tokens: int = 3000) -> dict:
        """
        í¬ê´„ì  í† í° ìµœì í™” ë¡œì§
        1. tiktokenìœ¼ë¡œ ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚°
        2. ìš°ì„ ìˆœìœ„ë³„ ë°ì´í„° ì••ì¶• (OHLCV > ì§€í‘œ > ë©”íƒ€ë°ì´í„°)
        3. ë™ì  ì••ì¶•ë¥  ì¡°ì •
        4. ìµœì†Œ í•„ìˆ˜ ë°ì´í„°ëŠ” ë³´ì¡´
        """
        try:
            # tiktoken ì¸ì½”ë” ì´ˆê¸°í™”
            enc = tiktoken.encoding_for_model("gpt-4o")
            
            # í˜„ì¬ í† í° ìˆ˜ ê³„ì‚°
            json_str = json.dumps(json_data, ensure_ascii=False)
            current_tokens = len(enc.encode(json_str))
            
            if current_tokens <= target_tokens:
                logging.debug(f"ğŸ“Š í† í° ìµœì í™” ë¶ˆí•„ìš”: {current_tokens}/{target_tokens}")
                return json_data
            
            logging.info(f"ğŸ”§ í† í° ìµœì í™” ì‹œì‘: {current_tokens} -> {target_tokens} ëª©í‘œ")
            
            # ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„±
            optimized = json_data.copy()
            
            # 1ë‹¨ê³„: ì •ë°€ë„ ì¡°ì • (ì†Œìˆ˜ì  2ìë¦¬ë¡œ ì œí•œ)
            optimized = self._round_numbers_precision(optimized, precision=2)
            json_str = json.dumps(optimized, ensure_ascii=False)
            current_tokens = len(enc.encode(json_str))
            
            if current_tokens <= target_tokens:
                logging.info(f"âœ… 1ë‹¨ê³„ ì •ë°€ë„ ì¡°ì •ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±: {current_tokens}")
                return optimized
            
            # 2ë‹¨ê³„: OHLCV ë°ì´í„° ì••ì¶• (ìš°ì„ ìˆœìœ„ë³„)
            compression_levels = [
                {"ohlcv_limit": 100, "desc": "OHLCV 100ê°œ"},
                {"ohlcv_limit": 50, "desc": "OHLCV 50ê°œ"},
                {"ohlcv_limit": 30, "desc": "OHLCV 30ê°œ"},
                {"ohlcv_limit": 20, "desc": "OHLCV 20ê°œ"}
            ]
            
            for level in compression_levels:
                if 'ohlcv' in optimized and len(optimized['ohlcv']) > level['ohlcv_limit']:
                    # ìµœì‹  ë°ì´í„° ìš°ì„ ìœ¼ë¡œ ì••ì¶•
                    optimized['ohlcv'] = optimized['ohlcv'][-level['ohlcv_limit']:]
                    
                    json_str = json.dumps(optimized, ensure_ascii=False)
                    current_tokens = len(enc.encode(json_str))
                    
                    logging.info(f"ğŸ”§ {level['desc']} ì••ì¶•: {current_tokens} í† í°")
                    
                    if current_tokens <= target_tokens:
                        logging.info(f"âœ… OHLCV ì••ì¶•ìœ¼ë¡œ ëª©í‘œ ë‹¬ì„±: {current_tokens}")
                        return optimized
            
            # 3ë‹¨ê³„: ì§€í‘œ ë°ì´í„° ì••ì¶•
            if 'indicators' in optimized:
                # ì¤‘ìš”ë„ ë‚®ì€ ì§€í‘œ ì œê±°
                low_priority_indicators = [
                    'volume_sma', 'ad_line', 'obv', 'cmf', 'vwap',
                    'williams_r', 'cci', 'atr_percent'
                ]
                
                for indicator in low_priority_indicators:
                    if indicator in optimized['indicators']:
                        del optimized['indicators'][indicator]
                        
                        json_str = json.dumps(optimized, ensure_ascii=False)
                        current_tokens = len(enc.encode(json_str))
                        
                        if current_tokens <= target_tokens:
                            logging.info(f"âœ… {indicator} ì œê±°ë¡œ ëª©í‘œ ë‹¬ì„±: {current_tokens}")
                            return optimized
            
            # 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì••ì¶•
            metadata_keys = ['timestamp', 'last_updated', 'data_source', 'version']
            for key in metadata_keys:
                if key in optimized:
                    del optimized[key]
                    
                    json_str = json.dumps(optimized, ensure_ascii=False)
                    current_tokens = len(enc.encode(json_str))
                    
                    if current_tokens <= target_tokens:
                        logging.info(f"âœ… {key} ì œê±°ë¡œ ëª©í‘œ ë‹¬ì„±: {current_tokens}")
                        return optimized
            
            # 5ë‹¨ê³„: ìµœì¢… ì••ì¶• (OHLCV ë” ì¶•ì†Œ)
            if 'ohlcv' in optimized and len(optimized['ohlcv']) > 10:
                optimized['ohlcv'] = optimized['ohlcv'][-10:]  # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
                
                json_str = json.dumps(optimized, ensure_ascii=False)
                current_tokens = len(enc.encode(json_str))
                
                logging.warning(f"âš ï¸ ìµœì¢… ì••ì¶• ì™„ë£Œ: {current_tokens} í† í° (ëª©í‘œ: {target_tokens})")
            
            return optimized
            
        except Exception as e:
            logging.error(f"âŒ í† í° ìµœì í™” ì‹¤íŒ¨: {e}")
            return json_data  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _round_numbers_precision(self, obj: Any, precision: int = 2) -> Any:
        """ìˆ«ì ì •ë°€ë„ ì¡°ì •"""
        if isinstance(obj, dict):
            return {k: self._round_numbers_precision(v, precision) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._round_numbers_precision(item, precision) for item in obj]
        elif isinstance(obj, float):
            return round(obj, precision)
        return obj
    
    def optimize_json_payload(self, json_data: dict, max_size_kb: int = 4) -> dict:
        """JSON í˜ì´ë¡œë“œ ìµœì í™” (ê¸°ì¡´ ë©”ì„œë“œ ê°œì„ )"""
        # ìƒˆë¡œìš´ í† í° ê¸°ë°˜ ìµœì í™” ì‚¬ìš©
        optimized = self.optimize_for_tokens(json_data, target_tokens=3000, strategy="comprehensive")
        
        # í¬ê¸° ì œí•œë„ í™•ì¸
        json_str = json.dumps(optimized, ensure_ascii=False)
        current_size_kb = len(json_str.encode('utf-8')) / 1024
        
        if current_size_kb > max_size_kb:
            # ì¶”ê°€ ì••ì¶•ì´ í•„ìš”í•œ ê²½ìš°
            optimized = self.optimize_for_tokens(optimized, target_tokens=2000, strategy="comprehensive")
            logging.warning(f"ğŸ“Š ì¶”ê°€ ì••ì¶•: {current_size_kb:.1f}KB -> ëª©í‘œ {max_size_kb}KB")
        
        return optimized

    def manage_api_rate_limits(self):
        """API ìœ¨ì œí•œ ê´€ë¦¬"""
        self.rate_limiter.wait_if_needed()

@dataclass
class AnalysisConfig:
    """ë¶„ì„ ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    mode: str = "hybrid"  # "json", "chart", "hybrid"
    batch_size: int = 3
    enable_caching: bool = True
    cache_ttl_minutes: int = 720
    api_timeout_seconds: int = 30
    max_retries: int = 3

# === í†µí•© GPT ë¶„ì„ ì—”ì§„ ===

def unified_gpt_analysis_engine(candidates: Union[List[dict], List[tuple]], analysis_config: Optional[dict] = None) -> List[dict]:
    """
    í†µí•© GPT ë¶„ì„ ì—”ì§„ - ëª¨ë“  ë¶„ì„ ë°©ì‹ì„ ì§€ì›í•˜ëŠ” ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤
    
    Args:
        candidates: ë¶„ì„ ëŒ€ìƒ ë°ì´í„°
            - JSON í˜•ì‹: [{"ticker": str, "ohlcv": [...], "indicators": {...}}]
            - íŠœí”Œ í˜•ì‹: [(ticker, score), ...]
        analysis_config: ë¶„ì„ ì„¤ì •
            {
                "mode": "json" | "chart" | "hybrid",
                "batch_size": 3,
                "enable_caching": True,
                "cache_ttl_minutes": 720,
                "api_timeout_seconds": 30,
                "max_retries": 3
            }
    
    Returns:
        list: [{"ticker": str, "score": int, "confidence": float, "analysis_method": str}]
    """
    
    # ê¸°ë³¸ ì„¤ì • ì ìš©
    config = _apply_default_config(analysis_config)
    
    # ì…ë ¥ ë°ì´í„° ì •ê·œí™”
    normalized_candidates = _normalize_input_candidates(candidates)
    
    logging.info(f"ğŸ”„ í†µí•© GPT ë¶„ì„ ì‹œì‘: {len(normalized_candidates)}ê°œ í›„ë³´, ëª¨ë“œ: {config.mode}")
    
    # ë¶„ì„ ëª¨ë“œë³„ ë¼ìš°íŒ…
    if config.mode == "json":
        return _execute_json_analysis_pipeline(normalized_candidates, config)
    elif config.mode == "chart":
        return _execute_chart_analysis_pipeline(normalized_candidates, config)
    else:  # hybrid
        return _execute_hybrid_analysis_pipeline(normalized_candidates, config)

def _apply_default_config(analysis_config: Optional[dict]) -> AnalysisConfig:
    """ê¸°ë³¸ ì„¤ì • ì ìš©"""
    if analysis_config is None:
        return AnalysisConfig()
    
    return AnalysisConfig(
        mode=analysis_config.get("mode", "hybrid"),
        batch_size=analysis_config.get("batch_size", 3),
        enable_caching=analysis_config.get("enable_caching", True),
        cache_ttl_minutes=analysis_config.get("cache_ttl_minutes", 720),
        api_timeout_seconds=analysis_config.get("api_timeout_seconds", 30),
        max_retries=analysis_config.get("max_retries", 3)
    )

def _normalize_input_candidates(candidates: Union[List[dict], List[tuple]]) -> List[dict]:
    """ì…ë ¥ ë°ì´í„° ì •ê·œí™”"""
    normalized = []
    
    for candidate in candidates:
        if isinstance(candidate, dict):
            # JSON í˜•ì‹ ë°ì´í„°
            normalized.append(candidate)
        elif isinstance(candidate, (tuple, list)) and len(candidate) >= 2:
            # íŠœí”Œ í˜•ì‹ ë°ì´í„° (ticker, score)
            ticker, score = candidate[0], candidate[1]
            normalized.append({
                "ticker": ticker,
                "base_score": score,
                "ohlcv": [],
                "indicators": {}
            })
        else:
            logging.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” í›„ë³´ ë°ì´í„° í˜•ì‹: {candidate}")
    
    return normalized

def _execute_json_analysis_pipeline(candidates: List[dict], config: AnalysisConfig) -> List[dict]:
    """JSON ê¸°ë°˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    results = []
    optimizer = GPTAnalysisOptimizerSingleton()
    
    # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    for batch in _create_batches(candidates, config.batch_size):
        try:
            # ìºì‹œ í™•ì¸
            cached_results = []
            uncached_batch = []
            
            if config.enable_caching:
                for candidate in batch:
                    cached_result = optimizer.cache_manager.get_from_cache(
                        candidate["ticker"], candidate
                    )
                    if cached_result:
                        cached_results.append(cached_result)
                    else:
                        uncached_batch.append(candidate)
            else:
                uncached_batch = batch
            
            # GPT API í˜¸ì¶œ
            if uncached_batch:
                batch_results = _call_gpt_json_batch(uncached_batch, config, optimizer)
                
                # ìºì‹œ ì €ì¥
                if config.enable_caching:
                    for candidate, result in zip(uncached_batch, batch_results):
                        optimizer.cache_manager.save_to_cache(
                            candidate["ticker"], candidate, result
                        )
                
                cached_results.extend(batch_results)
            
            results.extend(cached_results)
            
        except Exception as e:
            logging.error(f"âŒ JSON ë¶„ì„ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
            results.extend(_fallback_individual_processing(batch, config))
    
    return results

def _execute_chart_analysis_pipeline(candidates: List[dict], config: AnalysisConfig) -> List[dict]:
    """ì°¨íŠ¸ ê¸°ë°˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    results = []
    
    for candidate in candidates:
        ticker = candidate["ticker"]
        try:
            # ì°¨íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
            chart_path = f"charts/{ticker}.png"
            if not os.path.exists(chart_path):
                # ì°¨íŠ¸ ìƒì„± ì‹œë„
                from data_fetcher import generate_chart_image, generate_gpt_analysis_json
                generate_chart_image(ticker)
            
            if os.path.exists(chart_path):
                # ì°¨íŠ¸ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰
                with open(chart_path, "rb") as f:
                    chart_base64 = base64.b64encode(f.read()).decode("utf-8")
                
                gpt_result = call_gpt_with_chart_base64(ticker, chart_base64, candidate.get("indicators", {}))
                
                results.append({
                    "ticker": ticker,
                    "score": gpt_result.get("score", 50),
                    "confidence": 0.85,  # ì°¨íŠ¸ ë¶„ì„ ê¸°ë³¸ ì‹ ë¢°ë„
                    "analysis_method": "chart"
                })
            else:
                logging.warning(f"âš ï¸ {ticker} ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                results.append({
                    "ticker": ticker,
                    "score": candidate.get("base_score", 50),
                    "confidence": 0.50,
                    "analysis_method": "chart_failed"
                })
                
        except Exception as e:
            logging.error(f"âŒ {ticker} ì°¨íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            results.append({
                "ticker": ticker,
                "score": candidate.get("base_score", 50),
                "confidence": 0.50,
                "analysis_method": "chart_error"
            })
    
    return results

def _execute_hybrid_analysis_pipeline(candidates: List[dict], config: AnalysisConfig) -> List[dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ (JSON ìš°ì„ , ì°¨íŠ¸ ëŒ€ì²´)"""
    
    # 1ì°¨: JSON ë¶„ì„ ì‹œë„
    json_results = []
    failed_candidates = []
    
    try:
        json_config = AnalysisConfig(
            mode="json",
            batch_size=config.batch_size,
            enable_caching=config.enable_caching,
            cache_ttl_minutes=config.cache_ttl_minutes,
            api_timeout_seconds=config.api_timeout_seconds,
            max_retries=config.max_retries
        )
        json_results = _execute_json_analysis_pipeline(candidates, json_config)
        
        # JSON ë¶„ì„ ì„±ê³µë¥  í™•ì¸
        success_rate = len(json_results) / len(candidates) if candidates else 0
        if success_rate < 0.8:  # 80% ë¯¸ë§Œ ì„±ê³µ ì‹œ
            logging.warning(f"âš ï¸ JSON ë¶„ì„ ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.2%}")
            successful_tickers = {r["ticker"] for r in json_results}
            failed_candidates = [c for c in candidates if c["ticker"] not in successful_tickers]
            
    except Exception as e:
        logging.warning(f"âš ï¸ JSON ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        failed_candidates = candidates
    
    # 2ì°¨: ì‹¤íŒ¨í•œ í•­ëª©ì— ëŒ€í•´ ì°¨íŠ¸ ë¶„ì„
    chart_results = []
    if failed_candidates:
        logging.info(f"ğŸ”„ {len(failed_candidates)}ê°œ í•­ëª©ì— ëŒ€í•´ ì°¨íŠ¸ ë¶„ì„ ì‹¤í–‰")
        chart_config = AnalysisConfig(
            mode="chart",
            batch_size=config.batch_size,
            enable_caching=config.enable_caching,
            cache_ttl_minutes=config.cache_ttl_minutes,
            api_timeout_seconds=config.api_timeout_seconds,
            max_retries=config.max_retries
        )
        chart_results = _execute_chart_analysis_pipeline(failed_candidates, chart_config)
    
    # ê²°ê³¼ í†µí•©
    combined_results = json_results + chart_results
    
    # ë¶„ì„ ë°©ë²• íƒœê¹…
    for result in json_results:
        if "analysis_method" not in result:
            result["analysis_method"] = "json"
    for result in chart_results:
        if "analysis_method" not in result:
            result["analysis_method"] = "chart_fallback"
    
    return combined_results

def _create_batches(items: List[Any], batch_size: int) -> List[List[Any]]:
    """í•­ëª©ì„ ë°°ì¹˜ë¡œ ë¶„í• """
    for i in range(0, len(items), batch_size):
        yield items[i:i + batch_size]

def _call_gpt_json_batch(batch: List[dict], config: AnalysisConfig, optimizer: GPTAnalysisOptimizerSingleton) -> List[dict]:
    """JSON ë°°ì¹˜ GPT ë¶„ì„ í˜¸ì¶œ"""
    results = []
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    for candidate in batch:
        ticker = candidate["ticker"]
        start_time = time.time()
        
        try:
            # Rate limiting ì ìš©
            optimizer.manage_api_rate_limits()
            
            # GPT ë¶„ì„ìš© JSON ë°ì´í„° ìƒì„± (data_fetcherì˜ í•¨ìˆ˜ ì‚¬ìš©) - 200ì¼ë¡œ í™•ì¥
            json_data = generate_gpt_analysis_json(ticker, days=200)
            if json_data is None:
                logging.warning(f"âš ï¸ {ticker} JSON ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
                results.append({
                    "ticker": ticker,
                    "score": candidate.get("base_score", 50),
                    "confidence": 0.50,
                    "analysis_method": "json_data_failed"
                })
                continue
            
            # í† í° ìˆ˜ ê³„ì‚°
            enc = tiktoken.encoding_for_model("gpt-4o")
            token_count = len(enc.encode(json_data))
            
            # GPT ë¶„ì„ ì‹¤í–‰ - system_prompt ì‚¬ìš© ë° JSON í˜•ì‹ ìš”êµ¬
            messages = [
                {
                    "role": "system", 
                    "content": system_prompt  # system_prompt.txt ë‚´ìš© ì‚¬ìš©
                },
                {
                    "role": "user",
                    "content": f"""
Analyze this PRE-FILTERED buy candidate from the Makenaide trading system:

[Ticker] {ticker}
[OHLCV and Technical Indicators Data]
{json_data}

This ticker has already passed multi-stage filtering (blacklist, volume, technical indicators) 
and should theoretically be in Stage 1-2 with upward momentum patterns. 

Verify if this is a genuine uptrend candidate or a false signal that should be avoided.

RESPOND ONLY with this exact JSON format (no additional text):
{{
  "ticker": "{ticker}",
  "score": {{integer_0_to_100}},
  "confidence": {{decimal_0_to_1}},
  "action": "BUY | HOLD | AVOID",
  "market_phase": "Stage1 | Stage2 | Stage3 | Stage4",
  "pattern": "{{pattern_name}}",
  "reason": "{{brief_explanation_max_200_chars}}"
}}
                    """
                }
            ]

            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3,
                max_tokens=500,
                timeout=config.api_timeout_seconds
            )

            processing_time = time.time() - start_time
            output_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0
            
            # ì„±ê³µ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
            optimizer.monitor.track_api_call(token_count, processing_time, True, output_tokens=output_tokens)

            content = response.choices[0].message.content
            
            # ğŸ” ìƒì„¸ ì‘ë‹µ ë¡œê¹… ì¶”ê°€
            logging.info(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹ :")
            logging.info(f"   - ì‘ë‹µ ê¸¸ì´: {len(content)} characters")
            logging.info(f"   - ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {content[:200]}...")
            logging.info(f"   - ì „ì²´ ì‘ë‹µ:\n{content}")
            
            # JSON íŒŒì‹±ìœ¼ë¡œ ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            gpt_score = 50  # ê¸°ë³¸ê°’
            confidence = 0.85  # ê¸°ë³¸ confidence
            action = "AVOID"  # ê¸°ë³¸ê°’
            market_phase = "Unknown"  # ê¸°ë³¸ê°’
            pattern = "Unknown"  # ê¸°ë³¸ê°’
            reason = "Empty"  # ê¸°ë³¸ê°’
            
            try:
                # JSON ì‘ë‹µ íŒŒì‹±
                import json
                # JSON ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                clean_content = content.strip()
                if clean_content.startswith("```json"):
                    clean_content = clean_content[7:]
                if clean_content.endswith("```"):
                    clean_content = clean_content[:-3]
                
                parsed_result = json.loads(clean_content.strip())
                
                # ê° í•„ë“œ ì¶”ì¶œ ë° ê²€ì¦
                gpt_score = int(parsed_result.get("score", 50))
                gpt_score = max(0, min(100, gpt_score))  # ë²”ìœ„ ê²€ì¦
                
                confidence = float(parsed_result.get("confidence", 0.85))
                confidence = max(0.0, min(1.0, confidence))  # ë²”ìœ„ ê²€ì¦
                
                action = parsed_result.get("action", "AVOID").upper()
                if action not in ["BUY", "HOLD", "AVOID"]:
                    action = "AVOID"
                
                market_phase = parsed_result.get("market_phase", "Unknown")
                if market_phase not in ["Stage1", "Stage2", "Stage3", "Stage4"]:
                    market_phase = "Unknown"
                
                pattern = parsed_result.get("pattern", "Unknown")
                reason = parsed_result.get("reason", "Empty")[:200]  # 200ì ì œí•œ
                
                logging.info(f"âœ… {ticker} JSON íŒŒì‹± ì„±ê³µ: score={gpt_score}, action={action}, phase={market_phase}")
                
            except json.JSONDecodeError as e:
                logging.warning(f"âš ï¸ {ticker} JSON íŒŒì‹± ì‹¤íŒ¨, í´ë°± ì²˜ë¦¬: {str(e)}")
                # í´ë°±: í…ìŠ¤íŠ¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ ì‹œë„
                import re
                score_match = re.search(r'"score":\s*(\d+)', content)
                if score_match:
                    gpt_score = int(score_match.group(1))
                    gpt_score = max(0, min(100, gpt_score))
                
                # ì•¡ì…˜ ì¶”ì¶œ ì‹œë„
                action_match = re.search(r'"action":\s*"([^"]+)"', content)
                if action_match:
                    action = action_match.group(1).upper()
                    if action not in ["BUY", "HOLD", "AVOID"]:
                        action = "AVOID"
                
            except Exception as e:
                logging.warning(f"âš ï¸ {ticker} ì‘ë‹µ íŒŒì‹± ì™„ì „ ì‹¤íŒ¨: {str(e)}")

            logging.info(f"âœ… {ticker} JSON ë°©ì‹ GPT ë¶„ì„ ì™„ë£Œ: {gpt_score}ì  (confidence: {confidence:.2f}, ì‹œê°„: {processing_time:.2f}s)")
            
            results.append({
                "ticker": ticker,
                "score": gpt_score,
                "confidence": confidence,
                "action": action,
                "market_phase": market_phase,
                "pattern": pattern,
                "reason": reason,
                "analysis_method": "json",
                "processing_time": processing_time,
                "token_usage": token_count
            })
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_result = optimizer.error_handler.handle_api_error(e, ticker, candidate.get("base_score", 50))
            
            # ì‹¤íŒ¨ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
            error_type = error_result.get("error_details", {}).get("type", "unknown")
            token_count = token_count if 'token_count' in locals() else 0
            optimizer.monitor.track_api_call(token_count, processing_time, False, error_type)
            
            # ì˜¤ë¥˜ ë¶„ì„ ë¡œê¹…
            optimizer.error_handler.log_error_analytics(error_result.get("error_details", {}), ticker)
            
            logging.warning(f"âš ï¸ {ticker} JSON ë°©ì‹ GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            results.append({
                "ticker": ticker,
                "score": error_result.get("score", candidate.get("base_score", 50)),
                "confidence": error_result.get("confidence", 0.50),
                "analysis_method": "json_failed",
                "error_details": error_result.get("error_details", {})
            })
    
    return results

def _fallback_individual_processing(batch: List[dict], config: AnalysisConfig) -> List[dict]:
    """ê°œë³„ ì²˜ë¦¬ í´ë°±"""
    results = []
    for candidate in batch:
        ticker = candidate["ticker"]
        logging.warning(f"ğŸ”„ {ticker} ê°œë³„ ì²˜ë¦¬ í´ë°± ì‹¤í–‰")
        results.append({
            "ticker": ticker,
            "score": candidate.get("base_score", 50),
            "confidence": 0.50,
            "analysis_method": "fallback"
        })
    return results

def reload_system_prompt():
    """ë™ì  í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì§€ì› (config_loader ê¸°ë°˜)"""
    global system_prompt
    from config_loader import reload_system_prompt
    system_prompt = reload_system_prompt()
    logging.info("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¬ë¡œë”© ì™„ë£Œ")

# í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ëŠ” config_loaderë¡œ í†µí•©
from config_loader import get_cached_system_prompt
system_prompt = get_cached_system_prompt()

def fetch_selected_market_data(ticker_list):
    conn = psycopg2.connect(
        host=os.getenv("PG_HOST"),
        port=os.getenv("PG_PORT"),
        dbname=os.getenv("PG_DATABASE"),
        user=os.getenv("PG_USER"),
        password=os.getenv("PG_PASSWORD")
    )
    cur = conn.cursor()
    placeholders = ",".join("%s" for _ in ticker_list)
    query = f"""
        SELECT o.ticker, o.close as price, o.ma_50, o.ma_200, o.rsi_14, 
               NULL as mfi_14, o.ht_trendline, o.fibo_382, o.fibo_618,
               s.r1, s.s1, 'charts/' || o.ticker || '.png' as chart_path, 
               NULL as position_avg_price, o.date as updated_at,
               s.volume_change_7_30, s.supertrend_signal
        FROM (
            SELECT DISTINCT ON (ticker) ticker, close, ma_50, ma_200, rsi_14, 
                   ht_trendline, fibo_382, fibo_618, date
            FROM ohlcv 
            WHERE ticker IN ({placeholders})
            ORDER BY ticker, date DESC
        ) o
        LEFT JOIN static_indicators s ON o.ticker = s.ticker
    """
    cur.execute(query, ticker_list)
    data = cur.fetchall()
    conn.close()
    return data

# í™˜ê²½ ë³€ìˆ˜ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (API í‚¤ëŠ” main.pyì—ì„œ ì²˜ë¦¬)
client = OpenAI()

def get_current_price_safe(ticker, retries=3, delay=0.3):
    import time
    import pyupbit
    attempt = 0
    while attempt < retries:
        try:
            price_data = pyupbit.get_current_price(ticker)
            if price_data is None:
                raise ValueError("No data returned")
            # ë§Œì•½ ìˆ«ìí˜•ì´ë©´ ë°”ë¡œ ë°˜í™˜
            if isinstance(price_data, (int, float)):
                return price_data
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°: í‹°ì»¤ í‚¤ ë˜ëŠ” 'trade_price' í‚¤ í™œìš©
            if isinstance(price_data, dict):
                if ticker in price_data:
                    return price_data[ticker]
                elif 'trade_price' in price_data:
                    return price_data['trade_price']
                else:
                    first_val = next(iter(price_data.values()), None)
                    if first_val is not None:
                        return first_val
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°: ì²« ë²ˆì§¸ ìš”ì†Œì—ì„œ 'trade_price' í‚¤ ì¶”ì¶œ
            if isinstance(price_data, list) and len(price_data) > 0:
                trade_price = price_data[0].get("trade_price")
                if trade_price is not None:
                    return trade_price
            raise ValueError(f"Unexpected data format: {price_data}")
        except Exception as e:
            print(f"[ERROR] get_current_price_safe error for {ticker}: {e}")
            attempt += 1
            time.sleep(delay)
    return None
# market_dataì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ GPT APIì— ì „ì†¡
def fetch_market_data():
    conn = psycopg2.connect(
        host=os.getenv("PG_HOST"),
        port=os.getenv("PG_PORT"),
        dbname=os.getenv("PG_DATABASE"),
        user=os.getenv("PG_USER"),
        password=os.getenv("PG_PASSWORD")
    )
    cur = conn.cursor()
    
    cur.execute("""
        SELECT o.ticker, o.close as price, o.ma_50, o.ma_200, o.rsi_14, 
               NULL as mfi_14, o.ht_trendline, o.fibo_382, o.fibo_618,
               s.r1, s.s1, 'charts/' || o.ticker || '.png' as chart_path, 
               NULL as position_avg_price, o.date as updated_at,
               s.volume_change_7_30, s.supertrend_signal
        FROM (
            SELECT DISTINCT ON (ticker) ticker, close, ma_50, ma_200, rsi_14, 
                   ht_trendline, fibo_382, fibo_618, date
            FROM ohlcv 
            ORDER BY ticker, date DESC
        ) o
        LEFT JOIN static_indicators s ON o.ticker = s.ticker
    """)
    data = cur.fetchall()
    
    conn.close()
    return data

# GPT API í˜¸ì¶œ í•¨ìˆ˜ (í‘œì¤€í™”ëœ ì‘ë‹µ í˜•ì‹)
def analyze_trend_with_gpt(ticker: str, daily_data: dict, daily_chart_image_path: str, db_manager: DBManager):
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë´‰ ë°ì´í„°ì™€ ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì„¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        optimizer = GPTAnalysisOptimizerSingleton()
        
        # 2. ìºì‹œ í™•ì¸ (ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª… ì‚¬ìš©)
        cached_result = optimizer.cache_manager.get_from_cache(ticker, daily_data)
        if cached_result:
            logging.info(f"ğŸ”„ {ticker} ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©")
            return cached_result
        
        # 3. ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì ìš©
        optimizer.rate_limiter.wait_if_needed()
        
        # 4. ì°¨íŠ¸ ì´ë¯¸ì§€ base64 ì¸ì½”ë”©
        import base64
        if not os.path.exists(daily_chart_image_path):
            from data_fetcher import generate_chart_image
            generate_chart_image(ticker)
        
        with open(daily_chart_image_path, "rb") as image_file:
            chart_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        
        # 5. ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì¤€ë¹„
        indicators_summary = prepare_indicators_summary(daily_data)
        
        # 6. GPT í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ í€€íŠ¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì•”í˜¸í™”íì˜ ì°¨íŠ¸ì™€ ê¸°ìˆ ì  ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ íˆ¬ì ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

í‹°ì»¤: {ticker}
í˜„ì¬ê°€: {daily_data.get('price', 'N/A')}
MA50: {daily_data.get('ma_50', 'N/A')}
MA200: {daily_data.get('ma_200', 'N/A')}
RSI: {daily_data.get('rsi_14', 'N/A')}
ê±°ë˜ëŸ‰ ë³€í™”: {daily_data.get('volume_change_7_30', 'N/A')}%

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
Market Phase: [Stage 1/Stage 2/Stage 3/Stage 4/Unknown]
Confidence: [0.0-1.0]
Action: [BUY/SELL/HOLD]
Summary: [ê°„ë‹¨í•œ ë¶„ì„ ìš”ì•½]
Score: [0-100ì ]
"""
        
        # 7. OpenAI API í˜¸ì¶œ
        start_time = time.time()
        
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{chart_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        processing_time = time.time() - start_time
        gpt_response = response.choices[0].message.content
        
        # ğŸ” ìƒì„¸ ì‘ë‹µ ë¡œê¹… ì¶”ê°€
        logging.info(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹ :")
        logging.info(f"   - ì‘ë‹µ ê¸¸ì´: {len(gpt_response)} characters")
        logging.info(f"   - ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {gpt_response[:200]}...")
        logging.info(f"   - ì „ì²´ ì‘ë‹µ:\n{gpt_response}")
        
        # 8. ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦
        parsed_result = parse_gpt_response(gpt_response)
        
        # 9. í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
        validated_result = {
            'ticker': ticker,
            'market_phase': parsed_result.get('market_phase', 'Unknown'),
            'confidence': max(0.0, min(1.0, float(parsed_result.get('confidence', 0.5)))),
            'action': parsed_result.get('action', 'HOLD'),
            'summary': parsed_result.get('summary', 'ë¶„ì„ ì™„ë£Œ'),
            'score': max(0, min(100, int(parsed_result.get('score', 50)))),
            'processing_time': processing_time,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        # 10. DB ì €ì¥
        save_enhanced_analysis_to_db(validated_result, db_manager)
        
        # 11. ìºì‹œ ì €ì¥
        optimizer.cache_manager.save_to_cache(ticker, daily_data, validated_result)
        
        # 12. ëª¨ë‹ˆí„°ë§ ê¸°ë¡
        token_usage = response.usage.total_tokens if hasattr(response, 'usage') else 0
        optimizer.monitor.track_api_call(token_usage, processing_time, True)
        
        logging.info(f"âœ… {ticker} GPT ë¶„ì„ ì™„ë£Œ: {validated_result['market_phase']}, ì‹ ë¢°ë„: {validated_result['confidence']:.2f}")
        
        return validated_result
        
    except Exception as e:
        # ì—ëŸ¬ ì²˜ë¦¬
        logging.error(f"âŒ {ticker} GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        error_result = {
            'ticker': ticker,
            'error': str(e),
            'market_phase': 'Unknown',
            'confidence': 0.0,
            'action': 'HOLD',
            'summary': f'ë¶„ì„ ì‹¤íŒ¨: {str(e)}',
            'score': 50
        }
        
        # ì‹¤íŒ¨ ëª¨ë‹ˆí„°ë§
        if 'optimizer' in locals():
            optimizer.monitor.track_api_call(0, 0, False, type(e).__name__)
        
        return error_result

def prepare_indicators_summary(daily_data: dict) -> str:
    """
    ê¸°ìˆ ì  ì§€í‘œë“¤ì„ GPTê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ ìš”ì•½ìœ¼ë¡œ ë³€í™˜
    
    Args:
        daily_data (dict): ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„°
        
    Returns:
        str: ì§€í‘œ ìš”ì•½ í…ìŠ¤íŠ¸
    """
    try:
        summary_parts = []
        
        # RSI ë¶„ì„
        rsi = daily_data.get('rsi_14')
        if rsi:
            if rsi > 70:
                summary_parts.append(f"RSI ê³¼ë§¤ìˆ˜({rsi:.1f})")
            elif rsi < 30:
                summary_parts.append(f"RSI ê³¼ë§¤ë„({rsi:.1f})")
            else:
                summary_parts.append(f"RSI ì¤‘ë¦½({rsi:.1f})")
        
        # ì´ë™í‰ê·  ë¶„ì„
        ma_50 = daily_data.get('ma_50')
        ma_200 = daily_data.get('ma_200')
        current_price = daily_data.get('price', 0)
        
        if ma_50 and ma_200 and current_price:
            if current_price > ma_50 > ma_200:
                summary_parts.append("ê°•ì„¸ ì •ë ¬")
            elif current_price < ma_50 < ma_200:
                summary_parts.append("ì•½ì„¸ ì •ë ¬")
            else:
                summary_parts.append("í˜¼ì¡° ì •ë ¬")
        
        # ê±°ë˜ëŸ‰ ë¶„ì„
        volume_change = daily_data.get('volume_change_7_30')
        if volume_change:
            if volume_change > 50:
                summary_parts.append(f"ê±°ë˜ëŸ‰ ê¸‰ì¦({volume_change:.0f}%)")
            elif volume_change < -30:
                summary_parts.append(f"ê±°ë˜ëŸ‰ ìœ„ì¶•({volume_change:.0f}%)")
        
        # ì§€ì§€/ì €í•­ ë¶„ì„
        support = daily_data.get('s1')
        resistance = daily_data.get('r1')
        if support and resistance and current_price:
            support_distance = ((current_price - support) / support) * 100
            resistance_distance = ((resistance - current_price) / current_price) * 100
            summary_parts.append(f"ì§€ì§€ì„ +{support_distance:.1f}%, ì €í•­ì„ -{resistance_distance:.1f}%")
        
        # ìŠˆí¼íŠ¸ë Œë“œ ë¶„ì„
        supertrend = daily_data.get('supertrend_signal')
        if supertrend:
            if supertrend.lower() == 'up':
                summary_parts.append("ìŠˆí¼íŠ¸ë Œë“œ ìƒìŠ¹")
            elif supertrend.lower() == 'down':
                summary_parts.append("ìŠˆí¼íŠ¸ë Œë“œ í•˜ë½")
        
        # í”¼ë³´ë‚˜ì¹˜ ë¶„ì„
        fibo_382 = daily_data.get('fibo_382')
        fibo_618 = daily_data.get('fibo_618')
        if fibo_382 and fibo_618 and current_price:
            if current_price > fibo_618:
                summary_parts.append("í”¼ë³´ë‚˜ì¹˜ 61.8% ëŒíŒŒ")
            elif current_price > fibo_382:
                summary_parts.append("í”¼ë³´ë‚˜ì¹˜ 38.2%-61.8% êµ¬ê°„")
            else:
                summary_parts.append("í”¼ë³´ë‚˜ì¹˜ 38.2% ë¯¸ë§Œ")
        
        return "; ".join(summary_parts) if summary_parts else "ì •ìƒ ë²”ìœ„"
        
    except Exception as e:
        logging.error(f"âŒ indicators_summary ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return "ì§€í‘œ ë¶„ì„ ì‹¤íŒ¨"

def parse_gpt_response(gpt_response: str) -> dict:
    """GPT ì‘ë‹µ íŒŒì‹±"""
    result = {}
    
    for line in gpt_response.split('\n'):
        line = line.strip()
        if ':' in line:
            key, value = line.split(':', 1)
            key = key.strip().lower().replace(' ', '_')
            value = value.strip()
            
            if key == 'market_phase':
                result['market_phase'] = value
            elif key == 'confidence':
                try:
                    result['confidence'] = float(value)
                except:
                    result['confidence'] = 0.5
            elif key == 'action':
                result['action'] = value.upper()
            elif key == 'summary':
                result['summary'] = value
            elif key == 'score':
                try:
                    result['score'] = int(value)
                except:
                    result['score'] = 50
    
    return result

def validate_and_standardize_gpt_response(ticker: str, raw_response: dict, daily_data: dict) -> dict:
    """
    GPT ì‘ë‹µì„ ê²€ì¦í•˜ê³  í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    í‘œì¤€í™” í•­ëª©:
    1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    2. ì´ìƒê°’ í•„í„°ë§ (confidence, score ë²”ìœ„ ì²´í¬)
    3. market_phase ê°’ ê²€ì¦ ë° ë³´ì •
    4. action ê°’ í‘œì¤€í™”
    """
    try:
        if not raw_response or raw_response.get('error'):
            return create_standardized_error_response(ticker, "GPT ì‘ë‹µ ì˜¤ë¥˜", 0.2)
        
        # 1. ê¸°ë³¸ í•„ë“œ ì¶”ì¶œ ë° ê²€ì¦ - market_phase ê¸°ë³¸ê°’ ê°œì„ 
        score = raw_response.get('score', 50)
        confidence = raw_response.get('confidence', score / 100.0 if isinstance(score, (int, float)) else 0.5)
        action = raw_response.get('action', 'HOLD')
        
        # market_phase ì¶”ì¶œ - ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… ì‹œë„
        market_phase = (
            raw_response.get('market_phase') or 
            raw_response.get('phase') or 
            raw_response.get('stage') or 
            'Stage1'  # Unknown ëŒ€ì‹  Stage1ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        )
        
        comment = raw_response.get('comment', raw_response.get('reason', ''))
        
        # 2. ê°’ ë²”ìœ„ ê²€ì¦ ë° ë³´ì •
        # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (0-100)
        if not isinstance(score, (int, float)) or score < 0 or score > 100:
            logging.warning(f"âš ï¸ {ticker} ì´ìƒí•œ ì ìˆ˜ê°’: {score} â†’ 50ìœ¼ë¡œ ë³´ì •")
            score = 50
        
        # ì‹ ë¢°ë„ ë²”ìœ„ ê²€ì¦ (0-1)
        if not isinstance(confidence, (int, float)) or confidence < 0 or confidence > 1:
            logging.warning(f"âš ï¸ {ticker} ì´ìƒí•œ ì‹ ë¢°ë„ê°’: {confidence} â†’ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë³´ì •")
            confidence = score / 100.0
        
        # 3. ì•¡ì…˜ ê°’ í‘œì¤€í™”
        action = standardize_action_value(action)
        
        # 4. market_phase ê²€ì¦ ë° ë³´ì • - ê°„ë‹¨í•œ í˜•ì‹ ê²€ì¦ë§Œ ìˆ˜í–‰
        if market_phase and isinstance(market_phase, str):
            # Stage1, Stage2, Stage3, Stage4 ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
            valid_phases = ['Stage1', 'Stage2', 'Stage3', 'Stage4']
            if market_phase not in valid_phases:
                logging.warning(f"âš ï¸ {ticker} ì˜ëª»ëœ market_phase: {market_phase} â†’ Stage1ë¡œ ë³´ì •")
                market_phase = 'Stage1'
        else:
            market_phase = 'Stage1'
        
        # 5. ì¶”ê°€ ë¶„ì„ ë©”íƒ€ë°ì´í„° ìƒì„±
        analysis_quality = assess_analysis_quality(raw_response, daily_data)
        
        # 6. í‘œì¤€í™”ëœ ì‘ë‹µ êµ¬ì¡° ìƒì„±
        standardized_response = {
            "ticker": ticker,
            "action": action,
            "confidence": confidence,
            "score": score,
            "market_phase": market_phase,
            "comment": comment[:500] if comment else "ë¶„ì„ ì½”ë©˜íŠ¸ ì—†ìŒ",  # ê¸¸ì´ ì œí•œ
            "analysis_quality": analysis_quality,
            "validation_status": "validated",
            "raw_gpt_response": raw_response,
            "error": None,
            "timestamp": datetime.now().isoformat(),
            "analysis_method": "gpt_standardized"
        }
        
        # 7. ìµœì¢… ê²€ì¦ - í•„ìˆ˜ í•„ë“œ ì¬í™•ì¸
        required_fields = ['ticker', 'action', 'confidence', 'market_phase']
        for field in required_fields:
            if field not in standardized_response or standardized_response[field] is None:
                logging.error(f"âŒ {ticker} í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                return create_standardized_error_response(ticker, f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}", 0.2)
        
        return standardized_response
        
    except Exception as e:
        logging.error(f"âŒ {ticker} ì‘ë‹µ í‘œì¤€í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return create_standardized_error_response(ticker, f"í‘œì¤€í™” ì˜¤ë¥˜: {str(e)}", 0.1)


def standardize_action_value(action: str) -> str:
    """
    ì•¡ì…˜ ê°’ì„ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    if not isinstance(action, str):
        return 'HOLD'
    
    action_upper = action.upper().strip()
    
    # í‘œì¤€ ì•¡ì…˜ ë§¤í•‘
    action_mapping = {
        'BUY': 'BUY',
        'STRONG_BUY': 'BUY',
        'WEAK_BUY': 'BUY_WEAK',
        'BUY_WEAK': 'BUY_WEAK',
        'SELL': 'SELL',
        'STRONG_SELL': 'SELL',
        'WEAK_SELL': 'SELL_WEAK',
        'SELL_WEAK': 'SELL_WEAK',
        'HOLD': 'HOLD',
        'WAIT': 'HOLD',
        'NEUTRAL': 'HOLD'
    }
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
    for key, value in action_mapping.items():
        if key in action_upper:
            return value
    
    # ê¸°ë³¸ê°’
    return 'HOLD'


def assess_analysis_quality(raw_response: dict, daily_data: dict) -> dict:
    """
    ë¶„ì„ í’ˆì§ˆ í‰ê°€
    """
    quality_score = 0.0
    quality_factors = []
    
    # 1. ì‘ë‹µ ì™„ì„±ë„ ì²´í¬
    if raw_response.get('comment') and len(raw_response['comment']) > 50:
        quality_score += 0.3
        quality_factors.append("ìƒì„¸í•œ ì½”ë©˜íŠ¸")
    
    # 2. ì ìˆ˜ì™€ ì‹ ë¢°ë„ ì¼ê´€ì„± ì²´í¬
    score = raw_response.get('score', 50)
    confidence = raw_response.get('confidence', 0.5)
    if abs(score/100.0 - confidence) < 0.2:  # 20% ì´ë‚´ ì°¨ì´
        quality_score += 0.2
        quality_factors.append("ì ìˆ˜-ì‹ ë¢°ë„ ì¼ê´€ì„±")
    
    # 3. ê¸°ìˆ ì  ì§€í‘œì™€ ì•¡ì…˜ ì¼ê´€ì„± ì²´í¬
    action = raw_response.get('action', 'HOLD')
    if daily_data:
        rsi = daily_data.get('rsi_14', 50)
        if action in ['BUY', 'BUY_WEAK'] and rsi < 70:
            quality_score += 0.25
            quality_factors.append("ê¸°ìˆ ì  ì§€í‘œ ì¼ê´€ì„±")
        elif action in ['SELL', 'SELL_WEAK'] and rsi > 30:
            quality_score += 0.25
            quality_factors.append("ê¸°ìˆ ì  ì§€í‘œ ì¼ê´€ì„±")
        elif action == 'HOLD':
            quality_score += 0.15
            quality_factors.append("ì¤‘ë¦½ì  íŒë‹¨")
    
    # 4. ì‘ë‹µ êµ¬ì¡° ì™„ì •ì„±
    required_fields = ['score', 'action', 'confidence']
    if all(field in raw_response for field in required_fields):
        quality_score += 0.25
        quality_factors.append("ì™„ì „í•œ ì‘ë‹µ êµ¬ì¡°")
    
    # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
    if quality_score >= 0.8:
        quality_grade = "A (ìš°ìˆ˜)"
    elif quality_score >= 0.6:
        quality_grade = "B (ì–‘í˜¸)"
    elif quality_score >= 0.4:
        quality_grade = "C (ë³´í†µ)"
    else:
        quality_grade = "D (ê°œì„  í•„ìš”)"
    
    return {
        "score": quality_score,
        "grade": quality_grade,
        "factors": quality_factors,
        "completeness": len(quality_factors) / 4  # ìµœëŒ€ 4ê°œ ìš”ì¸
    }


def create_standardized_error_response(ticker: str, error_message: str, fallback_confidence: float = 0.3) -> dict:
    """
    í‘œì¤€í™”ëœ ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
    """
    return {
        "ticker": ticker,
        "action": "HOLD",
        "confidence": fallback_confidence,
        "score": int(fallback_confidence * 100),
        "market_phase": "Stage1",  # Unknown ëŒ€ì‹  Stage1 ì‚¬ìš©
        "comment": f"ë¶„ì„ ì‹¤íŒ¨: {error_message}",
        "analysis_quality": {
            "score": 0.0,
            "grade": "F (ì‹¤íŒ¨)",
            "factors": [],
            "completeness": 0.0
        },
        "validation_status": "error",
        "raw_gpt_response": None,
        "error": error_message,
        "timestamp": datetime.now().isoformat(),
        "analysis_method": "error_fallback"
    }

# íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥
def save_trend_analysis_to_db(analysis, db_manager):
    """
    íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ì €ì¥
    analysis: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    db_manager: DBManager ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        ticker = analysis.get('ticker', '')
        action = analysis.get('action', 'HOLD')
        type_ = analysis.get('type', 'AUTO')
        reason = analysis.get('reason', '')
        pattern = analysis.get('pattern', '')
        market_phase = analysis.get('market_phase', 'Stage1')  # Unknown ëŒ€ì‹  Stage1 ì‚¬ìš©
        confidence = analysis.get('confidence', 50)
        score = analysis.get('score', 50)
        
        # Market Phase ì¡°ê±´ ì²´í¬
        if market_phase not in ("Stage 1", "Stage 2") and not analysis.get('always_save', False):
            logging.info(f"â­ï¸ {ticker}: Market Phase {market_phase}, ì €ì¥ ìƒëµë¨")
            return
        
        query = """
            INSERT INTO trend_analysis (ticker, action, type, reason, pattern, market_phase, confidence, score, time_window, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT(ticker) DO UPDATE SET
                action = excluded.action,
                type = excluded.type,
                reason = excluded.reason,
                pattern = excluded.pattern,
                market_phase = excluded.market_phase,
                confidence = excluded.confidence,
                score = excluded.score,
                updated_at = CURRENT_TIMESTAMP;
        """
        values = (ticker, action, type_, reason, pattern, market_phase, confidence, score, "ìë™ë§¤ë§¤")
        
        db_manager.execute_query(query, values)
        logging.info(f"âœ… {ticker} GPT ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (score: {score})")
        
    except Exception as e:
        logging.error(f"âŒ {analysis.get('ticker', 'Unknown')} DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")

def save_trend_analysis_log(ticker, action, confidence, time_window, db_manager):
    """
    íŠ¸ë Œë“œ ë¶„ì„ ë¡œê·¸ë¥¼ DBì— ì €ì¥
    """
    try:
        query = """
        INSERT INTO trend_analysis_log (ticker, action, confidence, time_window)
        VALUES (%s, %s, %s, %s);
        """
        values = (ticker, action, confidence, time_window)
        db_manager.execute_query(query, values)
        logging.info(f"âœ… {ticker} trend_analysis_log ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        logging.error(f"âŒ {ticker} trend_analysis_log ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - DBManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  DB ì‘ì—…ì„ í†µì¼í™”
    """
    try:
        with DBManager() as db_manager:
            logging.info("ğŸ”„ trend_analyzer ë©”ì¸ ì‹¤í–‰ ì‹œì‘")
            
            market_data = fetch_market_data()
            
            for entry in market_data:
                ticker = entry[0]
                position_avg_price = entry[19]
                last_updated_at = entry[20]

                # DBManagerë¥¼ ì‚¬ìš©í•˜ì—¬ previous_action ì¡°íšŒ
                try:
                    query = "SELECT action FROM trend_analysis WHERE ticker = %s"
                    result = db_manager.fetch_one(query, (ticker,))
                    previous_action = result[0] if result else "UNKNOWN"
                except Exception as e:
                    logging.warning(f"âš ï¸ {ticker} previous_action ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                    previous_action = "UNKNOWN"

                data = {
                    "price": entry[1], "ma_50": entry[2], "ma_200": entry[3], "rsi_14": entry[4], "mfi_14": entry[5],
                    "ht_trendline": entry[6], "fibo_382": entry[7], "fibo_618": entry[8], 
                    "r1": entry[9], "s1": entry[10], "chart_path": entry[11],
                    "position_avg_price": position_avg_price,
                    "last_updated_at": last_updated_at,
                    "previous_action": previous_action,
                    "volume_change_7_30": entry[13] if len(entry) > 13 else None,
                    "supertrend_signal": entry[14] if len(entry) > 14 else None,
                }
                
                # DBManagerë¥¼ ì‚¬ìš©í•˜ì—¬ 4H market data ì¡°íšŒ
                try:
                    query_4h = """
                        SELECT ma_50, ma_200, rsi_14, ht_trendline,
                               fibo_382, fibo_618,
                               r1, s1, chart_path
                        FROM market_data_4h
                        WHERE ticker = %s
                    """
                    row_4h = db_manager.fetch_one(query_4h, (ticker,))
                    if row_4h:
                        keys_4h = ["ma_50", "ma_200", "rsi_14", "ht_trendline",
                                   "fibo_382", "fibo_618",
                                   "r1", "s1", "chart_path"]
                        data.update({f"4h_{k}": v for k, v in zip(keys_4h, row_4h)})
                        data["4h_chart_path"] = row_4h[-1].replace("_4h", "").replace("_", "-") if row_4h[-1] else None
                except Exception as e:
                    logging.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ market_data_4h ë¡œë”© ì‹¤íŒ¨: {str(e)}")

                print(f"ğŸ“Š {ticker} ë¶„ì„ ì§„í–‰ ì¤‘...")
                if not data or not data.get("chart_path"):
                    print(f"âš ï¸ Data or chart_path not available for ticker: {ticker}. Skipping GPT analysis.")
                    logging.warning(f"âš ï¸ Data or chart_path not available for ticker: {ticker}. Skipping GPT analysis.")
                    continue
                
                # analyze_trend_with_gpt í˜¸ì¶œ ì‹œ db_manager ì „ë‹¬
                analysis_result_dict = analyze_trend_with_gpt(ticker, data, data["chart_path"], db_manager)
                
                if analysis_result_dict and analysis_result_dict.get("error") is None:
                    market_phase = analysis_result_dict.get("market_phase", "Stage1")  # Unknown ëŒ€ì‹  Stage1 ì‚¬ìš©
                    confidence = analysis_result_dict.get("confidence", 0.0)
                    action = "BUY" if market_phase and ("Stage 2" in market_phase or "Stage1->Stage2" in market_phase) and confidence >= 0.7 else "HOLD"

                    print(f"âœ… {ticker} ë¶„ì„ ì™„ë£Œ. Market Phase: {market_phase}, Confidence: {confidence:.2f}")
                    logging.info(f"âœ… {ticker} ë¶„ì„ ì™„ë£Œ. Market Phase: {market_phase}, Confidence: {confidence:.2f}. Action (derived): {action}")

                elif analysis_result_dict and analysis_result_dict.get("error"):
                    print(f"âŒ {ticker} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {analysis_result_dict.get('summary')}")
                    logging.error(f"âŒ {ticker} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {analysis_result_dict.get('summary')}, Raw Response: {analysis_result_dict.get('raw_gpt_response')}")
                else:
                    print(f"âŒ {ticker} ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    logging.error(f"âŒ {ticker} ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. Result: {analysis_result_dict}")
                    
            logging.info("âœ… trend_analyzer ë©”ì¸ ì‹¤í–‰ ì™„ë£Œ")
            
    except Exception as e:
        logging.error(f"âŒ trend_analyzer ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise

def get_recent_gpt_response(ticker, max_age_minutes=720, db_manager=None):
    """
    ìµœê·¼ GPT ì‘ë‹µ ì¤‘ ì£¼ì–´ì§„ í‹°ì»¤ì— í•´ë‹¹í•˜ëŠ” ê°€ì¥ ìµœì‹  ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    max_age_minutes: ìœ íš¨í•œ ìºì‹œì˜ ìµœëŒ€ ì‹œê°„(ê¸°ë³¸: 12ì‹œê°„ = 720ë¶„)
    db_manager: DBManager ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
    """
    should_close_db = False
    try:
        if db_manager is None:
            db_manager = DBManager()
            should_close_db = True
            
        query = """
            SELECT action, type, reason, pattern, market_phase, confidence, created_at
            FROM trend_analysis
            WHERE ticker = %s
            AND updated_at >= NOW() - INTERVAL '%s minutes'
            ORDER BY updated_at DESC
            LIMIT 1
        """
        row = db_manager.fetch_one(query, (ticker, max_age_minutes))
        if row:
            return {
                "action": row[0],
                "type": row[1],
                "reason": row[2],
                "pattern": row[3],
                "market_phase": row[4],
                "confidence": int(row[5]),
                "created_at": row[6]
            }
        else:
            return None
    except Exception as e:
        logging.error(f"âŒ ìµœê·¼ GPT ì‘ë‹µ ì¡°íšŒ ì‹¤íŒ¨ ({ticker}): {str(e)}")
        return None
    finally:
        if should_close_db and db_manager:
            db_manager.close()

def should_reuse_gpt_response(ticker, current_data, max_age_minutes=720):
    """
    ìµœê·¼ GPT ì‘ë‹µì´ ìœ íš¨í•œì§€ íŒë‹¨í•˜ê³ , ìœ íš¨í•˜ë©´ ì‘ë‹µ dictë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    max_age_minutes: ìºì‹œ ìœ íš¨ ì‹œê°„(ë¶„, ê¸°ë³¸ 720ë¶„=12ì‹œê°„)
    current_data: í˜„ì¬ ë§ˆì¼“ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (support, resistance, r1, s1 ë“± í¬í•¨)
    ë°˜í™˜ê°’: ì‘ë‹µ dict ë˜ëŠ” None
    """
    prev_response = get_recent_gpt_response(ticker, max_age_minutes)
    if not prev_response:
        return None

    # ì„¤ì • íŒŒì¼ì—ì„œ ì„ê³„ì¹˜ ë¡œë“œ (ì§€í‘œë³„ í—ˆìš© ë³€ë™ ë¹„ìœ¨)
    config = load_config()
    thresholds = config.get("gpt_cache_thresholds", {
        "support": 0.01,
        "resistance": 0.015,
        "r1": 0.015,
        "s1": 0.015
    })

    # market_data ë¹„êµë¥¼ ìœ„í•´ í•„ìš”í•œ í‚¤ ì¶”ì¶œ
    # prev_responseì—ëŠ” support, resistance ë“±ì´ ì—†ìœ¼ë¯€ë¡œ ê°€ì ¸ì˜¨ DB trend_analysis ë ˆì½”ë“œì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŒ.
    # ëŒ€ì‹ , prev_responseì˜ 'reason' ë“± ì™¸, ì´ì „ market_dataëŠ” trend_analysisì— ì €ì¥ëœ pattern/market_phaseë§Œ ìˆì–´,
    # ë”°ë¼ì„œ require to store prev market_data in DB: but for now assume prev_response includes those keys.
    # Here, we assume prev_response dict has keys: support, resistance, r1, s1.
    if has_significant_market_change(prev_response, current_data, thresholds):
        print(f"[CACHE_EXPIRE] {ticker}: ì‹œì¥ ì§€í‘œ ë³€ë™ í¼, GPT ì¬í˜¸ì¶œ í•„ìš”")
        return None

    # ìºì‹œ ì¬ì‚¬ìš©ì„ ìœ„í•œ ì§€í‘œ ë³€í™” í™•ì¸ ê²°ê³¼ ë¡œê¹…
    for key, thresh in thresholds.items():
        prev_val = prev_response.get(key)
        curr_val = current_data.get(key)
        if prev_val is None or curr_val is None:
            continue
        try:
            change_ratio = abs(curr_val - prev_val) / prev_val
        except ZeroDivisionError:
            change_ratio = abs(curr_val - prev_val)
        print(f"[CACHE_METRIC] {ticker}: {key} prev={prev_val}, curr={curr_val}, change={change_ratio:.2%}, threshold={thresh:.2%}")
    print(f"[CACHE_HIT] {ticker}: ìºì‹œ ìœ íš¨ - ëª¨ë“  ì§€í‘œ ë³€í™” {max(thresholds.values())*100:.2f}% ì´í•˜")

    return prev_response


# ì£¼ìš” ì§€í‘œì˜ ë³€í™”ê°€ ì„ê³„ê°’ ì´ìƒì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
def has_significant_market_change(prev_response, current_data, thresholds):
    """
    ì´ì „ GPT ì‘ë‹µì˜ ì£¼ìš” ì§€í‘œì™€ í˜„ì¬ ë§ˆì¼“ ë°ì´í„° ê°„ ë³€ë™ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
    prev_response: {'support': float, 'resistance': float, ...}
    current_data: {'support': float, 'resistance': float, ...}
    thresholds: dict, ì§€í‘œë³„ í—ˆìš© ë³€ë™ ë¹„ìœ¨ ì˜ˆ: {'support': 0.01, 'resistance': 0.015}
    ë°˜í™˜: True if any indicator changed by >= threshold, else False
    """
    for key, thresh in thresholds.items():
        prev_val = prev_response.get(key)
        curr_val = current_data.get(key)
        if prev_val is None or curr_val is None:
            continue
        # ìƒëŒ€ì  ë³€í™”ìœ¨ ê³„ì‚°
        try:
            change_ratio = abs(curr_val - prev_val) / prev_val
        except ZeroDivisionError:
            # prev_valì´ 0ì´ë©´ ì ˆëŒ€ ë³€í™”ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨
            change_ratio = abs(curr_val - prev_val)
        if change_ratio >= thresh:
            return True
    return False

def analyze_selected_tickers(ticker_list):
    """
    ì„ íƒëœ í‹°ì»¤ë“¤ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ - DBManager ì‚¬ìš©
    """
    try:
        with DBManager() as db_manager:
            logging.info(f"ğŸ”„ ì„ íƒëœ í‹°ì»¤ ë¶„ì„ ì‹œì‘: {ticker_list}")
            
            selected_data = fetch_selected_market_data(ticker_list)

            for entry in selected_data:
                ticker = entry[0]
                position_avg_price = entry[19]
                last_updated_at = entry[20]

                # DBManagerë¥¼ ì‚¬ìš©í•˜ì—¬ previous_action ì¡°íšŒ
                try:
                    query = "SELECT action FROM trend_analysis WHERE ticker = %s"
                    result = db_manager.fetch_one(query, (ticker,))
                    previous_action = result[0] if result else "UNKNOWN"
                except Exception as e:
                    logging.warning(f"âš ï¸ {ticker} previous_action ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                    previous_action = "UNKNOWN"

                data = {
                    "price": entry[1], "ma_50": entry[2], "ma_200": entry[3], "rsi_14": entry[4], "mfi_14": entry[5],
                    "ht_trendline": entry[6], 
                    "fibo_382": entry[7], "fibo_618": entry[8], 
                    "r1": entry[9], "s1": entry[10], "chart_path": entry[11],
                    "position_avg_price": position_avg_price,
                    "last_updated_at": last_updated_at,
                    "previous_action": previous_action,
                    "volume_change_7_30": entry[13] if len(entry) > 13 else None,
                    "supertrend_signal": entry[14] if len(entry) > 14 else None,
                }

                # DBManagerë¥¼ ì‚¬ìš©í•˜ì—¬ 4H market data ì¡°íšŒ
                try:
                    query_4h = """
                        SELECT ma_50, ma_200, rsi_14, ht_trendline,
                               fibo_382, fibo_618,
                               r1, s1, chart_path
                        FROM market_data_4h
                        WHERE ticker = %s
                    """
                    row_4h = db_manager.fetch_one(query_4h, (ticker,))
                    if row_4h:
                        keys_4h = ["ma_50", "ma_200", "rsi_14", "ht_trendline",
                                   "fibo_382", "fibo_618",
                                   "r1", "s1", "chart_path"]
                        data.update({f"4h_{k}": v for k, v in zip(keys_4h, row_4h)})
                        data["4h_chart_path"] = row_4h[-1].replace("_4h", "").replace("_", "-") if row_4h[-1] else None
                except Exception as e:
                    logging.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ market_data_4h ë¡œë”© ì‹¤íŒ¨: {str(e)}")

                print(f"ğŸ“Š {ticker} ì„ íƒ ë¶„ì„ ì§„í–‰ ì¤‘...")
                if not data or not data.get("chart_path"):
                    print(f"âš ï¸ Data or chart_path not available for ticker: {ticker}. Skipping GPT analysis.")
                    logging.warning(f"âš ï¸ Data or chart_path not available for ticker: {ticker}. Skipping GPT analysis.")
                    continue
                    
                # analyze_trend_with_gpt í˜¸ì¶œ ì‹œ db_manager ì „ë‹¬
                analysis_result_dict = analyze_trend_with_gpt(ticker, data, data["chart_path"], db_manager)

                if analysis_result_dict and analysis_result_dict.get("error") is None:
                    market_phase = analysis_result_dict.get("market_phase", "Stage1")  # Unknown ëŒ€ì‹  Stage1 ì‚¬ìš©
                    confidence = analysis_result_dict.get("confidence", 0.0)
                    action = "BUY" if market_phase and ("Stage 2" in market_phase or "Stage1->Stage2" in market_phase) and confidence >= 0.7 else "HOLD"

                    print(f"âœ… {ticker} ì„ íƒ ë¶„ì„ ì™„ë£Œ. Market Phase: {market_phase}, Confidence: {confidence:.2f}")
                    logging.info(f"âœ… {ticker} ì„ íƒ ë¶„ì„ ì™„ë£Œ. Market Phase: {market_phase}, Confidence: {confidence:.2f}. Action (derived): {action}")

                elif analysis_result_dict and analysis_result_dict.get("error"):
                    print(f"âŒ {ticker} ì„ íƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {analysis_result_dict.get('summary')}")
                    logging.error(f"âŒ {ticker} ì„ íƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {analysis_result_dict.get('summary')}, Raw Response: {analysis_result_dict.get('raw_gpt_response')}")
                else:
                    print(f"âŒ {ticker} ì„ íƒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    logging.error(f"âŒ {ticker} ì„ íƒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. Result: {analysis_result_dict}")

            logging.info("âœ… ì„ íƒëœ í‹°ì»¤ ë¶„ì„ ì™„ë£Œ")
            
    except Exception as e:
        logging.error(f"âŒ analyze_selected_tickers ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise

def call_gpt_with_chart_base64(ticker: str, chart_base64: str, indicators: dict = None, optimizer: GPTAnalysisOptimizerSingleton = None) -> dict:
    """
    ì°¨íŠ¸ ì´ë¯¸ì§€(base64)ì™€ ê¸°ìˆ ì  ì§€í‘œë¥¼ ì´ìš©í•´ GPTì—ê²Œ ë¶„ì„ì„ ìš”ì²­í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from openai import OpenAI
    import base64
    from loguru import logger

    # ì „ì—­ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    if optimizer is None:
        optimizer = GPTAnalysisOptimizerSingleton()

    def log_token_usage(model: str, messages: list, ticker: str):
        enc = tiktoken.encoding_for_model(model)
        total = 0
        for msg in messages:
            total += 3  # ë©”ì‹œì§€ í—¤ë” í† í°
            for key, val in msg.items():
                if isinstance(val, str):
                    total += len(enc.encode(val))
        total += 3  # ì‘ë‹µ í† í° í—¤ë”
        print(f"[TOKEN] {ticker} total input tokens: {total}")
        logging.info(f"[TOKEN] {ticker} total input tokens: {total}")
        return total

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    indicator_text = ""
    if indicators:
        # ì§€í‘œ ë°ì´í„° ìµœì í™”
        optimized_indicators = optimizer.optimize_for_tokens(indicators, target_tokens=1000)
        indicator_text = "\n".join([f"{k}: {v}" for k, v in optimized_indicators.items()])
    
    user_prompt = f"""
    [í‹°ì»¤] {ticker}

    [ê¸°ìˆ ì  ì§€í‘œ]
    {indicator_text}

    ì•„ë˜ ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³ , í˜„ì¬ ì¶”ì„¸ê°€ ìƒìŠ¹/í•˜ë½/íš¡ë³´ ì¤‘ ë¬´ì—‡ì¸ì§€ íŒë‹¨í•´ ì ìˆ˜(0~100)ì™€ ì„¤ëª…ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """

    start_time = time.time()
    
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))  # ëª…ì‹œì ìœ¼ë¡œ API í‚¤ ì„¤ì •
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
            {"role": "user", "content": f"[ì°¨íŠ¸ ì´ë¯¸ì§€ - base64]: {chart_base64}"}
        ]
        
        # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ ì²´í¬
        token_count = log_token_usage("gpt-4o", messages, ticker)
        if token_count > 80000:
            print(f"[SKIP] {ticker} skipped: input tokens {token_count} exceed threshold (80,000)")
            logging.warning(f"[SKIP] {ticker} skipped: input tokens {token_count} exceed threshold (80,000)")
            
            # ëª¨ë‹ˆí„°ë§ì— ê¸°ë¡
            processing_time = time.time() - start_time
            optimizer.monitor.track_api_call(token_count, processing_time, False, "token_limit_exceeded")
            
            return {
                "score": 50,
                "comment": "í† í° ìˆ˜ ì´ˆê³¼ë¡œ ë¶„ì„ ê±´ë„ˆëœ€"
            }
        
        max_retries = 3
        response = None
        
        for attempt in range(max_retries):
            try:
                # Rate limiting ì ìš©
                optimizer.manage_api_rate_limits()
                
                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    temperature=0.3,
                    max_tokens=500,
                )
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                
            except Exception as retry_error:
                error_result = optimizer.error_handler.handle_api_error(retry_error, ticker)
                
                # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                if optimizer.error_handler.should_retry(error_result.get("error_details", {}), max_retries, attempt):
                    continue
                else:
                    # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                    processing_time = time.time() - start_time
                    error_type = error_result.get("error_details", {}).get("type", "unknown")
                    optimizer.monitor.track_api_call(token_count, processing_time, False, error_type)
                    
                    # ì˜¤ë¥˜ ë¶„ì„ ë¡œê¹…
                    optimizer.error_handler.log_error_analytics(error_result.get("error_details", {}), ticker)
                    
                    return {
                        "score": error_result.get("score", 50),
                        "comment": error_result.get("error_details", {}).get("message", "ë¶„ì„ ì‹¤íŒ¨")
                    }

        # ì„±ê³µì ì¸ ì‘ë‹µ ì²˜ë¦¬
        if response:
            processing_time = time.time() - start_time
            output_tokens = response.usage.completion_tokens if hasattr(response, 'usage') else 0
            
            # ì„±ê³µ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
            optimizer.monitor.track_api_call(token_count, processing_time, True, output_tokens=output_tokens)
            
            content = response.choices[0].message.content
            
            # ğŸ” ìƒì„¸ ì‘ë‹µ ë¡œê¹… ì¶”ê°€
            logging.info(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹ :")
            logging.info(f"   - ì‘ë‹µ ê¸¸ì´: {len(content)} characters")
            logging.info(f"   - ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {content[:200]}...")
            logging.info(f"   - ì „ì²´ ì‘ë‹µ:\n{content}")
            
            # ì‘ë‹µ content ë¡œê·¸ ì¶œë ¥ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
            wrapped_content = textwrap.indent(textwrap.fill(content, width=120), prefix="â”‚ ")
            logger.info(f"[GPT RESPONSE] {ticker}\n{wrapped_content}")
            
            # ì ìˆ˜ íŒŒì‹± ê°œì„ 
            lines = content.strip().split("\n")
            score = None
            comment = ""
            
            for line in lines:
                if "ì ìˆ˜" in line and ":" in line:
                    try:
                        score_part = line.split(":")[1].strip()
                        # ìˆ«ìë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'\d+', score_part)
                        if numbers:
                            score = int(numbers[0])
                            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
                            score = max(0, min(100, score))
                    except:
                        score = None
                else:
                    comment += line + "\n"

            return {
                "score": score if score is not None else 50,
                "comment": comment.strip(),
                "processing_time": processing_time,
                "token_usage": token_count
            }

    except Exception as e:
        processing_time = time.time() - start_time
        error_result = optimizer.error_handler.handle_api_error(e, ticker)
        
        # ì‹¤íŒ¨ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
        error_type = error_result.get("error_details", {}).get("type", "unknown")
        optimizer.monitor.track_api_call(token_count if 'token_count' in locals() else 0, processing_time, False, error_type)
        
        # ì˜¤ë¥˜ ë¶„ì„ ë¡œê¹…
        optimizer.error_handler.log_error_analytics(error_result.get("error_details", {}), ticker)
        
        logger.warning(f"[{ticker}] GPT ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return {
            "score": error_result.get("score", 50),
            "comment": error_result.get("error_details", {}).get("message", "ë¶„ì„ ì‹¤íŒ¨")
        }

def analyze_trend_with_gpt_bulk(candidates: list, optimizer: GPTAnalysisOptimizerSingleton = None) -> list:
    """
    í•„í„°ë§ëœ ì¢…ëª©ì— ëŒ€í•´ GPT ê¸°ë°˜ ì¶”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ ,
    optional_score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        candidates (list): [(ticker, score)] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
        optimizer (GPTAnalysisOptimizerSingleton): ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤

    Returns:
        list: GPT ë¶„ì„ ê¸°ë°˜ ì ìˆ˜ë¡œ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
    """
    import os
    from data_fetcher import generate_chart_image
    import base64

    if optimizer is None:
        optimizer = GPTAnalysisOptimizerSingleton()

    results = []

    logging.info(f"ğŸš€ GPT ë²Œí¬ ë¶„ì„ ì‹œì‘: {len(candidates)}ê°œ ì¢…ëª©")

    for ticker, base_score in candidates:
        try:
            # ì°¨íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
            chart_path = f"charts/{ticker}.png"
            if not os.path.exists(chart_path):
                generate_chart_image(ticker)

            # ì´ë¯¸ì§€ base64 ì¸ì½”ë”©
            with open(chart_path, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")

            # GPT ë¶„ì„ ì‹¤í–‰ (ìµœì í™”ê¸° ì „ë‹¬)
            gpt_result = call_gpt_with_chart_base64(ticker, encoded, optimizer=optimizer)
            gpt_score = gpt_result.get("score", base_score)

        except Exception as e:
            import traceback
            print(f"âŒ GPT ë¶„ì„ ì‹¤íŒ¨: {ticker} | {e}")
            traceback.print_exc()
            gpt_score = base_score  # ì‹¤íŒ¨ ì‹œ ì›ë˜ ì ìˆ˜ ìœ ì§€

        results.append((ticker, gpt_score))

    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    results.sort(key=lambda x: x[1], reverse=True)
    
    # ë¶„ì„ ì™„ë£Œ í›„ íš¨ìœ¨ì„± ë¦¬í¬íŠ¸ ì¶œë ¥
    efficiency_report = optimizer.monitor.get_efficiency_report()
    cost_alert = optimizer.monitor.get_cost_alert()
    
    logging.info(f"ğŸ“Š GPT ë²Œí¬ ë¶„ì„ ì™„ë£Œ:")
    logging.info(f"   - ì´ í˜¸ì¶œìˆ˜: {efficiency_report.get('ì´_API_í˜¸ì¶œìˆ˜', 0)}")
    logging.info(f"   - ì„±ê³µë¥ : {efficiency_report.get('ì„±ê³µë¥ ', '0%')}")
    logging.info(f"   - ì˜¤ëŠ˜ ë¹„ìš©: {efficiency_report.get('ì˜¤ëŠ˜_ë¹„ìš©', '$0.0000')}")
    logging.info(f"   - ë¹„ìš© ê²½ê³ : {cost_alert.get('message', '')}")
    
    return results

# UNUSED: í˜¸ì¶œë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ë“¤
# def get_gpt_analysis_performance_report() -> dict:
#     """
#     GPT ë¶„ì„ ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     """
#     optimizer = GPTAnalysisOptimizerSingleton()
#     
#     efficiency_report = optimizer.monitor.get_efficiency_report()
#     cost_alert = optimizer.monitor.get_cost_alert()
#     
#     return {
#         "efficiency_report": efficiency_report,
#         "cost_alert": cost_alert,
#         "timestamp": datetime.now().isoformat()
#     }

# def reset_gpt_analysis_monitor():
#     """
#     GPT ë¶„ì„ ëª¨ë‹ˆí„°ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤. (ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘ ì‹œ ì‚¬ìš©)
#     """
#     global _global_optimizer
#     _global_optimizer = GPTAnalysisOptimizerSingleton()
#     logging.info("ğŸ”„ GPT ë¶„ì„ ëª¨ë‹ˆí„°ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")

# def print_gpt_analysis_summary():
#     """
#     í˜„ì¬ ì„¸ì…˜ì˜ GPT ë¶„ì„ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
#     """
#     optimizer = GPTAnalysisOptimizerSingleton()
#     report = optimizer.monitor.get_efficiency_report()
#     alert = optimizer.monitor.get_cost_alert()
#     
#     print("\n" + "="*60)
#     print("ğŸ“Š GPT ë¶„ì„ ì„±ëŠ¥ ìš”ì•½")
#     print("="*60)
#     
#     if "error" in report:
#         print(f"âš ï¸ {report['error']}")
#         return
#     
#     print(f"ì´ API í˜¸ì¶œìˆ˜: {report['ì´_API_í˜¸ì¶œìˆ˜']}")
#     print(f"ì„±ê³µë¥ : {report['ì„±ê³µë¥ ']}")
#     print(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {report['í‰ê· _ì²˜ë¦¬ì‹œê°„']}")
#     print(f"í‰ê·  í† í°ì‚¬ìš©ëŸ‰: {report['í‰ê· _í† í°ì‚¬ìš©ëŸ‰']}")
#     print(f"ì´ í† í°ì‚¬ìš©ëŸ‰: {report['ì´_í† í°ì‚¬ìš©ëŸ‰']:,}")
#     print(f"ì˜¤ëŠ˜ ë¹„ìš©: {report['ì˜¤ëŠ˜_ë¹„ìš©']}")
#     print(f"ì´ ë¹„ìš©: {report['ì´_ë¹„ìš©']}")
#     print(f"ì‹œê°„ë‹¹ í˜¸ì¶œìˆ˜: {report['ì‹œê°„ë‹¹_í˜¸ì¶œìˆ˜']}")
#     
#     if report['ì˜¤ë¥˜_ìœ í˜•ë³„_í†µê³„']:
#         print(f"\nì˜¤ë¥˜ ìœ í˜•ë³„ í†µê³„:")
#         for error_type, count in report['ì˜¤ë¥˜_ìœ í˜•ë³„_í†µê³„'].items():
#             print(f"  - {error_type}: {count}íšŒ")
#     
#     print(f"\nğŸ’° ë¹„ìš© ìƒíƒœ: {alert['message']}")
#     if alert['alert']:
#         level_emoji = "ğŸš¨" if alert['level'] == 'critical' else "âš ï¸"
#         print(f"{level_emoji} ê²½ê³  ë ˆë²¨: {alert['level']}")
#     
#     print("="*60 + "\n")

# ì „ì—­ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
_global_optimizer = None

def get_optimizer() -> GPTAnalysisOptimizerSingleton:
    """ìµœì í™”ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    return GPTAnalysisOptimizerSingleton()

def get_global_optimizer() -> GPTAnalysisOptimizerSingleton:
    """ì „ì—­ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    return GPTAnalysisOptimizerSingleton()

def analyze_trend_with_gpt_enhanced(ticker: str, daily_data: dict, daily_chart_image_path: str, db_manager: DBManager):
    """
    VCP/Stage ë¶„ì„ì„ í†µí•©í•œ í–¥ìƒëœ GPT íŠ¸ë Œë“œ ë¶„ì„ í•¨ìˆ˜
    
    Args:
        ticker (str): ë¶„ì„í•  í‹°ì»¤ ì‹¬ë³¼
        daily_data (dict): í•´ë‹¹ í‹°ì»¤ì˜ ì¼ë´‰ ì§€í‘œ ë°ì´í„°
        daily_chart_image_path (str): ì°¨íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        db_manager (DBManager): ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
        
    Returns:
        dict: í†µí•© ë¶„ì„ ê²°ê³¼
    """
    try:
        logging.info(f"ğŸ” {ticker} í–¥ìƒëœ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
        
        # 1. ê¸°ì¡´ ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì¤€ë¹„
        technical_data = prepare_technical_data_for_analysis(daily_data)
        
        # 2. VCP íŒ¨í„´ ë¶„ì„ (ì‹¤ì œ êµ¬í˜„)
        vcp_analysis = detect_vcp_pattern(technical_data)
        logging.info(f"ğŸ“Š {ticker} VCP ë¶„ì„ ì™„ë£Œ: ì ìˆ˜={vcp_analysis.get('score', 0)}")
        
        # 3. Weinstein Stage ë¶„ì„ (ì‹¤ì œ êµ¬í˜„)
        stage_analysis = analyze_weinstein_stage(technical_data)
        logging.info(f"ğŸ“ˆ {ticker} Stage ë¶„ì„ ì™„ë£Œ: {stage_analysis.get('current_stage', 'Unknown')}")
        
        # 4. ë¸Œë ˆì´í¬ì•„ì›ƒ ì¡°ê±´ í™•ì¸ (ì‹¤ì œ êµ¬í˜„)
        breakout_conditions = check_breakout_conditions(technical_data, vcp_analysis, stage_analysis)
        logging.info(f"ğŸ¯ {ticker} ë¸Œë ˆì´í¬ì•„ì›ƒ ë¶„ì„ ì™„ë£Œ: {breakout_conditions.get('action', 'HOLD')}")
        
        # 5. í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± (VCP/Stage ë¶„ì„ ê²°ê³¼ í¬í•¨)
        enhanced_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ í€€íŠ¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì•”í˜¸í™”íì˜ ì°¨íŠ¸ì™€ ê¸°ìˆ ì  ì§€í‘œë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ íˆ¬ì ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

**ê¸°ë³¸ ì •ë³´:**
í‹°ì»¤: {ticker}
í˜„ì¬ê°€: {daily_data.get('price', 'N/A')}
MA50: {daily_data.get('ma_50', 'N/A')}
MA200: {daily_data.get('ma_200', 'N/A')}
RSI: {daily_data.get('rsi_14', 'N/A')}

**VCP íŒ¨í„´ ë¶„ì„ ê²°ê³¼:**
- VCP ì ìˆ˜: {vcp_analysis.get('score', 0)}/100
- íŒ¨í„´ ìƒíƒœ: {vcp_analysis.get('pattern_status', 'Unknown')}
- ìˆ˜ì¶•ë¥ : {vcp_analysis.get('contraction_rate', 'N/A')}%

**Weinstein Stage ë¶„ì„ ê²°ê³¼:**
- í˜„ì¬ ë‹¨ê³„: {stage_analysis.get('current_stage', 'Unknown')}
- ë‹¨ê³„ ì ìˆ˜: {stage_analysis.get('stage_score', 0)}/100
- íŠ¸ë Œë“œ ê°•ë„: {stage_analysis.get('trend_strength', 'N/A')}

**ë¸Œë ˆì´í¬ì•„ì›ƒ ì¡°ê±´:**
- ê¶Œì¥ ì•¡ì…˜: {breakout_conditions.get('action', 'HOLD')}
- ì‹ ë¢°ë„: {breakout_conditions.get('confidence', 0):.2f}
- ë¸Œë ˆì´í¬ì•„ì›ƒ ì ìˆ˜: {breakout_conditions.get('breakout_score', 0)}/100

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¢…í•© ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”:
Market Phase: [Stage 1/Stage 2/Stage 3/Stage 4/Unknown]
Confidence: [0.0-1.0]
Action: [BUY/SELL/HOLD]
Summary: [VCP/Stage ë¶„ì„ì„ í¬í•¨í•œ ì¢…í•© ì˜ê²¬]
Score: [0-100ì ]
VCP_Confirmation: [VCP íŒ¨í„´ì— ëŒ€í•œ GPT ì˜ê²¬]
Stage_Confirmation: [Weinstein Stageì— ëŒ€í•œ GPT ì˜ê²¬]
"""
        
        # 6. GPT API í˜¸ì¶œ (í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        import base64
        if not os.path.exists(daily_chart_image_path):
            from data_fetcher import generate_chart_image
            generate_chart_image(ticker)
        
        with open(daily_chart_image_path, "rb") as image_file:
            chart_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        
        start_time = time.time()
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": enhanced_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{chart_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=1500,
            temperature=0.3
        )
        
        processing_time = time.time() - start_time
        gpt_response = response.choices[0].message.content
        
        # ğŸ” ë¡œê¹… ë ˆë²¨ ìµœì í™” - í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì¡°ê±´ë¶€ ë¡œê¹…
        mask_level = os.getenv("GPT_LOG_MASK_LEVEL", "medium")  # low, medium, high
        masked_response = mask_sensitive_info(gpt_response, mask_level)
        
        if os.getenv("GPT_DETAILED_LOGGING", "false").lower() == "true":
            gpt_logger.info(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹ :")
            gpt_logger.info(f"   - ì‘ë‹µ ê¸¸ì´: {len(gpt_response)} characters")
            gpt_logger.info(f"   - ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {masked_response[:200]}...")
            gpt_logger.info(f"   - ì „ì²´ ì‘ë‹µ:\n{masked_response}")
            gpt_logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            gpt_logger.info(f"   - í† í° ì‚¬ìš©ëŸ‰: {response.usage.total_tokens if hasattr(response, 'usage') else 'N/A'}")
        else:
            logging.debug(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹  (ê¸¸ì´: {len(gpt_response)}, ì‹œê°„: {processing_time:.2f}s)")
            gpt_logger.debug(f"ğŸ“¤ {ticker} GPT ë¶„ì„ ì™„ë£Œ - ì‘ë‹µê¸¸ì´: {len(gpt_response)}, ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}s")
        
        # 7. GPT ì‘ë‹µ íŒŒì‹±
        gpt_result = parse_enhanced_gpt_response(gpt_response)
        
        # 8. ìµœì¢… ê²°ê³¼ í†µí•©
        integrated_result = integrate_analysis_results(
            ticker, gpt_result, vcp_analysis, stage_analysis, breakout_conditions, db_manager
        )
        
        logging.info(f"âœ… {ticker} í–¥ìƒëœ ë¶„ì„ ì™„ë£Œ: ìµœì¢… ì‹ ë¢°ë„={integrated_result.get('confidence', 0):.2f}")
        return integrated_result
        
    except Exception as e:
        logging.error(f"âŒ {ticker} í–¥ìƒëœ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "ticker": ticker,
            "error": f"Enhanced analysis failed: {str(e)}",
            "confidence": 0.0,
            "action": "HOLD",
            "analysis_method": "error_fallback"
        }

def prepare_technical_data_for_analysis(daily_data: dict) -> dict:
    """
    daily_dataë¥¼ strategy_analyzerì˜ VCP/Stage ë¶„ì„ í•¨ìˆ˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        daily_data (dict): ì›ë³¸ ë°ì´í„° (price, ma_50, ma_200, rsi_14 ë“±)
        
    Returns:
        dict: VCP/Stage ë¶„ì„ì— í•„ìš”í•œ í˜•íƒœë¡œ ë³€í™˜ëœ ë°ì´í„°
    """
    try:
        # daily_dataì—ì„œ í•„ìš”í•œ ì§€í‘œë“¤ì„ ì¶”ì¶œí•˜ì—¬ ë³€í™˜
        technical_data = {
            'close': daily_data.get('price', 0),
            'ma_50': daily_data.get('ma_50', 0),
            'ma_200': daily_data.get('ma_200', 0),
            'rsi_14': daily_data.get('rsi_14', 50),
            'volume_change_7_30': daily_data.get('volume_change_7_30', 0),
            'high_60': daily_data.get('high_60', daily_data.get('price', 0)),
            'support': daily_data.get('s1', daily_data.get('price', 0) * 0.95),
            'resistance': daily_data.get('r1', daily_data.get('price', 0) * 1.05),
            'pivot': daily_data.get('pivot', daily_data.get('price', 0)),
            'supertrend_signal': daily_data.get('supertrend_signal', 'neutral'),
            'fibo_382': daily_data.get('fibo_382', 0),
            'fibo_618': daily_data.get('fibo_618', 0),
            'ticker': daily_data.get('ticker', '')
        }
        
        # ì¶”ê°€ ê³„ì‚° í•„ìš”í•œ ì§€í‘œë“¤
        if technical_data['ma_50'] and technical_data['ma_200']:
            technical_data['ma_trend'] = 'bullish' if technical_data['ma_50'] > technical_data['ma_200'] else 'bearish'
        else:
            technical_data['ma_trend'] = 'neutral'
            
        # RSI ìƒíƒœ ë¶„ë¥˜
        rsi = technical_data['rsi_14']
        if rsi > 70:
            technical_data['rsi_status'] = 'overbought'
        elif rsi < 30:
            technical_data['rsi_status'] = 'oversold'
        else:
            technical_data['rsi_status'] = 'neutral'
        
        # ê°€ê²© ìœ„ì¹˜ ë¶„ì„
        current_price = technical_data['close']
        support = technical_data['support']
        resistance = technical_data['resistance']
        
        if current_price and support and resistance:
            price_position = (current_price - support) / (resistance - support) if resistance != support else 0.5
            technical_data['price_position'] = max(0, min(1, price_position))  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        else:
            technical_data['price_position'] = 0.5
            
        # íŠ¸ë Œë“œ ê°•ë„ ê³„ì‚°
        if technical_data['ma_50'] and technical_data['ma_200']:
            ma_separation = abs(technical_data['ma_50'] - technical_data['ma_200']) / technical_data['ma_200']
            technical_data['trend_strength'] = min(1.0, ma_separation * 10)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        else:
            technical_data['trend_strength'] = 0.0
            
        logging.info(f"âœ… ê¸°ìˆ ì  ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {technical_data.get('ticker', 'Unknown')}")
        return technical_data
        
    except Exception as e:
        logging.error(f"âŒ technical_data ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            'close': daily_data.get('price', 0),
            'ma_50': daily_data.get('ma_50', 0),
            'ma_200': daily_data.get('ma_200', 0),
            'rsi_14': daily_data.get('rsi_14', 50),
            'volume_change_7_30': daily_data.get('volume_change_7_30', 0),
            'ma_trend': 'neutral',
            'rsi_status': 'neutral',
            'price_position': 0.5,
            'trend_strength': 0.0,
            'ticker': daily_data.get('ticker', '')
        }

def _get_empty_technical_data():
    """ë¹ˆ ê¸°ìˆ ì  ë°ì´í„° ë°˜í™˜"""
    return {
        'timestamps': [],
        'open': [],
        'high': [],
        'low': [],
        'close': [],
        'volume': [],
        'current_price': 0,
        'ma50': 0,
        'ma200': 0,
        'rsi': 50,
        'ticker': ''
    }

def _get_fallback_technical_data(daily_data):
    """ëŒ€ì²´ ê¸°ìˆ ì  ë°ì´í„° ìƒì„± (DB ì¡°íšŒ ì‹¤íŒ¨ì‹œ)"""
    current_price = daily_data.get('price', 0)
    
    # ê°€ìƒì˜ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ì„ì‹œ ë°©í¸)
    import numpy as np
    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
    
    # 100ì¼ê°„ì˜ ê°€ìƒ ë°ì´í„° ìƒì„±
    base_price = current_price if current_price > 0 else 1000
    price_changes = np.random.normal(0, 0.02, 100)  # í‰ê·  0%, í‘œì¤€í¸ì°¨ 2%
    
    close_prices = []
    for i, change in enumerate(price_changes):
        if i == 0:
            price = base_price * (0.9 + 0.1 * i / 99)  # ì ì§„ì  ìƒìŠ¹ íŠ¸ë Œë“œ
        else:
            price = close_prices[-1] * (1 + change)
        close_prices.append(max(price, base_price * 0.5))  # ìµœì†Œê°€ ì œí•œ
    
    # ì‹œê°€/ê³ ê°€/ì €ê°€ ìƒì„±
    open_prices = [close_prices[0]] + close_prices[:-1]  # ì´ì „ ì¢…ê°€ê°€ ë‹¤ìŒ ì‹œê°€
    high_prices = [p * (1 + abs(np.random.normal(0, 0.01))) for p in close_prices]
    low_prices = [p * (1 - abs(np.random.normal(0, 0.01))) for p in close_prices]
    
    # ê±°ë˜ëŸ‰ ìƒì„±
    volumes = [max(1000000, int(np.random.normal(5000000, 1000000))) for _ in range(100)]
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ê³¼ê±° 100ì¼)
    from datetime import datetime, timedelta
    timestamps = []
    for i in range(100):
        date = datetime.now() - timedelta(days=100-i)
        timestamps.append(date)
    
    technical_data = {
        'timestamps': timestamps,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': volumes,
        'current_price': close_prices[-1],
        'ma50': daily_data.get('ma_50', sum(close_prices[-50:]) / 50 if len(close_prices) >= 50 else close_prices[-1]),
        'ma200': daily_data.get('ma_200', sum(close_prices) / len(close_prices)),
        'rsi': daily_data.get('rsi_14', 50),
        'ticker': daily_data.get('ticker', '')
    }
    
    logging.info(f"âš ï¸ {daily_data.get('ticker', 'Unknown')} ëŒ€ì²´ ê¸°ìˆ ì  ë°ì´í„° ìƒì„±ë¨")
    return technical_data

def call_gpt_with_enhanced_data(ticker: str, chart_image_path: str, enhanced_data: dict):
    """
    VCP/Stage ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ GPT ë¶„ì„ ì‹¤í–‰
    """
    try:
        # ì°¨íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
        abs_image_path = chart_image_path
        if not os.path.isabs(chart_image_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            abs_image_path = os.path.join(script_dir, chart_image_path)
        
        if not os.path.isfile(abs_image_path):
            logging.warning(f"âš ï¸ ì°¨íŠ¸ ì´ë¯¸ì§€ ì—†ìŒ: {abs_image_path}")
            return {"error": "Chart image not found"}
        
        with open(abs_image_path, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
        
        # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        enhanced_prompt = create_enhanced_analysis_prompt(ticker, enhanced_data)
        
        # GPT API í˜¸ì¶œ
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": enhanced_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"{ticker} ì°¨íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ VCP íŒ¨í„´ê³¼ Weinstein Stageë¥¼ ì¢…í•©í•œ íˆ¬ì ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=800,
            temperature=0.3
        )
        
        gpt_response = response.choices[0].message.content
        
        # ğŸ” ìƒì„¸ ì‘ë‹µ ë¡œê¹… ì¶”ê°€
        logging.info(f"ğŸ“¤ {ticker} GPT ì‘ë‹µ ìˆ˜ì‹ :")
        logging.info(f"   - ì‘ë‹µ ê¸¸ì´: {len(gpt_response)} characters")
        logging.info(f"   - ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {gpt_response[:200]}...")
        logging.info(f"   - ì „ì²´ ì‘ë‹µ:\n{gpt_response}")
        
        # GPT ê²°ê³¼ íŒŒì‹±
        return parse_enhanced_gpt_response(gpt_response)
        
    except Exception as e:
        logging.error(f"âŒ {ticker} í–¥ìƒëœ GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "score": 50, "comment": "GPT ë¶„ì„ ì‹¤íŒ¨"}

def create_enhanced_analysis_prompt(ticker: str, enhanced_data: dict):
    """
    VCP/Stage ë¶„ì„ì„ í¬í•¨í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    vcp_data = enhanced_data.get('vcp', {})
    stage_data = enhanced_data.get('stage', {})
    breakout_data = enhanced_data.get('breakout', {})
    
    prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ í€€íŠ¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì°¨íŠ¸ì™€ ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ íˆ¬ì ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

=== í˜„ì¬ ë¶„ì„ ê²°ê³¼ ===

ğŸ“Š VCP íŒ¨í„´ ë¶„ì„:
- VCP ì¡´ì¬ ì—¬ë¶€: {vcp_data.get('vcp_present', False)}
- VCP ì ìˆ˜: {vcp_data.get('score', 0)}/100
- ìˆ˜ì¶• íšŸìˆ˜: {vcp_data.get('contractions', 0)}
- ê±°ë˜ëŸ‰ ê°ì†Œ: {vcp_data.get('volume_decline_pct', 0):.1f}%
- ë³€ë™ì„± íŠ¸ë Œë“œ: {vcp_data.get('volatility_trend', 'unknown')}
- ë¸Œë ˆì´í¬ì•„ì›ƒ ì¤€ë¹„: {vcp_data.get('breakout_ready', False)}

ğŸ“ˆ Weinstein Stage ë¶„ì„:
- í˜„ì¬ ë‹¨ê³„: {stage_data.get('current_stage', 'Unknown')}  
- ì‹ ë¢°ë„: {stage_data.get('stage_confidence', 0):.2f}
- MA50 ê¸°ìš¸ê¸°: {stage_data.get('ma50_slope', 0):.4f}
- MA200 ê¸°ìš¸ê¸°: {stage_data.get('ma200_slope', 0):.4f}
- ê±°ë˜ëŸ‰ íŠ¸ë Œë“œ: {stage_data.get('volume_trend', 'unknown')}

ğŸ¯ ë¸Œë ˆì´í¬ì•„ì›ƒ ì¡°ê±´:
- ê¶Œì¥ ì•¡ì…˜: {breakout_data.get('action', 'HOLD')}
- ì§„ì… ì‹ ë¢°ë„: {breakout_data.get('confidence', 0)}%
- ìœ„í—˜ëŒ€ë¹„ìˆ˜ìµë¹„: {breakout_data.get('risk_reward_ratio', 0):.2f}
- ê¶Œì¥ í¬ì§€ì…˜ í¬ê¸°: {breakout_data.get('position_size', 0):.2%}

=== ë¶„ì„ ìš”ì²­ì‚¬í•­ ===
1. ì°¨íŠ¸ë¥¼ ë³´ê³  VCP íŒ¨í„´ì˜ ì‹œê°ì  í™•ì¸
2. Weinstein Stageì˜ ì°¨íŠ¸ìƒ ì¦ê±° í™•ì¸  
3. ë¸Œë ˆì´í¬ì•„ì›ƒ ê°€ëŠ¥ì„± í‰ê°€
4. ì¢…í•© íˆ¬ì ì˜ê²¬ (ë§¤ìˆ˜/ë§¤ë„/ë³´ìœ )
5. 0-100 ì ìˆ˜ë¡œ íˆ¬ì ë§¤ë ¥ë„ í‰ê°€

ì‘ë‹µ í˜•ì‹:
{{
    "score": (0-100 ì •ìˆ˜),
    "action": "BUY/SELL/HOLD",
    "confidence": (0.0-1.0 ì‹¤ìˆ˜),
    "comment": "ìƒì„¸ ë¶„ì„ ì˜ê²¬",
    "vcp_confirmation": "VCP íŒ¨í„´ ì‹œê°ì  í™•ì¸ ê²°ê³¼",
    "stage_confirmation": "Stage ë¶„ì„ ì°¨íŠ¸ ì¦ê±°",
    "risk_assessment": "ë¦¬ìŠ¤í¬ í‰ê°€"
}}
"""
    
    return prompt

def parse_enhanced_gpt_response(gpt_response: str):
    """
    í–¥ìƒëœ GPT ì‘ë‹µ íŒŒì‹± - ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì› ë° ê°•í™”ëœ ì •ê·œì‹
    """
    if not gpt_response or not gpt_response.strip():
        logging.error("âŒ ë¹ˆ GPT ì‘ë‹µ ìˆ˜ì‹ ")
        return _get_fallback_parse_result("ë¹ˆ ì‘ë‹µ")
    
    try:
        # 1. JSON í˜•ì‹ ìš°ì„  ì²˜ë¦¬
        json_result = _parse_json_response(gpt_response)
        if json_result:
            logging.debug("âœ… JSON í˜•ì‹ ì‘ë‹µ íŒŒì‹± ì„±ê³µ")
            return json_result
        
        # 2. í…ìŠ¤íŠ¸ í˜•ì‹ íŒŒì‹± - ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›
        text_result = _parse_text_response(gpt_response)
        if text_result:
            logging.debug("âœ… í…ìŠ¤íŠ¸ í˜•ì‹ ì‘ë‹µ íŒŒì‹± ì„±ê³µ")
            return text_result
        
        # 3. ìµœí›„ ìˆ˜ë‹¨ - í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì‹±
        fallback_result = _parse_with_keywords(gpt_response)
        logging.warning("âš ï¸ í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´")
        return fallback_result
            
    except Exception as e:
        error_msg = f"GPT ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logging.error(f"âŒ {error_msg}")
        return _get_fallback_parse_result(error_msg)

def _parse_json_response(gpt_response: str) -> dict:
    """JSON í˜•ì‹ ì‘ë‹µ íŒŒì‹±"""
    try:
        # JSON ë¸”ë¡ ì°¾ê¸° (ì¤‘ì²©ëœ JSONë„ ì§€ì›)
        json_patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # ë‹¨ìˆœ JSON
            r'```json\s*(\{.*?\})\s*```',        # ë§ˆí¬ë‹¤ìš´ JSON ë¸”ë¡
            r'```\s*(\{.*?\})\s*```',            # ì¼ë°˜ ì½”ë“œ ë¸”ë¡
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, gpt_response, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    # íŒ¨í„´ì— ë”°ë¼ ë§¤ì¹˜ ì²˜ë¦¬
                    json_str = match if isinstance(match, str) else match
                    if not json_str.strip().startswith('{'):
                        continue
                        
                    result = json.loads(json_str)
                    
                    # í•„ìˆ˜ í•„ë“œ ê²€ì¦ ë° ì •ê·œí™”
                    return _normalize_json_result(result)
                    
                except json.JSONDecodeError:
                    continue
        
        return None
        
    except Exception as e:
        logging.debug(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

def _parse_text_response(gpt_response: str) -> dict:
    """í…ìŠ¤íŠ¸ í˜•ì‹ ì‘ë‹µ íŒŒì‹± - ê°œì„ ëœ ì •ê·œì‹"""
    try:
        result = {
            "score": 50,
            "action": "HOLD",
            "confidence": 0.5,
            "comment": "",
            "vcp_confirmation": "",
            "stage_confirmation": "",
            "risk_assessment": ""
        }
        
        # ì ìˆ˜ ì¶”ì¶œ ì •ê·œì‹ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        score_patterns = [
            r'ì ìˆ˜[:ï¼š]\s*(\d+(?:\.\d+)?)',           # "ì ìˆ˜: 85"
            r'Score[:ï¼š]\s*(\d+(?:\.\d+)?)',          # "Score: 85"
            r'í‰ì \s*(\d+(?:\.\d+)?)ì ',              # "í‰ì  85ì "
            r'(\d+(?:\.\d+)?)/100',                   # "85/100"
            r'(\d+(?:\.\d+)?)%',                      # "85%"
            r'ìŠ¤ì½”ì–´[:ï¼š]\s*(\d+(?:\.\d+)?)',         # "ìŠ¤ì½”ì–´: 85"
            r'Rating[:ï¼š]\s*(\d+(?:\.\d+)?)',         # "Rating: 85"
        ]
        
        for pattern in score_patterns:
            match = re.search(pattern, gpt_response, re.IGNORECASE)
            if match:
                score = float(match.group(1))
                # ì ìˆ˜ ë²”ìœ„ ì •ê·œí™” (0-100)
                if score > 100:
                    score = min(score, 100)
                elif score <= 1:  # 0-1 ë²”ìœ„ì¸ ê²½ìš°
                    score = score * 100
                result["score"] = int(score)
                break
        
        # ì•¡ì…˜ ì¶”ì¶œ
        action_patterns = [
            r'Action[:ï¼š]\s*(BUY|SELL|HOLD)',
            r'ì•¡ì…˜[:ï¼š]\s*(BUY|SELL|HOLD|ë§¤ìˆ˜|ë§¤ë„|ë³´ìœ )',
            r'ê¶Œì¥[:ï¼š]\s*(BUY|SELL|HOLD|ë§¤ìˆ˜|ë§¤ë„|ë³´ìœ )',
            r'Recommendation[:ï¼š]\s*(BUY|SELL|HOLD)',
        ]
        
        for pattern in action_patterns:
            match = re.search(pattern, gpt_response, re.IGNORECASE)
            if match:
                action = match.group(1).upper()
                # í•œêµ­ì–´ ì•¡ì…˜ ë³€í™˜
                action_map = {"ë§¤ìˆ˜": "BUY", "ë§¤ë„": "SELL", "ë³´ìœ ": "HOLD"}
                result["action"] = action_map.get(action, action)
                break
        
        # ì‹ ë¢°ë„ ì¶”ì¶œ
        confidence_patterns = [
            r'Confidence[:ï¼š]\s*(\d+(?:\.\d+)?)',
            r'ì‹ ë¢°ë„[:ï¼š]\s*(\d+(?:\.\d+)?)',
            r'í™•ì‹ ë„[:ï¼š]\s*(\d+(?:\.\d+)?)',
        ]
        
        for pattern in confidence_patterns:
            match = re.search(pattern, gpt_response, re.IGNORECASE)
            if match:
                conf = float(match.group(1))
                # ì‹ ë¢°ë„ ì •ê·œí™” (0-1)
                if conf > 1:
                    conf = conf / 100
                result["confidence"] = min(max(conf, 0), 1)
                break
        
        # ì½”ë©˜íŠ¸ ì¶”ì¶œ
        result["comment"] = gpt_response[:500]  # ì²˜ìŒ 500ì
        
        # ì¶”ê°€ í•„ë“œ ì¶”ì¶œ
        if "VCP" in gpt_response:
            vcp_match = re.search(r'VCP[^.]*\.', gpt_response, re.IGNORECASE)
            if vcp_match:
                result["vcp_confirmation"] = vcp_match.group(0)
        
        if "Stage" in gpt_response:
            stage_match = re.search(r'Stage[^.]*\.', gpt_response, re.IGNORECASE)
            if stage_match:
                result["stage_confirmation"] = stage_match.group(0)
        
        return result
        
    except Exception as e:
        logging.error(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def _parse_with_keywords(gpt_response: str) -> dict:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë§ˆì§€ë§‰ ìˆ˜ë‹¨ íŒŒì‹±"""
    try:
        # ê¸ì •ì /ë¶€ì •ì  í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¶”ì •
        positive_keywords = ["ìƒìŠ¹", "ë§¤ìˆ˜", "BUY", "ì¢‹ì€", "ê°•ì„¸", "breakthrough", "bullish"]
        negative_keywords = ["í•˜ë½", "ë§¤ë„", "SELL", "ë‚˜ìœ", "ì•½ì„¸", "bearish", "sell"]
        
        positive_count = sum(1 for word in positive_keywords if word.lower() in gpt_response.lower())
        negative_count = sum(1 for word in negative_keywords if word.lower() in gpt_response.lower())
        
        if positive_count > negative_count:
            score = 60 + min(positive_count * 5, 25)
            action = "BUY"
        elif negative_count > positive_count:
            score = 40 - min(negative_count * 5, 25)
            action = "SELL"
        else:
            score = 50
            action = "HOLD"
        
        return {
            "score": score,
            "action": action,
            "confidence": 0.4,  # ë‚®ì€ ì‹ ë¢°ë„
            "comment": gpt_response[:300],
            "vcp_confirmation": "",
            "stage_confirmation": "",
            "risk_assessment": "",
            "parsing_method": "keyword_fallback"
        }
        
    except Exception as e:
        logging.error(f"í‚¤ì›Œë“œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return _get_fallback_parse_result(f"í‚¤ì›Œë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")

def _normalize_json_result(result: dict) -> dict:
    """JSON ê²°ê³¼ ì •ê·œí™” ë° ê²€ì¦"""
    try:
        normalized = {
            "score": int(result.get("score", 50)),
            "action": str(result.get("action", "HOLD")).upper(),
            "confidence": float(result.get("confidence", 0.5)),
            "comment": str(result.get("comment", "")),
            "vcp_confirmation": str(result.get("vcp_confirmation", "")),
            "stage_confirmation": str(result.get("stage_confirmation", "")),
            "risk_assessment": str(result.get("risk_assessment", ""))
        }
        
        # ê°’ ë²”ìœ„ ê²€ì¦ ë° ë³´ì •
        normalized["score"] = max(0, min(100, normalized["score"]))
        normalized["confidence"] = max(0, min(1, normalized["confidence"]))
        
        if normalized["action"] not in ["BUY", "SELL", "HOLD"]:
            normalized["action"] = "HOLD"
        
        return normalized
        
    except Exception as e:
        logging.error(f"JSON ê²°ê³¼ ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
        return _get_fallback_parse_result(f"ì •ê·œí™” ì‹¤íŒ¨: {e}")

def _get_fallback_parse_result(error_msg: str) -> dict:
    """íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜"""
    return {
        "score": 50,
        "action": "HOLD",
        "confidence": 0.3,
        "comment": f"íŒŒì‹± ì‹¤íŒ¨: {error_msg}",
        "vcp_confirmation": "",
        "stage_confirmation": "",
        "risk_assessment": "",
        "error": error_msg,
        "parsing_method": "fallback"
    }

def integrate_analysis_results(ticker: str, gpt_result: dict, vcp_analysis: dict, 
                             stage_analysis: dict, breakout_conditions: dict, db_manager: DBManager):
    """
    GPT, VCP, Stage ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ì˜ì‚¬ê²°ì • ìƒì„± (ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ + ë¶„í•  ë§¤ìˆ˜ ì§€ì›)
    """
    try:
        # í†µí•© ì ìˆ˜ ê³„ì‚° (VCP 30%, Stage 40%, ë¸Œë ˆì´í¬ì•„ì›ƒ 30%)
        unified_score_result = calculate_unified_score(vcp_analysis, stage_analysis, breakout_conditions)
        
        # ë¶„í•  ë§¤ìˆ˜ ì¡°ê±´ ì²´í¬
        scaling_conditions = check_scaling_in_conditions(vcp_analysis, stage_analysis, breakout_conditions)
        
        # í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
        position_size = calculate_position_size(unified_score_result['unified_score'], scaling_conditions)
        
        # ìµœì¢… ì•¡ì…˜ ê²°ì • (í†µí•© ì ìˆ˜ ê¸°ë°˜)
        final_action = unified_score_result['action']
        action_confidence = unified_score_result['unified_score'] * 100
        
        # GPT ê²°ê³¼ì™€ í†µí•© ë¶„ì„ ê²°ê³¼ ë¹„êµ ì¡°ì •
        gpt_action = gpt_result.get('action', 'HOLD')
        gpt_confidence = gpt_result.get('confidence', 0.5)
        
        # í†µí•© ì ìˆ˜ì™€ GPT ì‹ ë¢°ë„ ì°¨ì´ê°€ í´ ë•Œ ë³´ì •
        if abs(unified_score_result['unified_score'] - gpt_confidence) > 0.3:
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì‹ ë¢°ë„ ì¡°ì • (í†µí•© ë¶„ì„ 70%, GPT 30%)
            adjusted_confidence = (
                unified_score_result['unified_score'] * 0.7 + 
                gpt_confidence * 0.3
            )
            action_confidence = adjusted_confidence * 100
            
            logging.info(f"ğŸ“Š {ticker} ì‹ ë¢°ë„ ë³´ì •: {unified_score_result['unified_score']:.2f} â†’ {adjusted_confidence:.2f}")
        
        # ì•¡ì…˜ ì¼ì¹˜ì„± ê²€ì¦ ë° ì¡°ì •
        if gpt_action != final_action:
            if gpt_confidence > 0.8 and unified_score_result['unified_score'] < 0.6:
                # GPT ì‹ ë¢°ë„ê°€ ë†’ê³  í†µí•© ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ë³´ìˆ˜ì  ì¡°ì •
                final_action = 'HOLD'
                action_confidence = min(action_confidence, 70)
                logging.warning(f"âš ï¸ {ticker} ì•¡ì…˜ ë¶ˆì¼ì¹˜ë¡œ ë³´ìˆ˜ì  ì¡°ì •: {final_action}")
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë ˆì´ì–´ ì¶”ê°€
        risk_adjusted_result = apply_risk_management_layer(
            ticker, final_action, action_confidence, unified_score_result, 
            vcp_analysis, stage_analysis, breakout_conditions
        )
        
        # ìµœì¢… í†µí•© ê²°ê³¼
        integrated_result = {
            "ticker": ticker,
            "action": risk_adjusted_result.get('final_action', final_action),
            "confidence": risk_adjusted_result.get('final_confidence', action_confidence) / 100,
            "action_confidence": risk_adjusted_result.get('final_confidence', action_confidence),
            "unified_score": unified_score_result['unified_score'],
            "market_phase": stage_analysis.get('current_stage', 'Unknown'),
            "vcp_score": vcp_analysis.get('score', 0),
            "stage": stage_analysis.get('current_stage', 'Unknown'),
            "stage_confidence": stage_analysis.get('stage_confidence', 0),
            "breakout_ready": vcp_analysis.get('breakout_ready', False),
            "position_size": position_size,
            "risk_reward_ratio": breakout_conditions.get('risk_reward_ratio', 0),
            "entry_price": breakout_conditions.get('entry_price', 0),
            "stop_loss": breakout_conditions.get('stop_loss', 0),
            "target_price": breakout_conditions.get('target_price', 0),
            "scaling_conditions": scaling_conditions,
            "risk_management": risk_adjusted_result,
            "analysis_details": {
                "vcp": vcp_analysis,
                "stage": stage_analysis,
                "breakout": breakout_conditions,
                "gpt": gpt_result,
                "unified_score_breakdown": unified_score_result
            },
            "summary": create_enhanced_analysis_summary(
                vcp_analysis, stage_analysis, breakout_conditions, gpt_result, 
                unified_score_result, scaling_conditions
            ),
            "timestamp": datetime.now().isoformat(),
            "analysis_method": "enhanced_integrated_v2"
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— í†µí•© ê²°ê³¼ ì €ì¥
        save_enhanced_analysis_to_db(integrated_result, db_manager)
        
        return integrated_result
        
    except Exception as e:
        logging.error(f"âŒ {ticker} ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            "ticker": ticker,
            "error": f"Integration failed: {str(e)}",
            "confidence": 0.0,
            "action": "HOLD"
        }

def calculate_unified_score(vcp_analysis: dict, stage_analysis: dict, breakout_conditions: dict) -> dict:
    """
    VCP, Stage, ë¸Œë ˆì´í¬ì•„ì›ƒ ì¡°ê±´ì„ í†µí•©í•œ ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°
    
    ê°€ì¤‘ì¹˜: VCP 30%, Stage 40%, ë¸Œë ˆì´í¬ì•„ì›ƒ 30%
    """
    try:
        # ê°€ì¤‘ì¹˜ ì„¤ì • (ì‚¬ìš©ì ì˜ì‚¬ì½”ë“œ ê¸°ë°˜)
        vcp_weight = 0.3
        stage_weight = 0.4
        breakout_weight = 0.3
        
        # ê° ë¶„ì„ ê²°ê³¼ë¥¼ 0-1 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        vcp_score = vcp_analysis.get('score', 0) / 100.0
        
        # Weinstein Stageë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
        stage_score = convert_stage_to_score(stage_analysis.get('current_stage', 'Unknown'))
        
        # ë¸Œë ˆì´í¬ì•„ì›ƒ ì‹ ë¢°ë„ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
        breakout_score = breakout_conditions.get('confidence', 0) / 100.0
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        unified_score = (
            vcp_score * vcp_weight + 
            stage_score * stage_weight + 
            breakout_score * breakout_weight
        )
        
        # ì•¡ì…˜ ê²°ì • ë¡œì§
        action = determine_action(unified_score)
        
        logging.info(f"ğŸ“Š í†µí•© ì ìˆ˜ ê³„ì‚°: VCP={vcp_score:.2f}, Stage={stage_score:.2f}, Breakout={breakout_score:.2f} â†’ í†µí•©={unified_score:.2f}")
        
        return {
            'unified_score': unified_score,
            'action': action,
            'vcp_contribution': vcp_score * vcp_weight,
            'stage_contribution': stage_score * stage_weight,
            'breakout_contribution': breakout_score * breakout_weight,
            'breakdown': {
                'vcp_score': vcp_score,
                'stage_score': stage_score,
                'breakout_score': breakout_score,
                'weights': {
                    'vcp': vcp_weight,
                    'stage': stage_weight,
                    'breakout': breakout_weight
                }
            }
        }
        
    except Exception as e:
        logging.error(f"âŒ í†µí•© ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            'unified_score': 0.5,  # ì¤‘ë¦½ê°’
            'action': 'HOLD',
            'error': str(e)
        }


def convert_stage_to_score(current_stage: str) -> float:
    """
    Weinstein Stageë¥¼ 0-1 ì ìˆ˜ë¡œ ë³€í™˜
    """
    stage_scores = {
        'Stage 1': 0.2,   # ë°”ë‹¥ê¶Œ (ê´€ì‹¬)
        'Stage 2': 0.8,   # ìƒìŠ¹ (ë§¤ìˆ˜)
        'Stage 3': 0.4,   # ê³ ì ê¶Œ (ê´€ì‹¬)
        'Stage 4': 0.1,   # í•˜ë½ (ë§¤ë„)
        'Unknown': 0.5,   # ì¤‘ë¦½
        'Transition': 0.6  # ì „í™˜ê¸°
    }
    
    return stage_scores.get(current_stage, 0.5)


def determine_action(unified_score: float) -> str:
    """
    í†µí•© ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¤ë§¤ ì•¡ì…˜ ê²°ì •
    """
    if unified_score >= 0.75:
        return 'BUY'
    elif unified_score >= 0.6:
        return 'BUY_WEAK'  # ì•½í•œ ë§¤ìˆ˜ ì‹ í˜¸
    elif unified_score <= 0.25:
        return 'SELL'
    elif unified_score <= 0.4:
        return 'SELL_WEAK'  # ì•½í•œ ë§¤ë„ ì‹ í˜¸
    else:
        return 'HOLD'


def check_scaling_in_conditions(vcp_analysis: dict, stage_analysis: dict, breakout_conditions: dict) -> dict:
    """
    ë¶„í•  ë§¤ìˆ˜ íŠ¸ë¦¬ê±° ì¡°ê±´ ì²´í¬
    """
    try:
        scaling_conditions = {
            'initial_entry_complete': False,
            'additional_buy_triggers': [],
            'scaling_strategy': 'none',
            'max_position_additions': 0,
            'next_buy_level': None,
            'risk_per_addition': 0.02  # ì¶”ê°€ ë§¤ìˆ˜ë‹¹ 2% ë¦¬ìŠ¤í¬
        }
        
        # ì´ˆê¸° ì§„ì… ì¡°ê±´ ì²´í¬
        if (vcp_analysis.get('score', 0) >= 60 and 
            stage_analysis.get('current_stage') == 'Stage 2' and
            breakout_conditions.get('confidence', 0) >= 70):
            
            scaling_conditions['initial_entry_complete'] = True
            scaling_conditions['scaling_strategy'] = 'pyramid'
            scaling_conditions['max_position_additions'] = 2
            
            # ì¶”ê°€ ë§¤ìˆ˜ íŠ¸ë¦¬ê±° ì¡°ê±´ ì„¤ì •
            current_price = breakout_conditions.get('entry_price', 0)
            if current_price > 0:
                # 5% ìƒìŠ¹ í›„ ì¶”ê°€ ë§¤ìˆ˜ ê³ ë ¤
                scaling_conditions['additional_buy_triggers'].append({
                    'condition': 'price_breakout',
                    'trigger_price': current_price * 1.05,
                    'reason': 'ì´ˆê¸° ë¸Œë ˆì´í¬ì•„ì›ƒ í™•ì¸ í›„ ì¶”ê°€ ì§„ì…',
                    'position_addition': 0.5  # ì´ˆê¸° í¬ì§€ì…˜ì˜ 50% ì¶”ê°€
                })
                
                # ë³¼ë¥¨ ê¸‰ì¦ ì‹œ ì¶”ê°€ ë§¤ìˆ˜
                if vcp_analysis.get('volume_surge', False):
                    scaling_conditions['additional_buy_triggers'].append({
                        'condition': 'volume_surge',
                        'trigger_volume_ratio': 2.0,  # í‰ê·  ê±°ë˜ëŸ‰ì˜ 2ë°°
                        'reason': 'ê±°ë˜ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ê´€ì‹¬ë„ ì¦ê°€',
                        'position_addition': 0.3
                    })
        
        # ìœ„í—˜ë„ê°€ ë†’ì€ ê²½ìš° ë¶„í•  ë§¤ìˆ˜ ë¹„í™œì„±í™”
        if breakout_conditions.get('risk_reward_ratio', 0) < 2.0:
            scaling_conditions['scaling_strategy'] = 'single_entry'
            scaling_conditions['max_position_additions'] = 0
            scaling_conditions['additional_buy_triggers'] = []
        
        logging.info(f"ğŸ¯ ë¶„í•  ë§¤ìˆ˜ ì¡°ê±´: {scaling_conditions['scaling_strategy']}, ìµœëŒ€ ì¶”ê°€: {scaling_conditions['max_position_additions']}íšŒ")
        
        return scaling_conditions
        
    except Exception as e:
        logging.error(f"âŒ ë¶„í•  ë§¤ìˆ˜ ì¡°ê±´ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {'scaling_strategy': 'none', 'error': str(e)}


def calculate_position_size(unified_score: float, scaling_conditions: dict) -> float:
    """
    í†µí•© ì ìˆ˜ì™€ ë¶„í•  ë§¤ìˆ˜ ì¡°ê±´ì— ê¸°ë°˜í•œ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
    """
    try:
        base_position_size = 0.0
        
        # ê¸°ë³¸ í¬ì§€ì…˜ í¬ê¸° (í†µí•© ì ìˆ˜ ê¸°ë°˜)
        if unified_score >= 0.8:
            base_position_size = 0.15  # 15%
        elif unified_score >= 0.7:
            base_position_size = 0.12  # 12%
        elif unified_score >= 0.6:
            base_position_size = 0.08  # 8%
        elif unified_score >= 0.5:
            base_position_size = 0.05  # 5%
        else:
            base_position_size = 0.0   # ì§„ì…í•˜ì§€ ì•ŠìŒ
        
        # ë¶„í•  ë§¤ìˆ˜ ì „ëµì— ë”°ë¥¸ ì¡°ì •
        if scaling_conditions.get('scaling_strategy') == 'pyramid':
            # ë¶„í•  ë§¤ìˆ˜ ì‹œ ì´ˆê¸° í¬ì§€ì…˜ í¬ê¸° ì¶•ì†Œ
            initial_ratio = 0.6  # ì´ˆê¸°ì— 60%ë§Œ ì§„ì…
            base_position_size *= initial_ratio
            
            logging.info(f"ğŸ“Š ë¶„í•  ë§¤ìˆ˜ ì „ëµ: ì´ˆê¸° í¬ì§€ì…˜ {base_position_size:.1%} (ì „ì²´ ê³„íšì˜ {initial_ratio:.0%})")
        
        # ë¦¬ìŠ¤í¬ ê´€ë¦¬: ìµœëŒ€ í¬ì§€ì…˜ í¬ê¸° ì œí•œ
        max_position_size = 0.2  # 20% ì œí•œ
        position_size = min(base_position_size, max_position_size)
        
        return round(position_size, 3)
        
    except Exception as e:
        logging.error(f"âŒ í¬ì§€ì…˜ í¬ê¸° ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return 0.0


def apply_risk_management_layer(ticker: str, action: str, confidence: float, 
                              unified_result: dict, vcp_analysis: dict, 
                              stage_analysis: dict, breakout_conditions: dict) -> dict:
    """
    ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë ˆì´ì–´ ì ìš©
    """
    try:
        risk_factors = []
        risk_score = 0.0
        final_action = action
        final_confidence = confidence
        
        # 1. ì‹œì¥ ë³€ë™ì„± ì²´í¬
        if vcp_analysis.get('volatility_trend') == 'increasing':
            risk_factors.append('ë†’ì€ ë³€ë™ì„±')
            risk_score += 0.2
        
        # 2. ê±°ë˜ëŸ‰ í™•ì¸
        volume_trend = stage_analysis.get('volume_trend', 'unknown')
        if volume_trend == 'declining' and action in ['BUY', 'BUY_WEAK']:
            risk_factors.append('ê±°ë˜ëŸ‰ ê°ì†Œ')
            risk_score += 0.15
        
        # 3. ìœ„í—˜-ìˆ˜ìµ ë¹„ìœ¨ ì²´í¬
        risk_reward = breakout_conditions.get('risk_reward_ratio', 0)
        if risk_reward < 2.0 and action in ['BUY', 'BUY_WEAK']:
            risk_factors.append('ë‚®ì€ ìœ„í—˜-ìˆ˜ìµ ë¹„ìœ¨')
            risk_score += 0.25
        
        # 4. Stage ë¶„ì„ ìœ„í—˜ ìš”ì†Œ
        current_stage = stage_analysis.get('current_stage', 'Unknown')
        if current_stage in ['Stage 3', 'Stage 4'] and action in ['BUY', 'BUY_WEAK']:
            risk_factors.append(f'ë¶€ì ì ˆí•œ ë§¤ë§¤ì‹œì  ({current_stage})')
            risk_score += 0.3
        
        # 5. ë¦¬ìŠ¤í¬ ì ìˆ˜ì— ë”°ë¥¸ ì•¡ì…˜ ì¡°ì •
        if risk_score >= 0.5:  # ê³ ìœ„í—˜
            if action in ['BUY', 'BUY_WEAK']:
                final_action = 'HOLD'
                final_confidence = min(final_confidence, 60)
                risk_factors.append('ê³ ìœ„í—˜ìœ¼ë¡œ ë§¤ìˆ˜ ì œí•œ')
                
                logging.warning(f"ğŸš¨ {ticker} ê³ ìœ„í—˜ ê°ì§€ (ì ìˆ˜: {risk_score:.2f}) - ë§¤ìˆ˜ â†’ ê´€ë§")
        
        elif risk_score >= 0.3:  # ì¤‘ìœ„í—˜
            if action == 'BUY':
                final_action = 'BUY_WEAK'
                final_confidence = min(final_confidence, 75)
                
                logging.warning(f"âš ï¸ {ticker} ì¤‘ìœ„í—˜ ê°ì§€ (ì ìˆ˜: {risk_score:.2f}) - ë§¤ìˆ˜ ì„¸ê¸° ì¶•ì†Œ")
        
        return {
            'risk_score': risk_score,
            'risk_factors': risk_factors,
            'risk_level': 'ê³ ìœ„í—˜' if risk_score >= 0.5 else 'ì¤‘ìœ„í—˜' if risk_score >= 0.3 else 'ì €ìœ„í—˜',
            'original_action': action,
            'final_action': final_action,
            'original_confidence': confidence,
            'final_confidence': final_confidence,
            'adjustments_made': len(risk_factors) > 0
        }
        
    except Exception as e:
        logging.error(f"âŒ {ticker} ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì ìš© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            'risk_score': 0.5,
            'risk_level': 'ì¤‘ìœ„í—˜',
            'final_action': 'HOLD',
            'final_confidence': 50,
            'error': str(e)
        }


def create_enhanced_analysis_summary(vcp_analysis: dict, stage_analysis: dict, 
                                   breakout_conditions: dict, gpt_result: dict,
                                   unified_result: dict, scaling_conditions: dict) -> str:
    """
    í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
    """
    vcp_status = "ğŸŸ¢ ìš°ìˆ˜" if vcp_analysis.get('score', 0) >= 70 else "ğŸŸ¡ ë³´í†µ" if vcp_analysis.get('score', 0) >= 50 else "ğŸ”´ ë¶ˆëŸ‰"
    stage = stage_analysis.get('current_stage', 'Unknown')
    action = unified_result.get('action', 'HOLD')
    unified_score = unified_result.get('unified_score', 0)
    scaling_strategy = scaling_conditions.get('scaling_strategy', 'none')
    
    summary = f"""
ğŸ“Š VCP íŒ¨í„´: {vcp_status} (ì ìˆ˜: {vcp_analysis.get('score', 0)})
ğŸ“ˆ Weinstein Stage: {stage} (ì‹ ë¢°ë„: {stage_analysis.get('stage_confidence', 0):.1%})
ğŸ¯ í†µí•© ì ìˆ˜: {unified_score:.1%} â†’ ê¶Œì¥ ì•¡ì…˜: {action}
ğŸ—ï¸ ì§„ì… ì „ëµ: {scaling_strategy} (ìµœëŒ€ {scaling_conditions.get('max_position_additions', 0)}íšŒ ì¶”ê°€)
ğŸ’¡ GPT ì˜ê²¬: {gpt_result.get('comment', 'ì—†ìŒ')[:100]}...
"""
    
    return summary.strip()


def create_analysis_summary(vcp_analysis: dict, stage_analysis: dict, 
                          breakout_conditions: dict, gpt_result: dict):
    """
    ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    """
    vcp_status = "ğŸŸ¢ ìš°ìˆ˜" if vcp_analysis.get('score', 0) >= 70 else "ğŸŸ¡ ë³´í†µ" if vcp_analysis.get('score', 0) >= 50 else "ğŸ”´ ë¶ˆëŸ‰"
    stage = stage_analysis.get('current_stage', 'Unknown')
    action = breakout_conditions.get('action', 'HOLD')
    
    summary = f"""
ğŸ“Š VCP íŒ¨í„´: {vcp_status} (ì ìˆ˜: {vcp_analysis.get('score', 0)})
ğŸ“ˆ Weinstein Stage: {stage} (ì‹ ë¢°ë„: {stage_analysis.get('stage_confidence', 0):.1%})
ğŸ¯ ê¶Œì¥ ì•¡ì…˜: {action} (ì‹ ë¢°ë„: {breakout_conditions.get('confidence', 0)}%)
ğŸ’¡ GPT ì˜ê²¬: {gpt_result.get('comment', '')[:100]}...
"""
    
    return summary.strip()

def save_enhanced_analysis_to_db(result: dict, db_manager: DBManager):
    """
    í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    """
    try:
        # score í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
        score = result.get('score', 50)  # ê¸°ë³¸ê°’ 50
        confidence = result.get('confidence', 0.5)
        
        # trend_analysis í…Œì´ë¸”ì— ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        db_manager.execute_query("""
            INSERT INTO trend_analysis (
                ticker, action, type, reason, pattern, 
                market_phase, confidence, score, time_window, 
                vcp_score, stage_analysis, created_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT(ticker) DO UPDATE SET
                action = excluded.action,
                type = excluded.type,
                reason = excluded.reason,
                pattern = excluded.pattern,
                market_phase = excluded.market_phase,
                confidence = excluded.confidence,
                score = excluded.score,
                vcp_score = excluded.vcp_score,
                stage_analysis = excluded.stage_analysis,
                updated_at = CURRENT_TIMESTAMP
        """, (
            result['ticker'],
            result['action'],
            'enhanced_analysis',
            result.get('summary', ''),
            'VCP+Stage',
            result['market_phase'],
            result['confidence'],
            score,
            'ìë™ë§¤ë§¤',
            result['vcp_score'],
            json.dumps(result['analysis_details'], ensure_ascii=False)
        ))
        
        logging.info(f"âœ… {result['ticker']} í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ (score: {score})")
        
    except Exception as e:
        ticker = result.get('ticker', 'UNKNOWN')
        score = result.get('score', 'N/A')
        confidence = result.get('confidence', 'N/A')
        action = result.get('action', 'N/A')
        market_phase = result.get('market_phase', 'N/A')
        
        logging.error(f"âŒ {ticker} DB ì €ì¥ ì‹¤íŒ¨ ìƒì„¸ ì •ë³´:")
        logging.error(f"   - ì—ëŸ¬: {str(e)}")
        logging.error(f"   - ë°ì´í„°: score={score}, confidence={confidence}")
        logging.error(f"   - ì•¡ì…˜: {action}, ì‹œì¥ë‹¨ê³„: {market_phase}")
        logging.error("   - trend_analysis í…Œì´ë¸” êµ¬ì¡° í™•ì¸ í•„ìš”")
        
        # í…Œì´ë¸” êµ¬ì¡° í™•ì¸ ì¿¼ë¦¬ ì‹¤í–‰
        try:
            schema_result = db_manager.execute_query(
                "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'trend_analysis' ORDER BY ordinal_position;"
            )
            if schema_result:
                logging.error(f"   - í˜„ì¬ í…Œì´ë¸” êµ¬ì¡°: {schema_result}")
            else:
                logging.error("   - í…Œì´ë¸” êµ¬ì¡° ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as schema_e:
            logging.error(f"   - í…Œì´ë¸” êµ¬ì¡° í™•ì¸ ì‹¤íŒ¨: {str(schema_e)}")

def get_enhanced_analysis_for_ticker(ticker: str, db_manager: DBManager):
    """
    íŠ¹ì • í‹°ì»¤ì˜ í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    """
    try:
        result = db_manager.execute_query("""
            SELECT 
                ticker, action, market_phase, confidence,
                vcp_score, stage_analysis, created_at
            FROM trend_analysis 
            WHERE ticker = %s 
            AND type = 'enhanced_analysis'
            ORDER BY created_at DESC 
            LIMIT 1
        """, (ticker,))
        
        if result:
            row = result[0]
            return {
                'ticker': row[0],
                'action': row[1],
                'market_phase': row[2],
                'confidence': float(row[3]),
                'vcp_score': row[4],
                'stage_analysis': json.loads(row[5]) if row[5] else {},
                'created_at': row[6]
            }
        
        return None
        
    except Exception as e:
        logging.error(f"âŒ {ticker} í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# UNUSED: í˜¸ì¶œë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜
# def test_trend_analyzer_improvements():
#     """
#     trend_analyzer.py ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
#     
#     í…ŒìŠ¤íŠ¸ í•­ëª©:
#     1. ë¡œê·¸ ìˆœí™˜ ë° ì••ì¶• ì„¤ì •
#     2. ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
#     3. ë¡œê¹… ë ˆë²¨ ìµœì í™”
#     4. ê°•í™”ëœ ì‘ë‹µ íŒŒì‹±
#     """
#     print("ğŸ§ª trend_analyzer.py ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
#     
#     # 1. ë¡œê·¸ ìˆœí™˜ ì„¤ì • í…ŒìŠ¤íŠ¸
#     print("1ï¸âƒ£ ë¡œê·¸ ìˆœí™˜ ì„¤ì • í…ŒìŠ¤íŠ¸:")
#     try:
#         test_logger = setup_gpt_logging_rotation(
#             log_file_path="log/test_gpt_rotation.log",
#             max_bytes=1024*1024,  # 1MB
#             backup_count=3
#         )
#         test_logger.info("í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë©”ì‹œì§€")
#         print("   âœ… ë¡œê·¸ ìˆœí™˜ ì„¤ì • ì •ìƒ ì‘ë™")
#     except Exception as e:
#         print(f"   âŒ ë¡œê·¸ ìˆœí™˜ ì„¤ì • ì˜¤ë¥˜: {e}")
#     
#     # 2. ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸
#     print("\n2ï¸âƒ£ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸:")
#     try:
#         test_data = {
#             "api_key": "sk-abcd1234567890",
#             "price": "50000 KRW",
#             "email": "test@example.com",
#             "percentage": "12.5%",
#             "normal_text": "ì¼ë°˜ í…ìŠ¤íŠ¸"
#         }
#         
#         masked_low = mask_sensitive_info(test_data, "low")
#         masked_medium = mask_sensitive_info(test_data, "medium")
#         masked_high = mask_sensitive_info(test_data, "high")
#         
#         print(f"   ì›ë³¸: {test_data}")
#         print(f"   Low ë§ˆìŠ¤í‚¹: {masked_low}")
#         print(f"   Medium ë§ˆìŠ¤í‚¹: {masked_medium}")
#         print(f"   High ë§ˆìŠ¤í‚¹: {masked_high}")
#         print("   âœ… ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ ì •ìƒ ì‘ë™")
#     except Exception as e:
#         print(f"   âŒ ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹ ì˜¤ë¥˜: {e}")
#     
#     # 3. ì‘ë‹µ íŒŒì‹± í…ŒìŠ¤íŠ¸
#     print("\n3ï¸âƒ£ ê°•í™”ëœ ì‘ë‹µ íŒŒì‹± í…ŒìŠ¤íŠ¸:")
#     try:
#         test_responses = [
#             '{"score": 85, "action": "BUY", "confidence": 0.8}',  # JSON í˜•ì‹
#             'Score: 75/100\nAction: HOLD\nConfidence: 0.65',      # í…ìŠ¤íŠ¸ í˜•ì‹
#             'ì ìˆ˜: 90ì , ì•¡ì…˜: ë§¤ìˆ˜, ì‹ ë¢°ë„: 80%',                 # í•œêµ­ì–´ í˜•ì‹
#             'This is a positive analysis with bullish sentiment', # í‚¤ì›Œë“œ ê¸°ë°˜
#             '',  # ë¹ˆ ì‘ë‹µ
#         ]
#         
#         for i, response in enumerate(test_responses, 1):
#             result = parse_enhanced_gpt_response(response)
#             print(f"   í…ŒìŠ¤íŠ¸ {i}: {response[:30]}...")
#             print(f"   ê²°ê³¼: ì ìˆ˜={result['score']}, ì•¡ì…˜={result['action']}, ì‹ ë¢°ë„={result['confidence']:.2f}")
#         print("   âœ… ê°•í™”ëœ ì‘ë‹µ íŒŒì‹± ì •ìƒ ì‘ë™")
#     except Exception as e:
#         print(f"   âŒ ì‘ë‹µ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
#     
#     # 4. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
#     print("\n4ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸:")
#     gpt_detailed = os.getenv("GPT_DETAILED_LOGGING", "false")
#     mask_level = os.getenv("GPT_LOG_MASK_LEVEL", "medium")
#     
#     print(f"   GPT_DETAILED_LOGGING: {gpt_detailed}")
#     print(f"   GPT_LOG_MASK_LEVEL: {mask_level}")
#     
#     if gpt_detailed.lower() == "true":
#         print("   ğŸ“‹ ìƒì„¸ ë¡œê¹… í™œì„±í™”ë¨")
#     else:
#         print("   ğŸ“‹ ìƒì„¸ ë¡œê¹… ë¹„í™œì„±í™”ë¨ (ì„±ëŠ¥ ìµœì í™”)")
#     
#     print(f"\nâœ… trend_analyzer.py ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
#     
#     # ì‚¬ìš©ë²• ì•ˆë‚´
#     print("\nğŸ“– ì‚¬ìš©ë²• ì•ˆë‚´:")
#     print("í™˜ê²½ë³€ìˆ˜ ì„¤ì •:")
#     print("  export GPT_DETAILED_LOGGING=true  # ìƒì„¸ ë¡œê¹… í™œì„±í™”")
#     print("  export GPT_LOG_MASK_LEVEL=high    # ë†’ì€ ìˆ˜ì¤€ ë§ˆìŠ¤í‚¹")
#     print("\në¡œê·¸ íŒŒì¼ ìœ„ì¹˜:")
#     print("  - ë©”ì¸ ë¡œê·¸: log/gpt_analysis.log")
#     print("  - ì••ì¶• ë°±ì—…: log/gpt_analysis.log.1.gz, log/gpt_analysis.log.2.gz, ...")

# if __name__ == "__main__":
#     # ê°œì„ ì‚¬í•­ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
#     test_trend_analyzer_improvements()


# === DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë³µêµ¬ ì‹œìŠ¤í…œ ===

def validate_db_schema_consistency(db_manager: DBManager = None, auto_fix: bool = True) -> dict:
    """
    í•„ìˆ˜ í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìë™ ë³µêµ¬
    
    Args:
        db_manager: DB ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
        auto_fix: ìë™ ë³µêµ¬ í™œì„±í™” ì—¬ë¶€
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ ë° ë³µêµ¬ ë‚´ì—­
    """
    
    if db_manager is None:
        try:
            db_manager = DBManager()
        except Exception as e:
            return {
                'status': 'error',
                'message': f'DB ì—°ê²° ì‹¤íŒ¨: {str(e)}',
                'details': {}
            }
    
    # í•„ìˆ˜ í…Œì´ë¸” ë° ì»¬ëŸ¼ ì •ì˜
    required_schemas = {
        'trend_analysis': {
            'columns': [
                'ticker VARCHAR(20) PRIMARY KEY',
                'action VARCHAR(10)',
                'type VARCHAR(50)',
                'reason TEXT',
                'pattern VARCHAR(100)',
                'market_phase VARCHAR(50)',
                'confidence DECIMAL(5,2)',
                'time_window VARCHAR(50)',
                'vcp_score INTEGER',
                'stage_analysis JSONB',
                'created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP',
                'updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP'
            ],
            'indexes': [
                'CREATE INDEX IF NOT EXISTS idx_trend_analysis_created_at ON trend_analysis(created_at)',
                'CREATE INDEX IF NOT EXISTS idx_trend_analysis_action ON trend_analysis(action)',
                'CREATE INDEX IF NOT EXISTS idx_trend_analysis_confidence ON trend_analysis(confidence)'
            ]
        },
        'market_data': {
            'columns': [
                'ticker VARCHAR(20)',
                'date DATE',
                'open_price DECIMAL(15,2)',
                'high_price DECIMAL(15,2)',
                'low_price DECIMAL(15,2)',
                'close_price DECIMAL(15,2)',
                'volume BIGINT',
                'created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP',
                'PRIMARY KEY (ticker, date)'
            ],
            'indexes': [
                'CREATE INDEX IF NOT EXISTS idx_market_data_ticker ON market_data(ticker)',
                'CREATE INDEX IF NOT EXISTS idx_market_data_date ON market_data(date)'
            ]
        },
        'static_indicators': {
            'columns': [
                'ticker VARCHAR(20)',
                'date DATE',
                'rsi DECIMAL(5,2)',
                'macd DECIMAL(10,4)',
                'bb_upper DECIMAL(15,2)',
                'bb_middle DECIMAL(15,2)',
                'bb_lower DECIMAL(15,2)',
                'sma_20 DECIMAL(15,2)',
                'sma_50 DECIMAL(15,2)',
                'sma_200 DECIMAL(15,2)',
                'volume_sma BIGINT',
                'atr DECIMAL(10,4)',
                'obv BIGINT',
                'created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP',
                'PRIMARY KEY (ticker, date)'
            ],
            'indexes': [
                'CREATE INDEX IF NOT EXISTS idx_static_indicators_ticker ON static_indicators(ticker)',
                'CREATE INDEX IF NOT EXISTS idx_static_indicators_date ON static_indicators(date)'
            ]
        },
        'enhanced_analysis': {
            'columns': [
                'id SERIAL PRIMARY KEY',
                'ticker VARCHAR(20)',
                'analysis_date DATE',
                'vcp_analysis JSONB',
                'stage_analysis JSONB',
                'breakout_conditions JSONB',
                'unified_score DECIMAL(5,2)',
                'final_action VARCHAR(10)',
                'confidence INTEGER',
                'risk_score DECIMAL(3,2)',
                'position_size DECIMAL(3,2)',
                'created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP'
            ],
            'indexes': [
                'CREATE INDEX IF NOT EXISTS idx_enhanced_analysis_ticker ON enhanced_analysis(ticker)',
                'CREATE INDEX IF NOT EXISTS idx_enhanced_analysis_date ON enhanced_analysis(analysis_date)',
                'CREATE INDEX IF NOT EXISTS idx_enhanced_analysis_action ON enhanced_analysis(final_action)'
            ]
        }
    }
    
    validation_results = {
        'status': 'success',
        'checked_tables': 0,
        'missing_tables': [],
        'missing_columns': [],
        'created_tables': [],
        'added_columns': [],
        'created_indexes': [],
        'errors': []
    }
    
    try:
        secure_logger.info("ğŸ” DB ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
        
        for table_name, schema_info in required_schemas.items():
            validation_results['checked_tables'] += 1
            secure_logger.info(f"ğŸ“‹ {table_name} í…Œì´ë¸” ê²€ì¦ ì¤‘...")
            
            # 1. í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            table_exists = _check_table_exists(db_manager, table_name)
            
            if not table_exists:
                validation_results['missing_tables'].append(table_name)
                
                if auto_fix:
                    # í…Œì´ë¸” ìƒì„±
                    created = _create_table(db_manager, table_name, schema_info['columns'])
                    if created:
                        validation_results['created_tables'].append(table_name)
                        secure_logger.info(f"âœ… {table_name} í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
                    else:
                        validation_results['errors'].append(f"{table_name} í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨")
                        continue
            else:
                # 2. ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                missing_cols = _check_missing_columns(db_manager, table_name, schema_info['columns'])
                validation_results['missing_columns'].extend(missing_cols)
                
                if auto_fix and missing_cols:
                    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€
                    added_cols = _add_missing_columns(db_manager, table_name, missing_cols)
                    validation_results['added_columns'].extend(added_cols)
            
            # 3. ì¸ë±ìŠ¤ ìƒì„±
            if auto_fix and 'indexes' in schema_info:
                created_indexes = _create_indexes(db_manager, schema_info['indexes'])
                validation_results['created_indexes'].extend(created_indexes)
        
        # ê²°ê³¼ ìš”ì•½
        if validation_results['missing_tables'] or validation_results['missing_columns']:
            if auto_fix:
                secure_logger.info(f"ğŸ”§ ìë™ ë³µêµ¬ ì™„ë£Œ - í…Œì´ë¸”: {len(validation_results['created_tables'])}, ì»¬ëŸ¼: {len(validation_results['added_columns'])}")
            else:
                secure_logger.warning(f"âš ï¸ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ê°ì§€ - ëˆ„ë½ í…Œì´ë¸”: {len(validation_results['missing_tables'])}, ëˆ„ë½ ì»¬ëŸ¼: {len(validation_results['missing_columns'])}")
                validation_results['status'] = 'warning'
        else:
            secure_logger.info("âœ… DB ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ - ëª¨ë“  ìŠ¤í‚¤ë§ˆ ì •ìƒ")
        
        return validation_results
        
    except Exception as e:
        error_msg = f"DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        secure_logger.error(f"âŒ {error_msg}")
        validation_results['status'] = 'error'
        validation_results['errors'].append(error_msg)
        return validation_results


def _check_table_exists(db_manager: DBManager, table_name: str) -> bool:
    """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    try:
        result = db_manager.execute_query("""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = %s
            )
        """, (table_name,))
        return result[0][0] if result else False
    except:
        return False


def _create_table(db_manager: DBManager, table_name: str, columns: list) -> bool:
    """í…Œì´ë¸” ìƒì„±"""
    try:
        columns_sql = ',\n    '.join(columns)
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {columns_sql}
        )
        """
        db_manager.execute_query(create_sql)
        return True
    except Exception as e:
        secure_logger.error(f"âŒ {table_name} í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return False


def _check_missing_columns(db_manager: DBManager, table_name: str, required_columns: list) -> list:
    """ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸"""
    try:
        # í˜„ì¬ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ
        existing_columns = db_manager.execute_query("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = %s
        """, (table_name,))
        
        existing_col_names = {row[0] for row in existing_columns} if existing_columns else set()
        
        missing_columns = []
        for col_def in required_columns:
            if 'PRIMARY KEY' in col_def or 'INDEX' in col_def:
                continue
            
            col_name = col_def.split()[0]
            if col_name not in existing_col_names:
                missing_columns.append(col_def)
        
        return missing_columns
        
    except Exception as e:
        secure_logger.error(f"âŒ {table_name} ì»¬ëŸ¼ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return []


def _add_missing_columns(db_manager: DBManager, table_name: str, missing_columns: list) -> list:
    """ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€"""
    added_columns = []
    
    for col_def in missing_columns:
        try:
            alter_sql = f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS {col_def}"
            db_manager.execute_query(alter_sql)
            added_columns.append(f"{table_name}.{col_def.split()[0]}")
            secure_logger.info(f"âœ… {table_name} í…Œì´ë¸”ì— {col_def.split()[0]} ì»¬ëŸ¼ ì¶”ê°€ë¨")
        except Exception as e:
            secure_logger.error(f"âŒ {table_name}.{col_def.split()[0]} ì»¬ëŸ¼ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
    
    return added_columns


def _create_indexes(db_manager: DBManager, index_sqls: list) -> list:
    """ì¸ë±ìŠ¤ ìƒì„±"""
    created_indexes = []
    
    for index_sql in index_sqls:
        try:
            db_manager.execute_query(index_sql)
            # ì¸ë±ìŠ¤ ì´ë¦„ ì¶”ì¶œ
            index_name = index_sql.split('IF NOT EXISTS')[1].split('ON')[0].strip()
            created_indexes.append(index_name)
        except Exception as e:
            secure_logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    return created_indexes


def run_schema_validation_and_recovery():
    """
    ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë³µêµ¬ ì‹¤í–‰ (ë…ë¦½ ì‹¤í–‰ìš©)
    """
    print("ğŸ” DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë³µêµ¬ ì‹œì‘...\n")
    
    try:
        db_manager = DBManager()
        results = validate_db_schema_consistency(db_manager, auto_fix=True)
        
        print(f"âœ… ê²€ì¦ ìƒíƒœ: {results['status']}")
        print(f"ğŸ“Š ê²€ì‚¬í•œ í…Œì´ë¸”: {results['checked_tables']}ê°œ")
        
        if results['created_tables']:
            print(f"ğŸ†• ìƒì„±ëœ í…Œì´ë¸”: {', '.join(results['created_tables'])}")
        
        if results['added_columns']:
            print(f"â• ì¶”ê°€ëœ ì»¬ëŸ¼: {', '.join(results['added_columns'])}")
        
        if results['created_indexes']:
            print(f"ğŸ”— ìƒì„±ëœ ì¸ë±ìŠ¤: {', '.join(results['created_indexes'])}")
        
        if results['errors']:
            print(f"âŒ ì˜¤ë¥˜: {', '.join(results['errors'])}")
        
        print("\nğŸ‰ DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë³µêµ¬ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ì‹¤í–‰ë¶€ í™•ì¥
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--schema":
        run_schema_validation_and_recovery()
    elif len(sys.argv) > 1 and sys.argv[1] == "--cache-stats":
        # ìºì‹œ í†µê³„ í‘œì‹œ
        optimizer = get_global_optimizer()
        stats = optimizer.cache_manager.get_cache_stats()
        print("ğŸ“Š ìºì‹œ ì‹œìŠ¤í…œ í†µê³„:")
        print(f"  - ì´ ì—”íŠ¸ë¦¬: {stats['total_entries']:,}ê°œ / {stats['entries_limit']:,}ê°œ ({stats['entries_usage_pct']:.1f}%)")
        print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage_mb']:.2f}MB / {stats['memory_limit_mb']:.2f}MB ({stats['memory_usage_pct']:.1f}%)")
    else:
        test_trend_analyzer_improvements()