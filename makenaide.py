import time
import logging
import os
import sys
import pandas as pd
from datetime import datetime, date
from typing import Any, Dict
import builtins  # builtins ëª¨ë“ˆ ì¶”ê°€
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
import psutil  # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
from scanner import update_tickers
from utils import (
    get_db_connection, load_env, setup_logger, get_current_price_safe,
    MIN_KRW_ORDER, MIN_KRW_SELL_ORDER, TAKER_FEE_RATE,
    retry_on_error, handle_api_error, handle_db_error, handle_network_error,
    logger, load_blacklist, safe_strftime, safe_float_convert
)
import psycopg2
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor, as_completed
import pyupbit
import pandas as pd
from data_fetcher import get_ohlcv_d
import json

# âœ… db_manager.pyì˜ í•¨ìˆ˜ ì‚¬ìš©
from db_manager import get_db_connection_context



# ì¤‘ìš” ìƒìˆ˜ ì •ì˜
ONE_HMIL_KRW = 100_000_000  # 1ì–µì› (ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ê¸°ì¤€)

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logger()

# print í•¨ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
_original_print = builtins.print
def print(*args, **kwargs):
    """
    print í•¨ìˆ˜ë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ëª¨ë“  ì¶œë ¥ì„ ë¡œê·¸ íŒŒì¼ì—ë„ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    timestamp = safe_strftime(datetime.now(), '%Y-%m-%d %H:%M:%S')
    file_name = os.path.basename(__file__)
    message = ' '.join(str(a) for a in args)
    
    # ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡
    logger.info(f"[{timestamp}][{file_name}] {message}")
    
    # ì›ë˜ì˜ print í•¨ìˆ˜ í˜¸ì¶œ
    _original_print(f"[{timestamp}][{file_name}] {message}", **kwargs)

# ë¡œê¹… ì‹œì‘ ë©”ì‹œì§€
logger.info("="*50)
logger.info("Makenaide ë´‡ ì‹œì‘")
logger.info("="*50)

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ import ê²½ë¡œ ì„¤ì •
current_file = os.path.abspath(__file__)
current_dir = os.path.dirname(current_file)

# Python ê²½ë¡œ ì„¤ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ë§Œ ì¶”ê°€)
sys.path = [current_dir] + [p for p in sys.path if p != current_dir]

from portfolio_manager import PortfolioManager
from trend_analyzer import analyze_trend_with_gpt, save_trend_analysis_to_db, save_trend_analysis_log, should_reuse_gpt_response, unified_gpt_analysis_engine
from data_fetcher import (
    process_single_ticker as process_ticker_data,
    calculate_technical_indicators,
    generate_chart_image,
    get_ohlcv_d,
    get_ohlcv_4h,
    calculate_technical_indicators_4h,
    save_market_data_4h_to_db,
    enhanced_ohlcv_processor,
    generate_gpt_analysis_json
)
import trade_executor
from trade_executor import TrailingStopManager, check_and_execute_trailing_stop, sell_asset
import pyupbit
import psycopg2
from dotenv import load_dotenv
from auth import generate_jwt_token
import re
import requests  # NEW: for direct REST calls to Upbit

# --- Minimal JWT-based Upbit REST client --- #
class UpbitClient:
    """
    Minimal Upbit REST client that authenticates with a preâ€‘built JWT token.
    Only the methods actually used elsewhere in this file are implemented.
    """
    BASE_URL = "https://api.upbit.com"

    def __init__(self, jwt_token: str):
        self.jwt_token = jwt_token
        self.headers = {"Authorization": f"Bearer {jwt_token}"}
   

from utils import validate_and_correct_phase
from db_manager import DBManager
from config_loader import load_config
# === ìŠ¤ìœ—ìŠ¤íŒŸ/ë°±í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ import ===
from backtester import backtest_combo, SPOT_COMBOS, generate_strategy_report
# from parallel_processor import process_tickers_parallel, process_data_parallel
from filter_tickers import filter_breakout_candidates, filter_by_monthly_data_length, apply_timing_filter_4h

# ë””ë²„ê¹…ì„ ìœ„í•œ import ê²½ë¡œ ì¶œë ¥
logger.info(f"[DEBUG] Current working directory: {os.getcwd()}")
logger.info(f"[DEBUG] Current file path: {current_file}")
logger.info(f"[DEBUG] Current directory: {current_dir}")


class MakenaideBot:
    """
    ë©”ì¸ ìë™ë§¤ë§¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤. ìƒíƒœ(í¬íŠ¸í´ë¦¬ì˜¤, DB, ì„¤ì • ë“±)ì™€ ì£¼ìš” ê¸°ëŠ¥ì„ ë©¤ë²„ë¡œ ê´€ë¦¬í•œë‹¤.
    """
    def __init__(self):
        """
        MakenaideBot ì´ˆê¸°í™”: í™˜ê²½ë³€ìˆ˜, DB ì—°ê²°, API ì—°ê²°, ì„¤ì • ë¡œë“œ
        """
        start_time = time.time()
        logger.info("ğŸ”§ MakenaideBot ì´ˆê¸°í™” ì‹œì‘")
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_env()
        
        # DB ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.db_mgr = DBManager()
        
        # ê¸°ì¡´ ì„¤ì • ë¡œë“œ
        self.config = load_config("config/strategy.yaml")
        
        # ìƒˆë¡œìš´ íŠ¸ë ˆì´ë”© ì„¤ì • ë¡œë“œ
        try:
            from config_loader import get_trading_config  # í†µí•©ëœ ë²„ì „ ì‚¬ìš©
            self.trading_config = get_trading_config()
            logger.info("âœ… í†µí•© íŠ¸ë ˆì´ë”© ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ í†µí•© íŠ¸ë ˆì´ë”© ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.trading_config = None
        
        # === GPT ë¶„ì„ ë°©ì‹ ì„¤ì • ===
        self.use_json_instead_of_chart = True  # True: JSON ë°©ì‹, False: ì°¨íŠ¸ ì´ë¯¸ì§€ ë°©ì‹
        
        # === DB ì €ì¥ ê¸°ëŠ¥ ì„¤ì • ===
        self.save_to_db = True  # True: DBì— ì €ì¥, False: ì €ì¥ ê±´ë„ˆëœ€
        
        # --- Upbit/OpenAI ì¸ì¦ ì²˜ë¦¬ --- #
        self.access_key = os.getenv("UPBIT_ACCESS_KEY")
        self.secret_key = os.getenv("UPBIT_SECRET_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # API í‚¤ê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì˜¤ë¥˜
        if not self.access_key or not self.secret_key:
            logger.error("âŒ Upbit API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (.env í™•ì¸).")
            raise ValueError("Upbit API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

        self.upbit = None

        try:
            import pyupbit
            self.upbit = pyupbit.Upbit(self.access_key, self.secret_key)
            balance = self.upbit.get_balance("KRW")
            logger.info(f"ğŸ’° Upbit(pyupbit) ì¸ì¦ ì„±ê³µ (KRW ì”ì•¡: {balance:,.0f}ì›)") #[TODO]ë‹¤ìŒì—ëŠ” KRW ì”ì•¡ì„ í¬í•¨í•œ í˜„ì¬ í¬íŠ¸í´ì˜¤ë¥¼ print í•˜ë„ë¡
        except Exception as e:
            logger.warning(f"âš ï¸ pyupbit ì¸ì¦ ì‹¤íŒ¨: {e}")
        
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.pm = PortfolioManager(
            self.upbit,
            risk_pct=self.config['risk']['pct'],
            atr_period=self.config['atr']['period'],
            pyramiding_config=self.config['pyramiding']
        )
        
        # íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì„¤ì •
        self.ts_cfg = self.config['trailing_stop']
        self.trailing_manager = TrailingStopManager(atr_multiplier=self.ts_cfg['atr_multiplier'])
        
        # DB ë§¤ë‹ˆì €ì— ë°°ì¹˜ ì—…ë°ì´íŠ¸ ë©”ì„œë“œ ì¶”ê°€ (fallback)
        if not hasattr(self.db_mgr, 'batch_update_trailing_stops'):
            self.db_mgr.batch_update_trailing_stops = self._batch_update_trailing_stops_fallback
        
        # ëª¨ë“ˆ ì†ì„± ì´ˆê¸°í™” (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìœ„í•œ ëª¨ë“ˆ ì°¸ì¡°)
        self._initialize_modules()
        
        # GPT ë¶„ì„ ê²°ê³¼ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì ì´ˆê¸°í™”
        self._initialize_gpt_lifecycle_manager()
        
        # ì´ˆê¸°í™” ì™„ë£Œ ìƒíƒœ
        self.initialized = False
        
        logger.info(f"âœ… MakenaideBot ì´ˆê¸°í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
    
    @contextmanager
    def get_db_connection_safe(self):
        """í‘œì¤€í™”ëœ ì•ˆì „í•œ DB ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
        ëª¨ë“  DB ì‘ì—…ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ í‘œì¤€í™”
        """
        from utils import get_db_connection
        conn = None
        try:
            conn = get_db_connection()
            if conn is None:
                raise ConnectionError("DB ì—°ê²° ì‹¤íŒ¨")
            yield conn
        except Exception as e:
            logger.error(f"âŒ ì•ˆì „í•œ DB ì—°ê²° ì¤‘ ì˜¤ë¥˜: {e}")
            if conn:
                try:
                    conn.rollback()
                except:
                    pass
            raise
        finally:
            if conn:
                try:
                    conn.close()
                    logger.debug("ì•ˆì „í•œ DB ì—°ê²° ì¢…ë£Œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ DB ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _initialize_modules(self):
        """ëª¨ë“ˆ ì´ˆê¸°í™” ë° ì†ì„± í• ë‹¹"""
        try:
            import scanner
            import data_fetcher
            import filter_tickers
            import trend_analyzer
            
            # ëª¨ë“ˆì„ ì†ì„±ìœ¼ë¡œ í• ë‹¹
            self.scanner = scanner
            self.data_fetcher = data_fetcher
            self.filter_tickers = filter_tickers
            self.trend_analyzer = trend_analyzer
            
            logger.info("âœ… ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except ImportError as e:
            logger.error(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_gpt_lifecycle_manager(self):
        """GPT ë¶„ì„ ê²°ê³¼ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        try:
            from trend_analyzer import GPTAnalysisLifecycleManager
            
            # ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì ì´ˆê¸°í™”
            self.gpt_lifecycle_manager = GPTAnalysisLifecycleManager(
                db_manager=self.db_mgr,
                config=None  # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            )
            
            logger.info("âœ… GPT ë¶„ì„ ê²°ê³¼ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ GPT ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.gpt_lifecycle_manager = None
    
    def _check_gpt_analysis_cache(self, ticker: str) -> tuple[bool, dict]:
        """
        GPT ë¶„ì„ ê²°ê³¼ ìºì‹œ í™•ì¸
        
        Args:
            ticker: í™•ì¸í•  í‹°ì»¤
            
        Returns:
            tuple: (ê±´ë„ˆë›¸ì§€ ì—¬ë¶€, ê¸°ì¡´ ë¶„ì„ ë°ì´í„°)
        """
        try:
            from trend_analyzer import should_skip_gpt_analysis
            
            # ìºì‹± ì„¤ì •
            cache_config = {
                'max_age_minutes': 720,  # 12ì‹œê°„
                'enable_caching': True,
                'skip_if_fresh': True
            }
            
            # ìºì‹œ í™•ì¸
            should_skip, existing_analysis = should_skip_gpt_analysis(
                ticker, self.db_mgr, cache_config
            )
            
            return should_skip, existing_analysis
            
        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False, None

    def validate_static_indicators_data(self):
        """static_indicators í…Œì´ë¸”ì˜ ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        try:
            with self.db_mgr.get_connection_context() as conn:
                cursor = conn.cursor()
                
                # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ë¨¼ì € í™•ì¸
                cursor.execute("""
                    SELECT column_name, data_type 
                    FROM information_schema.columns 
                    WHERE table_name = 'static_indicators'
                    ORDER BY ordinal_position
                """)
                
                existing_columns = {row[0]: row[1] for row in cursor.fetchall()}
                
                if not existing_columns:
                    logger.error("âŒ static_indicators í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                logger.info(f"ğŸ“Š static_indicators í…Œì´ë¸” ì»¬ëŸ¼ {len(existing_columns)}ê°œ í™•ì¸ë¨")
                
                # ë¬¸ì œ ì»¬ëŸ¼ë“¤ì„ ë°ì´í„° íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
                problem_columns = ['nvt_relative', 'volume_change_7_30', 'adx', 'supertrend_signal']
                
                # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
                existing_problem_columns = [col for col in problem_columns if col in existing_columns]
                missing_columns = [col for col in problem_columns if col not in existing_columns]
                
                if missing_columns:
                    logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼ë“¤: {missing_columns}")
                
                # ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜
                numeric_columns = []
                text_columns = []
                
                for col in existing_problem_columns:
                    data_type = existing_columns[col]
                    if 'text' in data_type.lower() or 'varchar' in data_type.lower() or 'char' in data_type.lower():
                        text_columns.append(col)
                    else:
                        numeric_columns.append(col)
                
                logger.info(f"ğŸ“Š ê²€ì¦ ëŒ€ìƒ - ìˆ«ì ì»¬ëŸ¼: {numeric_columns}, í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {text_columns}")
                
                validation_results = {'success': 0, 'failed': 0, 'warnings': 0}
                
                # ìˆ«ì ì»¬ëŸ¼ ê²€ì¦
                for column in numeric_columns:
                    try:
                        cursor.execute(f"""
                            SELECT COUNT(DISTINCT {column}) as unique_count,
                                   COUNT(*) as total_count,
                                   AVG({column}) as avg_value,
                                   MIN({column}) as min_value,
                                   MAX({column}) as max_value
                            FROM static_indicators 
                            WHERE {column} IS NOT NULL
                        """)
                        
                        result = cursor.fetchone()
                        if result:
                            unique_count, total_count, avg_val, min_val, max_val = result
                            
                            if unique_count <= 1 and total_count > 10:
                                logger.warning(f"âš ï¸ {column} ì»¬ëŸ¼ ë°ì´í„° ì´ìƒ: ëª¨ë“  ê°’ì´ ë™ì¼í•¨ (ê°’: {avg_val})")
                                validation_results['warnings'] += 1
                                
                                # ë°ì´í„° ì¬ê³„ì‚° ì‹œë„
                                self._attempt_column_recalculation(column)
                            else:
                                logger.info(f"âœ… {column} ì»¬ëŸ¼ ì •ìƒ: ê³ ìœ ê°’ {unique_count}ê°œ, ë²”ìœ„ {min_val}~{max_val}")
                                validation_results['success'] += 1
                                
                    except Exception as e:
                        logger.error(f"âŒ {column} ì»¬ëŸ¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
                        validation_results['failed'] += 1
                        
                        # ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… í™•ì¸ ë° ìˆ˜ì • ì‹œë„
                        self._attempt_column_type_fix(column, e)
                
                # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ê²€ì¦ (supertrend_signal)
                for column in text_columns:
                    try:
                        cursor.execute(f"""
                            SELECT COUNT(DISTINCT {column}) as unique_count,
                                   COUNT(*) as total_count,
                                   MIN({column}) as min_value,
                                   MAX({column}) as max_value
                            FROM static_indicators 
                            WHERE {column} IS NOT NULL
                        """)
                        
                        result = cursor.fetchone()
                        if result:
                            unique_count, total_count, min_val, max_val = result
                            
                            if unique_count <= 1 and total_count > 10:
                                logger.warning(f"âš ï¸ {column} ì»¬ëŸ¼ ë°ì´í„° ì´ìƒ: ëª¨ë“  ê°’ì´ ë™ì¼í•¨ (ê°’: {min_val})")
                                validation_results['warnings'] += 1
                                
                                # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¬ê³„ì‚° ì‹œë„
                                self._attempt_text_column_recalculation(column)
                            else:
                                logger.info(f"âœ… {column} ì»¬ëŸ¼ ì •ìƒ: ê³ ìœ ê°’ {unique_count}ê°œ, ë²”ìœ„ {min_val}~{max_val}")
                                validation_results['success'] += 1
                                
                    except Exception as e:
                        logger.error(f"âŒ {column} ì»¬ëŸ¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
                        validation_results['failed'] += 1
                        
                        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ íƒ€ì… í™•ì¸
                        self._attempt_text_column_fix(column, e)
                
                # ê²€ì¦ ê²°ê³¼ ìš”ì•½
                total_checked = validation_results['success'] + validation_results['failed'] + validation_results['warnings']
                logger.info(f"ğŸ“Š static_indicators ê²€ì¦ ì™„ë£Œ: ì„±ê³µ {validation_results['success']}/{total_checked}, "
                          f"ê²½ê³  {validation_results['warnings']}, ì‹¤íŒ¨ {validation_results['failed']}")
                
                return validation_results['failed'] == 0
                        
        except Exception as e:
            logger.error(f"âŒ static_indicators ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _attempt_column_recalculation(self, column):
        """ì»¬ëŸ¼ ë°ì´í„° ì¬ê³„ì‚° ì‹œë„"""
        try:
            logger.info(f"ğŸ”§ {column} ì»¬ëŸ¼ ë°ì´í„° ì¬ê³„ì‚° ì‹œë„ ì¤‘...")
            
            # ì¬ê³„ì‚° ë¡œì§ì€ data_fetcherì˜ static_indicators ê³„ì‚° í•¨ìˆ˜ í™œìš©
            # ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ì‹¤ì œ ì¬ê³„ì‚°ì€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆ˜í–‰
            logger.warning(f"âš ï¸ {column} ì»¬ëŸ¼ ì¬ê³„ì‚° í•„ìš” - data_fetcher.process_all_static_indicators() ì‹¤í–‰ ê¶Œì¥")
            
        except Exception as e:
            logger.error(f"âŒ {column} ì»¬ëŸ¼ ì¬ê³„ì‚° ì‹¤íŒ¨: {e}")

    def _attempt_column_type_fix(self, column, error):
        """ì»¬ëŸ¼ ë°ì´í„° íƒ€ì… ë¬¸ì œ í•´ê²° ì‹œë„"""
        try:
            error_str = str(error).lower()
            
            if 'function avg(text)' in error_str:
                logger.warning(f"ğŸ”§ {column} ì»¬ëŸ¼ì´ TEXT íƒ€ì…ìœ¼ë¡œ ì €ì¥ë¨ - ìˆ«ì ë³€í™˜ í•„ìš”")
                # ì‹¤ì œ íƒ€ì… ë³€í™˜ì€ ë³„ë„ ë§ˆì´ê·¸ë ˆì´ì…˜ì—ì„œ ìˆ˜í–‰
                
            elif 'does not exist' in error_str:
                logger.warning(f"ğŸ”§ {column} ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ í•„ìš”")
                
        except Exception as e:
            logger.error(f"âŒ {column} ì»¬ëŸ¼ íƒ€ì… ìˆ˜ì • ì‹¤íŒ¨: {e}")

    def _attempt_text_column_recalculation(self, column):
        """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¬ê³„ì‚° ì‹œë„"""
        try:
            logger.info(f"ğŸ”§ {column} í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¬ê³„ì‚° ì‹œë„ ì¤‘...")
            
            if column == 'supertrend_signal':
                logger.warning(f"âš ï¸ {column} ì»¬ëŸ¼ ì¬ê³„ì‚° í•„ìš” - Supertrend ì§€í‘œ ì¬ê³„ì‚° ê¶Œì¥")
                
        except Exception as e:
            logger.error(f"âŒ {column} í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¬ê³„ì‚° ì‹¤íŒ¨: {e}")

    def _attempt_text_column_fix(self, column, error):
        """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë¬¸ì œ í•´ê²° ì‹œë„"""
        try:
            error_str = str(error).lower()
            
            if 'does not exist' in error_str:
                logger.warning(f"ğŸ”§ {column} í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ - ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ í•„ìš”")
                
        except Exception as e:
            logger.error(f"âŒ {column} í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìˆ˜ì • ì‹¤íŒ¨: {e}")

    def validate_ohlcv_precision(self, sample_size=10):
        """OHLCV ë°ì´í„°ì˜ ì†Œìˆ˜ì  ì •ë°€ë„ ê²€ì¦ (ìŠ¤ëª°ìº¡ ì½”ì¸ ì§€ì›)"""
        try:
            with self.db_mgr.get_connection_context() as conn:
                cursor = conn.cursor()
                
                # ê°€ê²©ì´ 0ì¸ ë ˆì½”ë“œ í™•ì¸
                cursor.execute("""
                    SELECT ticker, COUNT(*) as zero_count
                    FROM ohlcv 
                    WHERE (open = 0 OR high = 0 OR low = 0 OR close = 0)
                    GROUP BY ticker
                    ORDER BY zero_count DESC
                    LIMIT %s
                """, (sample_size,))
                
                zero_records = cursor.fetchall()
                
                if zero_records:
                    logger.warning(f"âš ï¸ ê°€ê²©ì´ 0ì¸ ë ˆì½”ë“œ ë°œê²¬: {len(zero_records)}ê°œ í‹°ì»¤")
                    for ticker, count in zero_records:
                        logger.warning(f"   - {ticker}: {count}ê°œ ë ˆì½”ë“œ")
                else:
                    logger.info("âœ… ê°€ê²©ì´ 0ì¸ ë ˆì½”ë“œ ì—†ìŒ")
                
                # ê·¹ì†Œê°’ ê°€ê²© í™•ì¸ (ìŠ¤ëª°ìº¡ ì½”ì¸)
                cursor.execute("""
                    SELECT ticker, close, volume
                    FROM ohlcv 
                    WHERE close < 0.01 AND close > 0
                    AND date >= CURRENT_DATE - INTERVAL '7 days'
                    ORDER BY close ASC
                    LIMIT %s
                """, (sample_size,))
                
                small_cap_records = cursor.fetchall()
                
                if small_cap_records:
                    logger.info(f"ğŸ“Š ìŠ¤ëª°ìº¡ ì½”ì¸ ë°œê²¬: {len(small_cap_records)}ê°œ")
                    for ticker, price, volume in small_cap_records:
                        logger.info(f"   - {ticker}: ê°€ê²© {price:.8f}, ê±°ë˜ëŸ‰ {volume}")
                else:
                    logger.info("ğŸ“Š ìŠ¤ëª°ìº¡ ì½”ì¸ ì—†ìŒ")
                    
        except Exception as e:
            logger.error(f"âŒ OHLCV ì •ë°€ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")

    @retry_on_error(max_retries=3, delay=5)
    def init_db(self):
        from init_db_pg import create_tables
        logger.info("ğŸ”§ DB í…Œì´ë¸” ìƒì„± í™•ì¸ ë° ì´ˆê¸°í™”")
        try:
            create_tables()
        except Exception as e:
            handle_db_error(e, "DB í…Œì´ë¸” ìƒì„±")
            raise

    @retry_on_error(max_retries=3, delay=5)
    def update_tickers(self):
        try:
            logger.info("ğŸ”„ í‹°ì»¤ ëª©ë¡ ì—…ë°ì´íŠ¸ ì¤‘")
            update_tickers()
            logger.info("âœ… í‹°ì»¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            handle_network_error(e, "í‹°ì»¤ ì—…ë°ì´íŠ¸")
            raise

    def update_trailing_stops_batch(self, assets_data):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì—…ë°ì´íŠ¸ ìµœì í™”"""
        if not assets_data:
            logger.info("ğŸ’¼ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì—…ë°ì´íŠ¸í•  ìì‚°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ”„ {len(assets_data)}ê°œ ìì‚° íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # ëª¨ë“  í‹°ì»¤ì— ëŒ€í•œ í˜„ì¬ê°€ ë° ATR ë°ì´í„° ì¼ê´„ ì¡°íšŒ
            all_tickers = [asset['ticker'] for asset in assets_data]
            current_prices = {}
            atr_values = {}
            
            # í˜„ì¬ê°€ ì¼ê´„ ì¡°íšŒ
            try:
                import pyupbit
                ticker_chunks = [all_tickers[i:i+20] for i in range(0, len(all_tickers), 20)]
                
                for chunk in ticker_chunks:
                    chunk_prices = pyupbit.get_current_price(chunk)
                    if isinstance(chunk_prices, dict):
                        current_prices.update(chunk_prices)
                    elif len(chunk) == 1 and chunk_prices is not None:
                        current_prices[chunk[0]] = chunk_prices
            except Exception as e:
                logger.error(f"âŒ í˜„ì¬ê°€ ì¼ê´„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                return
            
            # ATR ê°’ ì¼ê´„ ì¡°íšŒ (ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¡°íšŒ)
            try:
                with self.db_mgr.get_connection_context() as conn:
                    cursor = conn.cursor()
                    placeholders = ','.join(['%s'] * len(all_tickers))
                    
                    # static_indicators í…Œì´ë¸”ì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¡°íšŒ
                    atr_query = f"""
                        SELECT ticker, atr, volume_change_7_30
                        FROM static_indicators 
                        WHERE ticker IN ({placeholders})
                    """
                    cursor.execute(atr_query, all_tickers)
                    
                    for row in cursor.fetchall():
                        ticker, atr, volume_change = row
                        if atr is not None:
                            atr_values[ticker] = {
                                'atr': safe_float_convert(atr, context=f"{ticker} ATR"),
                                'volume_ratio': safe_float_convert(volume_change, context=f"{ticker} Volume Change")
                            }
                    
                    # RSIëŠ” ohlcv í…Œì´ë¸”ì—ì„œ ë³„ë„ ì¡°íšŒ (ìµœì‹  ë°ì´í„° ê¸°ì¤€)
                    rsi_query = f"""
                        SELECT ticker, rsi_14
                        FROM ohlcv 
                        WHERE ticker IN ({placeholders})
                        AND date >= CURRENT_DATE - INTERVAL '7 days'
                        ORDER BY ticker, date DESC
                    """
                    cursor.execute(rsi_query, all_tickers)
                    
                    # í‹°ì»¤ë³„ ìµœì‹  RSI ê°’ ìˆ˜ì§‘
                    rsi_data = {}
                    for row in cursor.fetchall():
                        ticker, rsi_14 = row
                        if ticker not in rsi_data and rsi_14 is not None:
                            rsi_data[ticker] = safe_float_convert(rsi_14, context=f"{ticker} RSI")
                    
                    # ATR ë°ì´í„°ì— RSI ì¶”ê°€
                    for ticker in atr_values:
                        atr_values[ticker]['rsi'] = rsi_data.get(ticker, 50)  # ê¸°ë³¸ê°’ 50
                        
            except Exception as e:
                logger.error(f"âŒ ATR ë°ì´í„° ì¼ê´„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                return
            
            # ë™ì  ìŠ¤íƒ‘ ë¡œìŠ¤ ê³„ì‚° ë° ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤€ë¹„
            batch_updates = []
            
            for asset in assets_data:
                ticker = asset['ticker']
                
                if ticker not in current_prices or ticker not in atr_values:
                    continue
                
                current_price = current_prices[ticker]
                atr_data = atr_values[ticker]
                
                # ATR ê¸°ë°˜ ë™ì  ìŠ¤íƒ‘ ë¡œìŠ¤ ê³„ì‚°
                dynamic_stop = self.calculate_dynamic_stop(ticker, current_price, atr_data)
                
                if dynamic_stop:
                    batch_updates.append({
                        'ticker': ticker,
                        'stop_price': dynamic_stop['stop_price'],
                        'activation_price': dynamic_stop['activation_price'],
                        'atr_value': dynamic_stop['atr_value'],
                        'updated_at': datetime.now()
                    })
            
            # ë°°ì¹˜ DB ì—…ë°ì´íŠ¸ ì‹¤í–‰
            if batch_updates:
                self.db_mgr.batch_update_trailing_stops(batch_updates)
                logger.info(f"âœ… {len(batch_updates)}ê°œ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def calculate_dynamic_stop(self, ticker, current_price, atr_data):
        """ATR ê¸°ë°˜ ë™ì  ìŠ¤íƒ‘ ë¡œìŠ¤ ê³„ì‚°"""
        try:
            atr_value = atr_data['atr']
            rsi = atr_data.get('rsi', 50)
            volume_ratio = atr_data.get('volume_ratio', 1.0)
            
            # ATR ê¸°ë°˜ ë³€ë™ì„± ê³„ì‚° (ê°€ê²© ëŒ€ë¹„ í¼ì„¼íŠ¸)
            atr_pct = (atr_value / current_price) * 100
            
            # ì‹œì¥ ìƒí™©ì„ ê³ ë ¤í•œ ë™ì  ë°°ìˆ˜ ê³„ì‚°
            # RSIê°€ ë†’ì„ìˆ˜ë¡ (ê³¼ë§¤ìˆ˜) ë” ë³´ìˆ˜ì  ìŠ¤íƒ‘
            # ê±°ë˜ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ë” ë³´ìˆ˜ì  ìŠ¤íƒ‘
            rsi_multiplier = 1.0 + (rsi - 50) * 0.01  # RSI 50 ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
            volume_multiplier = min(1.0 + (volume_ratio - 1.0) * 0.2, 1.5)  # ìµœëŒ€ 1.5ë°°
            
            # ê¸°ë³¸ ATR ë°°ìˆ˜ì— ë™ì  ìš”ì†Œ ì ìš©
            base_multiplier = self.ts_cfg.get('atr_multiplier', 2.0)
            dynamic_multiplier = base_multiplier * rsi_multiplier * volume_multiplier
            
            # ìµœì¢… ìŠ¤íƒ‘ ê°€ê²© ê³„ì‚°
            stop_distance = atr_value * dynamic_multiplier
            stop_price = current_price - stop_distance
            
            # í™œì„±í™” ê°€ê²© (í˜„ì¬ê°€ ê¸°ì¤€ 5% ìƒìŠ¹)
            activation_price = current_price * 1.05
            
            logger.debug(f"ğŸ“Š {ticker} ë™ì  ìŠ¤íƒ‘ ê³„ì‚°: "
                        f"ATR={atr_value:.2f}({atr_pct:.2f}%), "
                        f"ë°°ìˆ˜={dynamic_multiplier:.2f}, "
                        f"ìŠ¤íƒ‘ê°€={stop_price:.2f}")
            
            return {
                'stop_price': stop_price,
                'activation_price': activation_price,
                'atr_value': atr_value,
                'dynamic_multiplier': dynamic_multiplier
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ë™ì  ìŠ¤íƒ‘ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def update_portfolio(self):
        """
        1. ê³„ì • ì •ë³´ ì—…ë°ì´íŠ¸
        2. í¬íŠ¸í´ë¦¬ì˜¤ ìì‚° ë³€ë™ ê¸°ë¡
        3. ë³´ìœ ì¢…ëª© TrailingStop ì„¤ì •/ê´€ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)
        """
        try:
            logger.info("ğŸ”„ í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ ì—…ë°ì´íŠ¸ ì¤‘")
            balances = self.upbit.get_balances()

            # ğŸ”§ [ìˆ˜ì •] balances ì‘ë‹µ í˜•ì‹ ê²€ì¦ ë° ë³€í™˜ (portfolio_manager.pyì™€ ë™ì¼í•œ ë¡œì§)
            logger.debug(f"ğŸ” update_portfolio balances ì‘ë‹µ íƒ€ì…: {type(balances)}")
            
            # Noneì¸ ê²½ìš° ì²˜ë¦¬
            if balances is None:
                logger.warning("âš ï¸ get_balancesê°€ Noneì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # ë¬¸ìì—´ë¡œ ë°˜í™˜ëœ ê²½ìš° JSON íŒŒì‹± ì‹œë„
            if isinstance(balances, str):
                try:
                    import json
                    balances = json.loads(balances)
                    logger.info("âœ… ë¬¸ìì—´ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹± ì™„ë£Œ")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    return []
            
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(balances, list):
                logger.info(f"ğŸ“Š update_portfolio: balances ë°˜í™˜ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ (íƒ€ì…: {type(balances)}) - ë³€í™˜ ì‹œë„")
                if isinstance(balances, dict):
                    if 'data' in balances:
                        balances = balances['data']
                        logger.info("âœ… 'data' í‚¤ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                    elif 'result' in balances:
                        balances = balances['result']
                        logger.info("âœ… 'result' í‚¤ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
                    else:
                        # ë‹¨ì¼ ì”ê³  ì •ë³´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        balances = [balances]
                        logger.info("âœ… ë‹¨ì¼ ì”ê³  ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ balances í˜•ì‹: {type(balances)}")
                    return []

            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
            blacklist = load_blacklist()

            # ğŸ”§ [ìˆ˜ì •] ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ì¢…ëª© í•„í„°ë§ (ì•ˆì „í•œ ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼)
            filtered_balances = []
            for balance in balances:
                try:
                    if isinstance(balance, dict) and balance.get('currency'):
                        currency = balance.get('currency')
                        if f"KRW-{currency}" not in blacklist:
                            filtered_balances.append(balance)
                        else:
                            logger.debug(f"â­ï¸ {currency}ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ì œì™¸ë©ë‹ˆë‹¤.")
                except Exception as e:
                    logger.warning(f"âš ï¸ balance í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e} - {balance}")
                    continue
            
            balances = filtered_balances

            # DBì— í¬íŠ¸í´ë¦¬ì˜¤ ì •ë³´ ì €ì¥
            self.db_mgr.save_portfolio_history(balances)
            
            # í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ ì •ë³´ ì¶œë ¥ (simple_portfolio_summary ì‚¬ìš©)
            logger.info("ğŸ“Š í¬íŠ¸í´ë¦¬ì˜¤ í˜„í™© ì¶œë ¥ ì‹œì‘")
            self.pm.simple_portfolio_summary()
            logger.info("ğŸ“Š í¬íŠ¸í´ë¦¬ì˜¤ í˜„í™© ì¶œë ¥ ì™„ë£Œ")
            
            # í¬íŠ¸í´ë¦¬ì˜¤ ìì‚° ì¶”ì  ë° ê¸°ë¡
            total = self.pm.get_total_balance()
            logger.info(f"ğŸ’° ì´ ìì‚°: {total:,.0f} KRW")

            # íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì„¤ì • ë° ê´€ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)
            if not balances or len(balances) <= 1:  # KRWë§Œ ìˆëŠ” ê²½ìš°
                logger.info("ğŸ’¼ ë³´ìœ  ìì‚°ì´ ì—†ì–´ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì„¤ì • ê±´ë„ˆëœ€")
                return balances
                
            # KRWë¥¼ ì œì™¸í•œ ì‹¤ì œ ë³´ìœ  ìì‚° í•„í„°ë§
            assets = [balance for balance in balances if balance.get('currency') != 'KRW']
            if not assets:
                logger.info("ğŸ’¼ KRW ì™¸ ë³´ìœ  ìì‚°ì´ ì—†ì–´ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì„¤ì • ê±´ë„ˆëœ€")
                return balances
            
            # ìì‚° ë°ì´í„° êµ¬ì¡°í™”
            assets_data = []
            for asset in assets:
                currency = asset.get('currency')
                ticker = f"KRW-{currency}"
                assets_data.append({
                    'ticker': ticker,
                    'currency': currency,
                    'balance': safe_float_convert(asset.get('balance', 0), context=f"{ticker} balance"),
                    'avg_buy_price': safe_float_convert(asset.get('avg_buy_price', 0), context=f"{ticker} avg_buy_price")
                })
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì—…ë°ì´íŠ¸
            self.update_trailing_stops_batch(assets_data)
                
            logger.info("âœ… í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return balances
            
        except Exception as e:
            logger.error(f"âŒ í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def save_gpt_analysis_to_db(self, gpt_results: list):
        """
        GPT ë¶„ì„ ê²°ê³¼ë¥¼ PostgreSQLì˜ trend_analysis í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            gpt_results (list): GPT ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.save_to_db:
            logger.info("ğŸ’¾ DB ì €ì¥ ì„¤ì •ì´ ë¹„í™œì„±í™”ë˜ì–´ GPT ë¶„ì„ ê²°ê³¼ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        if not gpt_results:
            logger.warning("âš ï¸ ì €ì¥í•  GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        logger.info(f"ğŸ”„ GPT ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì‹œì‘: {len(gpt_results)}ê°œ ì¢…ëª©")
        
        # ë””ë²„ê¹…: ë°›ì€ ë°ì´í„° ë¡œê¹…
        for i, result in enumerate(gpt_results):
            logger.info(f"ğŸ“ ì €ì¥í•  ë°ì´í„° {i+1}: {result}")
            
        try:
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'trend_analysis'
                    );
                """)
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    logger.error("âŒ trend_analysis í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                    return
                
                logger.info("âœ… trend_analysis í…Œì´ë¸” ì¡´ì¬ í™•ì¸ë¨")
                
                # âœ… trend_analysis í…Œì´ë¸”ì˜ ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìˆ˜ì •
                # ì‹¤ì œ ì»¬ëŸ¼: ticker, score, confidence, action, market_phase, pattern, reason
                insert_query = """
                INSERT INTO trend_analysis (ticker, score, confidence, action, market_phase, pattern, reason)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (ticker) DO UPDATE SET
                score = EXCLUDED.score,
                confidence = EXCLUDED.confidence,
                action = EXCLUDED.action,
                market_phase = EXCLUDED.market_phase,
                pattern = EXCLUDED.pattern,
                reason = EXCLUDED.reason,
                created_at = CURRENT_TIMESTAMP
                """
                
                data_to_insert = []
                for result in gpt_results:
                    try:
                        ticker = result.get("ticker")
                        if not ticker:
                            logger.warning(f"âš ï¸ í‹°ì»¤ê°€ ì—†ëŠ” ê²°ê³¼ ê±´ë„ˆëœ€: {result}")
                            continue
                        
                        # ğŸ”§ ì•ˆì „í•œ íƒ€ì… ë³€í™˜ ì ìš©
                        score = safe_float_convert(result.get("score", 0), context=f"GPTë¶„ì„ {ticker} score")
                        confidence = safe_float_convert(result.get("confidence", 0), context=f"GPTë¶„ì„ {ticker} confidence")
                        action = result.get("action", "HOLD")
                        market_phase = result.get("market_phase", "Unknown")
                        pattern = result.get("pattern", "")
                        reason = result.get("reason", "")
                        
                        # âœ… í•„ë“œ ëˆ„ë½ ê²€ì¦ ë° ê²½ê³  (ê°œì„ ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦)
                        if not action or action == "HOLD":
                            logger.warning(f"âš ï¸ {ticker} action í•„ë“œ ëˆ„ë½ ë˜ëŠ” ê¸°ë³¸ê°’: {action}")
                        if not market_phase or market_phase == "Unknown":
                            logger.warning(f"âš ï¸ {ticker} market_phase í•„ë“œ ëˆ„ë½ ë˜ëŠ” ê¸°ë³¸ê°’: {market_phase}")
                        if not pattern:
                            logger.warning(f"âš ï¸ {ticker} pattern í•„ë“œ ëˆ„ë½")
                        if not reason:
                            logger.warning(f"âš ï¸ {ticker} reason í•„ë“œ ëˆ„ë½")
                        
                        # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                        if not isinstance(action, str) or action not in ["BUY", "HOLD", "AVOID", "SELL"]:
                            logger.warning(f"âš ï¸ {ticker} ì˜ëª»ëœ action ê°’: {action}, HOLDë¡œ ë³€ê²½")
                            action = "HOLD"
                        
                        insert_data = (ticker, score, confidence, action, market_phase, pattern, reason)
                        data_to_insert.append(insert_data)
                        logger.info(f"âœ… {ticker} ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: score={score}, action={action}, phase={market_phase}")
                        
                    except Exception as e:
                        logger.error(f"âŒ GPT ë¶„ì„ ê²°ê³¼ ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {result.get('ticker', 'Unknown')} | {str(e)}")
                        import traceback
                        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        continue
                
                if data_to_insert:
                    logger.info(f"ğŸ”„ {len(data_to_insert)}ê°œ ë°ì´í„° DB ì‚½ì… ì‹œì‘")
                    
                    # ê°œë³„ ì‚½ì…ìœ¼ë¡œ ë” ìƒì„¸í•œ ë””ë²„ê¹…
                    success_count = 0
                    for i, data in enumerate(data_to_insert):
                        try:
                            cursor.execute(insert_query, data)
                            success_count += 1
                            logger.info(f"âœ… {data[0]} ì‚½ì… ì„±ê³µ ({i+1}/{len(data_to_insert)})")
                        except Exception as insert_error:
                            logger.error(f"âŒ {data[0]} ì‚½ì… ì‹¤íŒ¨: {str(insert_error)}")
                            logger.error(f"ì‚½ì… ë°ì´í„°: {data}")
                    
                    # ì»¤ë°‹
                    conn.commit()
                    logger.info(f"âœ… íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì™„ë£Œ: {success_count}/{len(data_to_insert)}ê°œ ì„±ê³µ")
                    
                    # ì‹¤ì œ ì €ì¥ í™•ì¸
                    cursor.execute("SELECT COUNT(*) FROM trend_analysis WHERE ticker = ANY(%s)", 
                                 ([row[0] for row in data_to_insert],))
                    saved_count = cursor.fetchone()[0]
                    logger.info(f"ğŸ” ì €ì¥ ê²€ì¦: DBì—ì„œ {saved_count}ê°œ í™•ì¸ë¨")
                    
                    if saved_count == len(data_to_insert):
                        logger.info(f"âœ… GPT ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ: {len(data_to_insert)}ê°œ ì¢…ëª©")
                    else:
                        logger.warning(f"âš ï¸ ì €ì¥ ë¶ˆì¼ì¹˜: ì‹œë„={len(data_to_insert)}, ì‹¤ì œì €ì¥={saved_count}")
                else:
                    logger.warning("âš ï¸ ì €ì¥ ê°€ëŠ¥í•œ GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
        except Exception as db_error:
            # psycopg2.ErrorëŠ” psycopg2ê°€ importë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë°˜ Exceptionìœ¼ë¡œ ì²˜ë¦¬
            logger.error(f"âŒ PostgreSQL DB ì˜¤ë¥˜: {str(db_error)}")
            if hasattr(db_error, 'pgcode'):
                logger.error(f"ì˜¤ë¥˜ ì½”ë“œ: {db_error.pgcode}")
            import traceback
            logger.error(f"ìƒì„¸ ìŠ¤íƒ: {traceback.format_exc()}")
        except Exception as e:
            logger.error(f"âŒ GPT ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            logger.error(f"ìƒì„¸ ìŠ¤íƒ: {traceback.format_exc()}")

    def save_trade_log_to_db(self, trade_logs: list):
        """
        ë§¤ìˆ˜ ì´ë ¥ì„ PostgreSQLì˜ trade_log í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            trade_logs (list): ë§¤ìˆ˜ ì´ë ¥ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.save_to_db:
            logger.info("ğŸ’¾ DB ì €ì¥ ì„¤ì •ì´ ë¹„í™œì„±í™”ë˜ì–´ ë§¤ìˆ˜ ì´ë ¥ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
            
        if not trade_logs:
            logger.info("ğŸ’¾ ì €ì¥í•  ë§¤ìˆ˜ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        try:
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                # trade_log í…Œì´ë¸”ì— ë°ì´í„° ì‚½ì… (qty ì»¬ëŸ¼ ì¶”ê°€)
                insert_query = """
                INSERT INTO trade_log (ticker, action, buy_price, qty, score, confidence, trade_amount_krw, bought_at, status, error_msg)
                VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), %s, %s)
                """
                
                data_to_insert = []
                for log in trade_logs:
                    try:
                        ticker = log.get("ticker")
                        
                        # ğŸ”§ ì•ˆì „í•œ íƒ€ì… ë³€í™˜ ì ìš©
                        buy_price = safe_float_convert(log.get("buy_price", 0), context=f"ë§¤ìˆ˜ì´ë ¥ {ticker} buy_price")
                        score = safe_float_convert(log.get("score", 0), context=f"ë§¤ìˆ˜ì´ë ¥ {ticker} score")
                        confidence = safe_float_convert(log.get("confidence", 0), context=f"ë§¤ìˆ˜ì´ë ¥ {ticker} confidence")
                        trade_amount_krw = safe_float_convert(log.get("trade_amount_krw", 0), context=f"ë§¤ìˆ˜ì´ë ¥ {ticker} trade_amount_krw")
                        
                        status = log.get("status", "UNKNOWN")
                        error_msg = log.get("error_msg", None)
                        
                        # action ì»¬ëŸ¼ ì„¤ì • (ë§¤ìˆ˜ ì‹œë„ì´ë¯€ë¡œ 'BUY'ë¡œ ì„¤ì •)
                        action = "BUY"
                        
                        # qty ê³„ì‚° (trade_amount_krw / buy_price)
                        qty = 0.0
                        if buy_price > 0:
                            qty = trade_amount_krw / buy_price
                        
                        data_to_insert.append((ticker, action, buy_price, qty, score, confidence, trade_amount_krw, status, error_msg))
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë§¤ìˆ˜ ì´ë ¥ ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {log.get('ticker', 'Unknown')} | {str(e)}")
                        continue
                
                if data_to_insert:
                    cursor.executemany(insert_query, data_to_insert)
                    conn.commit()
                    logger.info(f"âœ… ë§¤ìˆ˜ ì´ë ¥ DB ì €ì¥ ì™„ë£Œ: {len(data_to_insert)}ê°œ ì´ë ¥")
                else:
                    logger.warning("âš ï¸ ì €ì¥ ê°€ëŠ¥í•œ ë§¤ìˆ˜ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ë§¤ìˆ˜ ì´ë ¥ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    @retry_on_error(max_retries=3, delay=5)
    def update_all_tickers_ohlcv(self, ticker):
        """
        Fetch incremental 1d OHLCV data for `ticker`, save to DB, and delete data older than 251 days.
        """
        from datetime import date
        from data_fetcher import get_ohlcv_d, delete_old_ohlcv

        # 1) Determine the last stored date
        try:
            row = self.db_mgr.execute_query(
                "SELECT MAX(date) FROM ohlcv WHERE ticker = %s", (ticker,), fetchone=True
            )
            last_date = row[0] if row and row[0] else None
        except Exception as e:
            logger.error(f"âŒ {ticker} ê°€ì¥ ìµœì‹  OHLCV ë‚ ì§œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            last_date = None

        today = date.today()
        # 2) Fetch data
        if last_date is None:
            # No data in DB, fetch full history
            df = get_ohlcv_d(ticker)
        else:
            days_diff = (today - last_date).days
            if days_diff <= 0:
                logger.info(f"âœ… {ticker} OHLCVê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤ (ë§ˆì§€ë§‰ ì €ì¥ì¼: {last_date})")
                return
            df = get_ohlcv_d(ticker, interval='1d', count=days_diff)

        # ê¸°ì¡´: df = get_ohlcv_d() ì´í›„
        if df is not None and not df.empty:
            # í†µí•©ëœ ì €ì¥ ë¡œì§ ì‚¬ìš© (ë‚ ì§œ ë³µêµ¬ + ì ì‘í˜• ì†Œìˆ˜ì  + ì›ìì  ì €ì¥)
            save_result = enhanced_ohlcv_processor(ticker, df, data_source="api")
            if save_result:
                logger.info(f"âœ… {ticker} OHLCV ì—…ë°ì´íŠ¸ ì €ì¥ ì™„ë£Œ (í†µí•© íŒŒì´í”„ë¼ì¸)")
            else:
                logger.error(f"âŒ {ticker} OHLCV ì—…ë°ì´íŠ¸ ì €ì¥ ì‹¤íŒ¨ (í†µí•© íŒŒì´í”„ë¼ì¸)")

        # 3) Delete old records
        delete_old_ohlcv(ticker)
        logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì¶”ê°€ {len(df)}ê°œ ë´‰)")

    # ğŸ”§ [ì œê±°] ì¤‘ë³µ í•¨ìˆ˜ ì œê±° - data_fetcher.calculate_technical_indicators ì§ì ‘ ì‚¬ìš©

    def save_chart_image(self, ticker: str, df: pd.DataFrame) -> str:
        """ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤"""
        try:
            from data_fetcher import save_chart_image
            save_chart_image(ticker, df)
            return f"charts/{ticker}.png"
        except Exception as e:
            logger.error(f"âŒ {ticker} ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None

    def generate_ohlcv_json(self, ticker: str) -> str:
        """
        ìˆ˜ì • ëª©í‘œ: ë¶„ë¦¬ëœ í…Œì´ë¸”ì—ì„œ ì•ˆì „í•˜ê²Œ ë°ì´í„° ì¡°íšŒ
        
        ìˆ˜ì • ë‚´ìš©:
        1. get_combined_ohlcv_and_static_data() í•¨ìˆ˜ ì‚¬ìš©
        2. ohlcv ë™ì  ì§€í‘œì™€ static ì§€í‘œë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
        3. JSON êµ¬ì¡°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½:
        {
            "ticker": "KRW-BTC",
            "ohlcv": [...],  // ê¸°ë³¸ OHLCV + ë™ì  ì§€í‘œ
            "indicators": {
                "static": {...},  // pivot, r1, s1, support, resistance
                "dynamic": {...}  // ì‹œê³„ì—´ ë™ì  ì§€í‘œ
            }
        }
        """
        try:
            from utils import get_combined_ohlcv_and_static_data
            
            # ë¶„ë¦¬ëœ í…Œì´ë¸”ì—ì„œ ì•ˆì „í•˜ê²Œ ë°ì´í„° ì¡°íšŒ
            combined_data = get_combined_ohlcv_and_static_data(ticker, limit_days=100)
            
            if not combined_data['ohlcv_data']:
                logger.warning(f"âš ï¸ {ticker} OHLCV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # OHLCV ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            df = get_ohlcv_d(ticker, interval="day", count=100, force_fetch=False)
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ {ticker} ê¸°ë³¸ OHLCV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì •ì  ì§€í‘œ ë°ì´í„° ì¤€ë¹„
            static_indicators = {}
            if combined_data['static_data'] and combined_data['static_columns']:
                static_columns = combined_data['static_columns']
                static_values = combined_data['static_data']
                
                # ì •ì  ì§€í‘œë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ticker ì»¬ëŸ¼ ì œì™¸)
                for i, col in enumerate(static_columns):
                    if col != 'ticker' and i < len(static_values):
                        static_indicators[col] = static_values[i]
                
                logger.info(f"âœ… {ticker} ì •ì  ì§€í‘œ {len(static_indicators)}ê°œ ìˆ˜ì§‘: {list(static_indicators.keys())}")
            
            # JSON ë³€í™˜ì„ ìœ„í•´ ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            df.index = [safe_strftime(idx, '%Y-%m-%d') for idx in df.index]
            
            # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜í•˜ì—¬ JSON í˜¸í™˜ì„± í™•ë³´
            df = df.where(pd.notnull(df), None)
            
            # ìƒˆë¡œìš´ JSON êµ¬ì¡° ìƒì„±
            json_structure = {
                "ticker": ticker,
                "ohlcv": df.to_dict(orient='index'),
                "indicators": {
                    "static": static_indicators,
                    "dynamic": {
                        "description": "ì‹œê³„ì—´ ë™ì  ì§€í‘œë“¤ì€ ohlcv ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                        "available_in_ohlcv": ["ma_50", "ma_200", "rsi_14", "macd_histogram", "ht_trendline"]
                    }
                }
            }
            
            # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            import json
            json_data = json.dumps(json_structure, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… {ticker} ë¶„ë¦¬ëœ êµ¬ì¡°ì˜ JSON ë°ì´í„° ìƒì„± ì™„ë£Œ")
            logger.info(f"   - OHLCV ë°ì´í„°: {len(df)}ì¼ì¹˜")
            logger.info(f"   - ì •ì  ì§€í‘œ: {len(static_indicators)}ê°œ")
            
            return json_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} ë¶„ë¦¬ êµ¬ì¡° JSON ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            try:
                logger.info(f"ğŸ”„ {ticker} ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°± ì‹œë„")
                df = get_ohlcv_d(ticker, interval="day", count=100, force_fetch=False)
                
                if df is not None and not df.empty:
                    df.index = [safe_strftime(idx, '%Y-%m-%d') for idx in df.index]
                    df = df.where(pd.notnull(df), None)
                    json_data = df.to_json(orient='index', indent=2)
                    logger.info(f"âœ… {ticker} í´ë°± JSON ìƒì„± ì„±ê³µ")
                    return json_data
                
            except Exception as fallback_error:
                logger.error(f"âŒ {ticker} í´ë°±ë„ ì‹¤íŒ¨: {fallback_error}")
            
            return None

    def fetch_market_data_internal(self, tickers: list, timeframe: str = '1d') -> pd.DataFrame:
        """
        ì‹œì¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ - ë‚ ì§œ ë³µêµ¬ ìš°ì„  ì‹¤í–‰
        """
        try:
            import pandas as pd
            start_time = time.time()
            logger.info(f"ğŸ”„ {timeframe} ë´‰ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ë° í•„í„°ë§
            blacklist = load_blacklist() or {}
            filtered_tickers = [ticker for ticker in tickers if ticker not in blacklist]
            if not filtered_tickers:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            logger.info(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ í‹°ì»¤: {len(filtered_tickers)}ê°œ (ì „ì²´: {len(tickers)}ê°œ, ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œì™¸: {len(tickers) - len(filtered_tickers)}ê°œ)")

            processed_tickers = []
            failed_tickers = []

            for i, ticker in enumerate(filtered_tickers, 1):
                try:
                    logger.info(f"ğŸ”„ [{i}/{len(filtered_tickers)}] {ticker} ì²˜ë¦¬ ì‹œì‘...")
                    
                    if ticker in load_blacklist():
                        logger.info(f"â­ï¸ {ticker}ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆì–´ ì²˜ë¦¬ ê±´ë„ˆëœ€")
                        continue

                    # OHLCV ê°€ì ¸ì˜¤ê¸° - ê°œì„ ëœ í†µí•© ë°©ì‹
                    if timeframe == '1d':
                        # 1ë‹¨ê³„: DBì—ì„œ ë¨¼ì € ì¡°íšŒ
                        ohlcv_data = self.db_mgr.fetch_ohlcv(ticker, days=450)
                        
                        # 2ë‹¨ê³„: DBì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ APIì—ì„œ ìˆ˜ì§‘
                        if ohlcv_data is None or ohlcv_data.empty:
                            logger.warning(f"âš ï¸ {ticker} DBì— OHLCV ë°ì´í„° ì—†ìŒ â†’ API ìˆ˜ì§‘ ì‹œë„")
                            from data_fetcher import get_ohlcv_d
                            ohlcv_data = get_ohlcv_d(ticker, count=450)
                            
                            if ohlcv_data is None or ohlcv_data.empty:
                                logger.error(f"âŒ {ticker} API ìˆ˜ì§‘ ì‹¤íŒ¨")
                                failed_tickers.append(ticker)
                                continue
                            else:
                                # ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥
                                self.db_mgr.insert_ohlcv(ticker, ohlcv_data)
                                logger.info(f"âœ… {ticker} API ìˆ˜ì§‘ ë° DB ì €ì¥ ì™„ë£Œ")
                    else:
                        # 4ì‹œê°„ë´‰ì€ ë³„ë„ ì²˜ë¦¬
                        from data_fetcher import get_ohlcv_4h
                        ohlcv_data = get_ohlcv_4h(ticker)

                    # ë°ì´í„° ê²€ì¦ ê°•í™”
                    if ohlcv_data is None or ohlcv_data.empty:
                        logger.error(f"âŒ {ticker} OHLCV ë°ì´í„° ìˆ˜ì§‘/ì¡°íšŒ ì™„ì „ ì‹¤íŒ¨")
                        failed_tickers.append(ticker)
                        continue

                    # âœ… í•µì‹¬: ì§€í‘œ ê³„ì‚° ì „ì— í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬
                    if hasattr(ohlcv_data.index, 'year') and len(ohlcv_data.index) > 0 and ohlcv_data.index[0].year == 1970:
                        logger.warning(f"ğŸš¨ {ticker} 1970 ë‚ ì§œ ê°ì§€ - ì§€í‘œ ê³„ì‚° ì „ í†µí•© ì²˜ë¦¬")
                        enhanced_ohlcv_processor(ticker, ohlcv_data, data_source="api")
                        logger.info(f"âœ… {ticker} í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ")

                    # ì•ˆì „í•œ ë‚ ì§œ ì¶œë ¥ì„ ìœ„í•œ ì²˜ë¦¬
                    try:
                        from utils import safe_strftime
                        if hasattr(ohlcv_data.index, '__len__') and len(ohlcv_data.index) > 0:
                            # ì¸ë±ìŠ¤ê°€ DatetimeIndexì¸ì§€ í™•ì¸
                            if isinstance(ohlcv_data.index, pd.DatetimeIndex):
                                start_date = safe_strftime(ohlcv_data.index[0])
                                end_date = safe_strftime(ohlcv_data.index[-1])
                                
                                # 1970-01-01 íŒ¨í„´ ê°ì§€ ë° í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
                                if start_date == "1970-01-01" and end_date == "1970-01-01":
                                    logger.warning(f"ğŸš¨ {ticker} ë‚ ì§œ ì´ìƒ ê°ì§€: ê¸°ê°„: {start_date} ~ {end_date}")
                                    ohlcv_data_before = f"{start_date} ~ {end_date}"
                                    
                                    # í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë‚ ì§œ ë³µêµ¬ + ì†Œìˆ˜ì  ì²˜ë¦¬ + ì €ì¥)
                                    enhanced_ohlcv_processor(ticker, ohlcv_data, data_source="api")
                                    
                                    logger.info(f"ğŸ“… {ticker} í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ: {ohlcv_data_before}")
                                    logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ")
                                else:
                                    logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ (ê¸°ê°„: {start_date} ~ {end_date})")
                            else:
                                # DatetimeIndexê°€ ì•„ë‹Œ ê²½ìš° ë³€í™˜ ì‹œë„
                                ohlcv_data.index = pd.to_datetime(ohlcv_data.index)
                                start_date = safe_strftime(ohlcv_data.index[0])
                                end_date = safe_strftime(ohlcv_data.index[-1])
                                
                                # 1970-01-01 íŒ¨í„´ ê°ì§€ ë° í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
                                if start_date == "1970-01-01" and end_date == "1970-01-01":
                                    logger.warning(f"ğŸš¨ {ticker} ë‚ ì§œ ì´ìƒ ê°ì§€: ê¸°ê°„: {start_date} ~ {end_date}")
                                    ohlcv_data_before = f"{start_date} ~ {end_date}"
                                    
                                    # í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë‚ ì§œ ë³µêµ¬ + ì†Œìˆ˜ì  ì²˜ë¦¬ + ì €ì¥)
                                    enhanced_ohlcv_processor(ticker, ohlcv_data, data_source="api")
                                    
                                    logger.info(f"ğŸ“… {ticker} í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ: {ohlcv_data_before}")
                                    logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ")
                                else:
                                    logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ (ê¸°ê°„: {start_date} ~ {end_date})")
                        else:
                            logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ (ë‚ ì§œ ì •ë³´ ì—†ìŒ)")
                    except Exception as date_err:
                        logger.info(f"âœ… {ticker} OHLCV ë°ì´í„° í™•ë³´: {len(ohlcv_data)}ê°œ ë ˆì½”ë“œ")
                        logger.debug(f"ë‚ ì§œ ì¶œë ¥ ì˜¤ë¥˜: {date_err}")

                    # ========== í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ==========
                    # 1ë‹¨ê³„: í†µí•© OHLCV ì²˜ë¦¬ (ë‚ ì§œ ë³µêµ¬ + ì†Œìˆ˜ì  ì²˜ë¦¬ + ì €ì¥)
                    logger.info(f"1ë‹¨ê³„: {ticker} í†µí•© OHLCV ì²˜ë¦¬")
                    from data_fetcher import enhanced_ohlcv_processor
                    save_result = enhanced_ohlcv_processor(ticker, ohlcv_data, data_source="api")
                    if not save_result:
                        logger.error(f"âŒ {ticker} í†µí•© OHLCV ì²˜ë¦¬ ì‹¤íŒ¨ - ì§€í‘œ ê³„ì‚° ì¤‘ë‹¨")
                        failed_tickers.append(ticker)
                        continue
                    
                    # 2ë‹¨ê³„: ì§€í‘œ ê³„ì‚°
                    logger.info(f"2ë‹¨ê³„: {ticker} ì§€í‘œ ê³„ì‚°")
                    # ========================================
                    
                    # âœ… ë³µêµ¬ëœ ì˜¬ë°”ë¥¸ ë‚ ì§œë¡œ ì§€í‘œ ê³„ì‚°
                    logger.info(f"ğŸ”„ {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘...")
                    if timeframe == '1d':
                        # í†µí•© ì§€í‘œ ê³„ì‚° ì‚¬ìš© (static_indicators + ohlcv ë™ì ì§€í‘œ)
                        from data_fetcher import calculate_unified_indicators
                        df_with_indicators = calculate_unified_indicators(ohlcv_data, ticker)
                    else:
                        from data_fetcher import calculate_technical_indicators_4h
                        df_with_indicators = calculate_technical_indicators_4h(ohlcv_data)

                    if df_with_indicators is None or df_with_indicators.empty:
                        logger.warning(f"âš ï¸ {ticker} {timeframe} ë´‰ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                        failed_tickers.append(ticker)
                        continue
                    else:
                        logger.info(f"âœ… {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ: {len(df_with_indicators)}ê°œ ë ˆì½”ë“œ")

                    # DB ì €ì¥
                    logger.info(f"ğŸ”„ {ticker} DB ì €ì¥ ì‹œì‘...")
                    if timeframe == '1d':
                        # ì›ìì  í†µí•© ì €ì¥ (static_indicators + ohlcv ë™ì ì§€í‘œë§Œ)
                        from data_fetcher import save_all_indicators_atomically
                        save_result = save_all_indicators_atomically(ticker, df_with_indicators, timeframe)
                        if save_result:
                            logger.info(f"âœ… {ticker} static_indicators + ohlcv ë™ì ì§€í‘œ ì €ì¥ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ {ticker} ì§€í‘œ ì €ì¥ ì‹¤íŒ¨")
                            failed_tickers.append(ticker)
                            continue
                    else:
                        from data_fetcher import save_market_data_4h_to_db
                        save_market_data_4h_to_db(ticker, df_with_indicators)

                    processed_tickers.append(ticker)
                    logger.info(f"âœ… {ticker} {timeframe} ë´‰ ì²˜ë¦¬ ì™„ë£Œ")

                except Exception as e:
                    logger.error(f"âŒ {ticker} {timeframe} ë´‰ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {str(e)}")
                    import traceback
                    logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    failed_tickers.append(ticker)
                    continue

            # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
            logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"   - ì„±ê³µ: {len(processed_tickers)}ê°œ")
            logger.info(f"   - ì‹¤íŒ¨: {len(failed_tickers)}ê°œ")
            if failed_tickers:
                logger.warning(f"   - ì‹¤íŒ¨ í‹°ì»¤: {failed_tickers[:10]}" + (f" ì™¸ {len(failed_tickers)-10}ê°œ" if len(failed_tickers) > 10 else ""))

            # ê²°ê³¼ ì¡°íšŒ - static_indicators í…Œì´ë¸”ì—ì„œ ì¡°íšŒ (í‘œì¤€í™”ëœ DB ì—°ê²° ì‚¬ìš©)
            if not processed_tickers:
                logger.warning("âš ï¸ ì²˜ë¦¬ëœ í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            with self.get_db_connection_safe() as conn:
                if timeframe == '1d':
                    # 1ì¼ë´‰: static_indicators í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                    table_name = "static_indicators"
                    logger.info(f"ğŸ” {table_name} í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                    
                    market_data = pd.read_sql_query(
                        f"SELECT * FROM {table_name} WHERE ticker IN %s",
                        conn,
                        params=(tuple(processed_tickers),)
                    )
                else:
                    # 4ì‹œê°„ë´‰: ê¸°ì¡´ market_data_4h í…Œì´ë¸” ì‚¬ìš©
                    table_name = "market_data_4h"
                    logger.info(f"ğŸ” {table_name} í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                    
                    market_data = pd.read_sql_query(
                        f"SELECT * FROM {table_name} WHERE ticker IN %s",
                        conn,
                        params=(tuple(processed_tickers),)
                    )

                logger.info(f"ğŸ” {table_name} í…Œì´ë¸” ì¡°íšŒ ê²°ê³¼: {len(market_data)}ê°œ ë ˆì½”ë“œ")
                
                if not market_data.empty:
                    market_data.set_index('ticker', inplace=True)
                    logger.info(f"âœ… {timeframe} ë´‰ ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(market_data)}ê°œ í‹°ì»¤")
                    
                    # ì¡°íšŒëœ í‹°ì»¤ ëª©ë¡ ë¡œê¹…
                    retrieved_tickers = market_data.index.tolist()
                    logger.info(f"ğŸ“‹ ì¡°íšŒëœ í‹°ì»¤: {retrieved_tickers[:10]}" + (f" ì™¸ {len(retrieved_tickers)-10}ê°œ" if len(retrieved_tickers) > 10 else ""))
                else:
                    logger.error(f"âŒ {table_name} í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŒ (ì „ì²´ ë ˆì½”ë“œ: 0ê°œ)")
                    
                    # í…Œì´ë¸” ì „ì²´ ìƒíƒœ í™•ì¸
                    total_count_query = f"SELECT COUNT(*) as cnt FROM {table_name}"
                    total_count_df = pd.read_sql_query(total_count_query, conn)
                    total_count = total_count_df.iloc[0]['cnt'] if not total_count_df.empty else 0
                    logger.error(f"âŒ {table_name} í…Œì´ë¸” ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {total_count}ê°œ")
            
            logger.info(f"âœ… {timeframe} ë´‰ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(processed_tickers)}ê°œ, ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")
            return market_data

        except Exception as e:
            logger.error(f"âŒ {timeframe} ë´‰ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return pd.DataFrame()
    
    def run_backtest_and_report(self, ohlcv_df, market_df) -> bool:
        """í†µí•©ëœ backtester.py ì‚¬ìš©ìœ¼ë¡œ ê¸°ëŠ¥ í™•ì¥"""
        if ohlcv_df is None or ohlcv_df.empty:
            logger.warning("âš ï¸ OHLCV ë°ì´í„° ì—†ìŒ")
            return False
        if market_df is None or market_df.empty:
            logger.warning("âš ï¸ ì‹œì¥ ë°ì´í„° ì—†ìŒ")
            return False
            
        try:
            # ğŸ”§ [ê°œì„ ] ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± - backtest_ohlcv, backtest_sessions í…Œì´ë¸” ì—…ë°ì´íŠ¸
            logger.info("ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹œì‘...")
            from backtester import BacktestDataManager
            import pandas as pd
            
            # ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
            backtest_manager = BacktestDataManager()
            
            # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë°±í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ìƒì„±
            from datetime import datetime
            session_name = f"makenaide_backtest_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± (ìµœê·¼ 200ì¼ ë°ì´í„°)
            session_id = backtest_manager.create_backtest_snapshot(session_name, period_days=200)
            
            if session_id:
                logger.info(f"âœ… ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± ì™„ë£Œ: {session_id}")
                
                # ìƒì„±ëœ ìŠ¤ëƒ…ìƒ· ë°ì´í„° í™•ì¸
                snapshot_data = backtest_manager.get_backtest_data(session_id)
                if not snapshot_data.empty:
                    logger.info(f"ğŸ“Š ìŠ¤ëƒ…ìƒ· ë°ì´í„° í™•ì¸: {len(snapshot_data)}ê°œ ë ˆì½”ë“œ")
                else:
                    logger.warning("âš ï¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            else:
                logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹¤íŒ¨")
            
            # ê¸°ì¡´ SPOT_COMBOS ë°±í…ŒìŠ¤íŠ¸ + ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
            from backtester import (
                backtest_combo, SPOT_COMBOS, HYBRID_SPOT_COMBOS, 
                generate_strategy_report, HybridFilteringBacktester,
                backtest_hybrid_filtering_performance
            )
            
            # 1. ê¸°ì¡´ ì „ëµ ì¡°í•© ë°±í…ŒìŠ¤íŠ¸
            logger.info("ğŸ¯ ê¸°ì¡´ ì „ëµ ì¡°í•© ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘")
            all_results = []
            
            # ê¸°ì¡´ SPOT_COMBOS + ìƒˆë¡œìš´ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
            all_combos = SPOT_COMBOS + HYBRID_SPOT_COMBOS
            
            for combo in all_combos:
                logger.info(f'â–¶ï¸ {combo["name"]} ë°±í…ŒìŠ¤íŠ¸ ì¤‘...')
                results = backtest_combo(ohlcv_df, market_df, combo)
                if results:
                    all_results.extend(results)
                    
            if all_results:
                df_result = pd.DataFrame(all_results)
                logger.info('=== í™•ì¥ëœ ìŠ¤ìœ—ìŠ¤íŒŸ ì¡°ê±´ë³„ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===')
                logger.info(df_result.groupby('combo').agg({
                    'win_rate':'mean','avg_return':'mean','mdd':'mean',
                    'trades':'sum','b':'mean','kelly':'mean',
                    'kelly_1_2':'mean','swing_score':'mean'
                }))
                
                # ğŸ”§ [ê°œì„ ] DB ì €ì¥ ì¶”ê°€
                try:
                    from backtester import BacktestDataManager
                    backtest_manager = BacktestDataManager()
                    
                    # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ê¸°ê°„ ì •ë³´ ì¶”ê°€
                    from datetime import datetime, timedelta
                    period_end = datetime.now().date()
                    period_start = period_end - timedelta(days=200)  # 200ì¼ ê¸°ê°„
                    
                    for result in all_results:
                        result['period_start'] = period_start
                        result['period_end'] = period_end
                    
                    # DBì— ê²°ê³¼ ì €ì¥
                    if hasattr(backtest_manager, 'save_backtest_results_to_db'):
                        save_success = backtest_manager.save_backtest_results_to_db(all_results, session_id)
                        if save_success:
                            logger.info("âœ… ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ DB ì €ì¥ ì™„ë£Œ")
                            
                            # ì €ì¥ëœ ê²°ê³¼ í™•ì¸
                            saved_results = backtest_manager.get_backtest_results_from_db(session_id)
                            if not saved_results.empty:
                                logger.info(f"ğŸ“Š DB ì €ì¥ í™•ì¸: {len(saved_results)}ê°œ ì „ëµ ê²°ê³¼")
                            else:
                                logger.warning("âš ï¸ DB ì €ì¥ í™•ì¸ ì‹¤íŒ¨")
                        else:
                            logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨")
                    else:
                        logger.warning("âš ï¸ save_backtest_results_to_db ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                except Exception as db_error:
                    logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {db_error}")
                
                # ê¸°ì¡´ CSV ì €ì¥ ìœ ì§€ (í˜¸í™˜ì„±)
                df_result.to_csv('backtest_spot_results_with_hybrid.csv', index=False, float_format='%.2f')
                logger.info('ê²°ê³¼ê°€ backtest_spot_results_with_hybrid.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.')
            
            # 2. í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì„±ëŠ¥ ë¹„êµ ë°±í…ŒìŠ¤íŠ¸
            logger.info("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
            
            # ìµœê·¼ 30ì¼ê°„ í•˜ì´ë¸Œë¦¬ë“œ vs ì •ì ì „ìš© ë¹„êµ
            from datetime import datetime, timedelta
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            backtest_period = f"{start_date}:{end_date}"
            
            hybrid_comparison, optimal_weights = backtest_hybrid_filtering_performance(backtest_period)
            
            if hybrid_comparison:
                logger.info("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
                logger.info(f"   - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: {hybrid_comparison.get('hybrid', {})}")
                logger.info(f"   - ì •ì ì „ìš© ëª¨ë“œ: {hybrid_comparison.get('static_only', {})}")
                logger.info(f"   - ìµœì  ê°€ì¤‘ì¹˜: {optimal_weights}")
                
                # í•˜ì´ë¸Œë¦¬ë“œ ë¹„êµ ê²°ê³¼ ì €ì¥
                with open('hybrid_filtering_comparison.json', 'w', encoding='utf-8') as f:
                    json.dump({
                        'comparison': hybrid_comparison,
                        'optimal_weights': optimal_weights,
                        'period': backtest_period,
                        'generated_at': datetime.now().isoformat()
                    }, f, indent=2, ensure_ascii=False)
            
            # 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œê³¼ ì—°ë™
            logger.info("ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì—°ë™")
            try:
                from performance_monitor import get_performance_monitor
                monitor = get_performance_monitor()
                
                backtest_metrics = {
                    'strategy_count': len(all_results) if all_results else 0,
                    'hybrid_comparison': hybrid_comparison,
                    'optimal_weights': optimal_weights,
                    'total_combos_tested': len(all_combos),
                    'hybrid_combos_tested': len([c for c in all_combos if c.get('hybrid_filtering')]),
                    'backtest_period': backtest_period
                }
                
                # ë°±í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ê¸°ë¡ (ìƒˆë¡œìš´ ë©”ì„œë“œ í•„ìš”)
                if hasattr(monitor, 'record_backtest_session'):
                    monitor.record_backtest_session(backtest_metrics)
                
            except Exception as monitor_error:
                logger.warning(f"âš ï¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—°ë™ ì¤‘ ì˜¤ë¥˜: {monitor_error}")
            
            # 4. ê¸°ì¡´ ì „ëµ ë¦¬í¬íŠ¸ ìƒì„±
            logger.info("ğŸ“„ ì „ëµë³„ ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±")
            generate_strategy_report(period_days=30, output_path='strategy_report.csv', send_email=True)
            
            # ğŸ”§ [ê°œì„ ] ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¶”ê°€
            logger.info("ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
            try:
                if hasattr(backtest_manager, 'generate_backtest_analysis_report'):
                    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
                    markdown_report = backtest_manager.generate_backtest_analysis_report(
                        session_id=session_id, 
                        output_format="markdown"
                    )
                    
                    if markdown_report and not markdown_report.startswith("âš ï¸"):
                        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
                        report_filename = f"backtest_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                        with open(report_filename, 'w', encoding='utf-8') as f:
                            f.write(markdown_report)
                        logger.info(f"âœ… ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_filename}")
                        
                        # HTML ë¦¬í¬íŠ¸ë„ ìƒì„±
                        html_report = backtest_manager.generate_backtest_analysis_report(
                            session_id=session_id, 
                            output_format="html"
                        )
                        if html_report and not html_report.startswith("âš ï¸"):
                            html_filename = f"backtest_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                            with open(html_filename, 'w', encoding='utf-8') as f:
                                f.write(html_report)
                            logger.info(f"âœ… HTML ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {html_filename}")
                    else:
                        logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
                else:
                    logger.warning("âš ï¸ generate_backtest_analysis_report ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                    
            except Exception as report_error:
                logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {report_error}")
            
            # 5. ë¦¬í¬íŠ¸ ê¸°ë°˜ ìë™ íŠœë‹ ì‹¤í–‰
            logger.info('ğŸ”§ ì „ëµë³„ Kelly fraction ìë™ íŠœë‹ ì‹¤í–‰')
            try:
                from strategy_tuner import auto_tune_strategies
                auto_tune_strategies(report_path='strategy_report.csv', config_path='config/strategy.yaml')
            except ImportError:
                logger.warning("âš ï¸ strategy_tuner ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìë™ íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            # ğŸ”§ [ê°œì„ ] ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
            logger.info("ğŸ” ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦ ì¤‘...")
            try:
                # ìµœì‹  ë°±í…ŒìŠ¤íŠ¸ ì„¸ì…˜ í™•ì¸
                from backtester import BacktestDataManager
                backtest_manager = BacktestDataManager()
                
                # ìµœì‹  ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
                latest_session = backtest_manager._get_latest_session_id()
                if latest_session:
                    session_info = backtest_manager.get_session_info(latest_session)
                    if session_info:
                        logger.info(f"âœ… ìµœì‹  ë°±í…ŒìŠ¤íŠ¸ ì„¸ì…˜ í™•ì¸: {latest_session}")
                        logger.info(f"   - ì„¸ì…˜ëª…: {session_info.get('name', 'N/A')}")
                        logger.info(f"   - ê¸°ê°„: {session_info.get('period_start', 'N/A')} ~ {session_info.get('period_end', 'N/A')}")
                        logger.info(f"   - ìƒì„±ì¼: {session_info.get('created_at', 'N/A')}")
                    else:
                        logger.warning("âš ï¸ ì„¸ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
                else:
                    logger.warning("âš ï¸ ìµœì‹  ë°±í…ŒìŠ¤íŠ¸ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
            except Exception as verify_error:
                logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {verify_error}")
            
            logger.info("âœ… í™•ì¥ëœ ë°±í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f'âŒ í™•ì¥ëœ ë°±í…ŒìŠ¤íŠ¸/ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}')
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def _auto_sync_blacklist_with_is_active(self):
        """
        ì¼ê´€ì„± ì €í•˜ ì‹œ ìë™ ë™ê¸°í™” ìˆ˜í–‰
        
        ë¸”ë™ë¦¬ìŠ¤íŠ¸ì™€ is_active ì»¬ëŸ¼ ê°„ì˜ ë¶ˆì¼ì¹˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
        ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ is_active ì»¬ëŸ¼ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        ë™ê¸°í™” ë¡œì§:
        1. ëª¨ë“  í‹°ì»¤ë¥¼ ì¼ë‹¨ í™œì„±í™” (is_active = true)
        2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í‹°ì»¤ë“¤ì„ ë¹„í™œì„±í™” (is_active = false)
        3. ë™ê¸°í™” ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…
        
        Returns:
            bool: ë™ê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ğŸ”„ ë¸”ë™ë¦¬ìŠ¤íŠ¸-is_active ìë™ ë™ê¸°í™” ì‹œì‘...")
            
            blacklist = load_blacklist()
            if not blacklist:
                logger.warning("âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨")
                return False
            
            # ë™ê¸°í™” ì „ ìƒíƒœ í™•ì¸
            pre_sync_query = """
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN is_active = true THEN 1 END) as active_before,
                    COUNT(CASE WHEN is_active = false THEN 1 END) as inactive_before
                FROM tickers
            """
            pre_sync_result = self.db_mgr.execute_query(pre_sync_query)
            if pre_sync_result:
                total, active_before, inactive_before = pre_sync_result[0]
                logger.info(f"ğŸ“Š ë™ê¸°í™” ì „ ìƒíƒœ: ì´ {total}ê°œ (í™œì„± {active_before}ê°œ, ë¹„í™œì„± {inactive_before}ê°œ)")
            
            # 1ë‹¨ê³„: ëª¨ë“  í‹°ì»¤ë¥¼ ì¼ë‹¨ í™œì„±í™”
            logger.info("1ï¸âƒ£ ëª¨ë“  í‹°ì»¤ í™œì„±í™” ì¤‘...")
            update_query = "UPDATE tickers SET is_active = true"
            update_result = self.db_mgr.execute_query(update_query)
            
            # 2ë‹¨ê³„: ë¸”ë™ë¦¬ìŠ¤íŠ¸ í‹°ì»¤ë“¤ì„ ë¹„í™œì„±í™”
            blacklisted_tickers = list(blacklist.keys())
            logger.info(f"2ï¸âƒ£ ë¸”ë™ë¦¬ìŠ¤íŠ¸ í‹°ì»¤ ë¹„í™œì„±í™” ì¤‘: {len(blacklisted_tickers)}ê°œ")
            
            if blacklisted_tickers:
                placeholders = ','.join(['%s'] * len(blacklisted_tickers))
                deactivate_query = f"""
                    UPDATE tickers 
                    SET is_active = false 
                    WHERE ticker IN ({placeholders})
                """
                deactivate_result = self.db_mgr.execute_query(deactivate_query, blacklisted_tickers)
                
                # ì‹¤ì œë¡œ ì—…ë°ì´íŠ¸ëœ í‹°ì»¤ í™•ì¸
                verify_query = f"""
                    SELECT ticker FROM tickers 
                    WHERE ticker IN ({placeholders}) AND is_active = false
                """
                verify_result = self.db_mgr.execute_query(verify_query, blacklisted_tickers)
                updated_tickers = [row[0] for row in verify_result] if verify_result else []
                
                logger.info(f"   - ë¸”ë™ë¦¬ìŠ¤íŠ¸ ëŒ€ìƒ: {len(blacklisted_tickers)}ê°œ")
                logger.info(f"   - ì‹¤ì œ ë¹„í™œì„±í™”: {len(updated_tickers)}ê°œ")
                
                if len(updated_tickers) != len(blacklisted_tickers):
                    missing_tickers = set(blacklisted_tickers) - set(updated_tickers)
                    logger.warning(f"   âš ï¸ ë¹„í™œì„±í™” ì‹¤íŒ¨ í‹°ì»¤: {missing_tickers}")
            
            # ë™ê¸°í™” í›„ ìƒíƒœ í™•ì¸
            post_sync_result = self.db_mgr.execute_query(pre_sync_query)
            if post_sync_result:
                total, active_after, inactive_after = post_sync_result[0]
                logger.info(f"ğŸ“Š ë™ê¸°í™” í›„ ìƒíƒœ: ì´ {total}ê°œ (í™œì„± {active_after}ê°œ, ë¹„í™œì„± {inactive_after}ê°œ)")
                
                # ë³€í™”ëŸ‰ ê³„ì‚°
                active_change = active_after - active_before
                inactive_change = inactive_after - inactive_before
                logger.info(f"ğŸ“ˆ ë³€í™”ëŸ‰: í™œì„± {active_change:+d}ê°œ, ë¹„í™œì„± {inactive_change:+d}ê°œ")
            
            logger.info(f"âœ… ìë™ ë™ê¸°í™” ì™„ë£Œ: {len(blacklisted_tickers)}ê°œ í‹°ì»¤ ë¹„í™œì„±í™”")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def _validate_active_status_only(self, ticker_list):
        """
        ì´ë¯¸ í•„í„°ë§ëœ í‹°ì»¤ë“¤ì˜ í™œì„± ìƒíƒœë§Œ ê²€ì¦ (ì—­ì¦ê°€ ë°©ì§€)
        + ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¡œì§ ì¶”ê°€
        
        Args:
            ticker_list: ì´ë¯¸ ê±°ë˜ëŒ€ê¸ˆ/ê¸°ìˆ ì  ë¶„ì„ìœ¼ë¡œ í•„í„°ë§ëœ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: í™œì„± ìƒíƒœì´ë©´ì„œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” í‹°ì»¤ë“¤ë§Œ ë°˜í™˜
        """
        import time
        start_time = time.time()
        
        if not ticker_list:
            return []
        
        try:
            # is_active ì»¬ëŸ¼ í™œìš© ê²€ì¦
            placeholders = ','.join(['%s'] * len(ticker_list))
            active_query = f"""
                SELECT ticker FROM tickers 
                WHERE ticker IN ({placeholders}) AND is_active = true
            """
            
            active_result = self.db_mgr.execute_query(active_query, ticker_list)
            active_tickers = [row[0] for row in active_result] if active_result else []
            
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ í•„í„°ë§
            blacklist = load_blacklist()
            final_tickers = [t for t in active_tickers if t not in blacklist]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            processing_time = time.time() - start_time
            efficiency = len(final_tickers) / len(ticker_list) if ticker_list else 0
            throughput = len(ticker_list) / processing_time if processing_time > 0 else 0
            
            # ê¸°ë³¸ ê²€ì¦ ë¡œê¹…
            logger.info(f"ğŸ” í‹°ì»¤ í™œì„± ìƒíƒœ ê²€ì¦:")
            logger.info(f"   - ì…ë ¥ í‹°ì»¤: {len(ticker_list)}ê°œ")
            logger.info(f"   - í™œì„± í‹°ì»¤: {len(active_tickers)}ê°œ") 
            logger.info(f"   - ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œì™¸ í›„: {len(final_tickers)}ê°œ")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤ ë¡œê¹…
            logger.info(f"âš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤:")
            logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
            logger.info(f"   - í•„í„°ë§ íš¨ìœ¨: {efficiency:.2%}")
            logger.info(f"   - ì²˜ë¦¬ ì†ë„: {throughput:.0f} í‹°ì»¤/ì´ˆ")
            
            # ì„±ëŠ¥ ì„ê³„ì¹˜ í™•ì¸ ë° ê²½ê³ 
            if processing_time > 0.1:  # 100ms ì´ˆê³¼
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼: {processing_time:.3f}ì´ˆ (ì„ê³„ì¹˜: 0.1ì´ˆ)")
            
            if efficiency < 0.8:  # íš¨ìœ¨ 80% ë¯¸ë§Œ
                logger.warning(f"âš ï¸ í•„í„°ë§ íš¨ìœ¨ ì €í•˜: {efficiency:.2%} (ì„ê³„ì¹˜: 80%)")
            
            return final_tickers
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ í™œì„± ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {processing_time:.3f}ì´ˆ): {e}")
            
            # í´ë°±: ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ ì ìš©
            try:
                blacklist = load_blacklist()
                filtered_tickers = [t for t in ticker_list if t not in blacklist]
                fallback_time = time.time() - start_time
                logger.warning(f"ğŸ”„ í´ë°± í•„í„°ë§ (ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ): {len(filtered_tickers)}ê°œ (ì†Œìš”ì‹œê°„: {fallback_time:.3f}ì´ˆ)")
                return filtered_tickers
            except:
                total_time = time.time() - start_time
                logger.error(f"âŒ í´ë°± í•„í„°ë§ë„ ì‹¤íŒ¨ (ì´ ì†Œìš”ì‹œê°„: {total_time:.3f}ì´ˆ)")
                return ticker_list

    def filter_comprehensive_indicators(self, market_df=None, timeframe='1d'):
        """
        ìˆ˜ì •ëœ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§: ì´ë¯¸ í•„í„°ë§ëœ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í™œì„± í‹°ì»¤ ê²€ì¦
        
        ì •ì +ë™ì  ì§€í‘œë¥¼ ì¡°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ìœ¼ë¡œ ëŒíŒŒ ë§¤ë§¤ í›„ë³´ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
        
        ë°ì´í„° ì†ŒìŠ¤:
        - static_indicators: ì •ì  ì§€í‘œ (resistance, support, atr, adx, price, high_60 ë“±)
        - ohlcv: ë™ì  ì§€í‘œ (rsi_14, macd_histogram, bb_upper, bb_lower, volume_20ma ë“±)
        
        ìˆ˜ì •ëœ ë…¼ë¦¬ì  ìˆœì„œ:
        1. static_indicatorsì—ì„œ ì´ë¯¸ í•„í„°ë§ëœ í‹°ì»¤ ì¡°íšŒ (ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì™„ë£Œ)
        2. ì´ í‹°ì»¤ë“¤ì˜ í™œì„± ìƒíƒœë§Œ ê²€ì¦ (is_active=true í™•ì¸)
        3. ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œì™¸í•˜ì—¬ ìµœì¢… ëŒ€ìƒ í™•ì •
        4. ê²€ì¦ëœ í‹°ì»¤ë“¤ë¡œë§Œ í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ìˆ˜í–‰
        
        Args:
            market_df (pd.DataFrame, optional): ê¸°ì¡´ ì‹œì¥ ë°ì´í„° (í˜¸í™˜ì„± ìœ ì§€ìš©)
            timeframe (str): ì‹œê°„ í”„ë ˆì„ ('1d', '4h' ë“±)

        Returns:
            pd.DataFrame: í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ì„ í†µê³¼í•œ ì¢…ëª© ë°ì´í„° (ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ í¬í•¨)
        """
        try:
            import time
            from datetime import datetime
            self._filter_start_time = time.time()  # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            
            logger.info("ğŸ” ìˆ˜ì •ëœ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ (ì •ì +ë™ì  ì§€í‘œ) ì‹œì‘...")

            # í•„í„°ë§ ì„¤ì • ë¡œë“œ (YAML íŒŒì¼ì—ì„œ)
            from filter_tickers import load_filter_config
            config = load_filter_config("config/filter_rules_config.yaml")
            
            # ì‚¬ìš©ì ì •ì˜ ì„¤ì •ì´ ìˆìœ¼ë©´ mode í‚¤ë¥¼ ë³´ì¡´í•˜ë©° ë³‘í•©
            if hasattr(self, 'config') and self.config.get('filter'):
                user_config = self.config.get('filter', {})
                original_mode = config.get('mode')
                config.update(user_config)
                if 'mode' not in user_config and original_mode is not None:
                    config['mode'] = original_mode

            # í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ í™œì„±í™” í™•ì¸
            enable_hybrid = config.get('enable_hybrid_filtering', True)
            if not enable_hybrid:
                logger.info("í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ì´ ë¹„í™œì„±í™”ë¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
                if market_df is None or market_df.empty:
                    logger.warning("âš ï¸ ì‹œì¥ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return pd.DataFrame()
                from filter_tickers import filter_breakout_candidates
                return filter_breakout_candidates(market_df, config)

            # === 1ë‹¨ê³„: ì´ë¯¸ í•„í„°ë§ëœ ì‹œì¥ ë°ì´í„° ê¸°ì¤€ ===
            if market_df is None or market_df.empty:
                # static_indicatorsì—ì„œ í•„í„°ë§ëœ í‹°ì»¤ ì¡°íšŒ (ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì™„ë£Œ)
                static_result = self.db_mgr.execute_query("""
                    SELECT ticker FROM static_indicators 
                    WHERE price > 0 AND high_60 > 0
                """)
                if not static_result:
                    logger.warning("âš ï¸ í•„í„°ë§ëœ static ë°ì´í„° ì—†ìŒ")
                    return pd.DataFrame()
                
                pre_filtered_tickers = [row[0] for row in static_result]
                logger.info(f"ğŸ“Š ì‚¬ì „ í•„í„°ë§ ì™„ë£Œëœ í‹°ì»¤: {len(pre_filtered_tickers)}ê°œ")
            else:
                pre_filtered_tickers = market_df.index.tolist()
                logger.info(f"ğŸ“Š market_df ê¸°ì¤€ í‹°ì»¤: {len(pre_filtered_tickers)}ê°œ")
            
            # === 2ë‹¨ê³„: ì‚¬ì „ í•„í„°ë§ëœ í‹°ì»¤ë“¤ì˜ í™œì„± ìƒíƒœë§Œ ê²€ì¦ ===
            validated_tickers = self._validate_active_status_only(pre_filtered_tickers)
            logger.info(f"ğŸ“Š í™œì„± ìƒíƒœ ê²€ì¦ í›„: {len(validated_tickers)}ê°œ í‹°ì»¤")
            
            # === ìë™ ë™ê¸°í™” íŠ¸ë¦¬ê±° ì¡°ê±´ ê²€ì‚¬ ===
            if len(validated_tickers) < len(pre_filtered_tickers) * 0.7:  # 30% ì´ìƒ ì†ì‹¤ì‹œ
                loss_rate = (len(pre_filtered_tickers) - len(validated_tickers)) / len(pre_filtered_tickers)
                logger.warning(f"ğŸ“‰ í•„í„°ë§ íš¨ìœ¨ ì €í•˜ ê°ì§€: {loss_rate:.1%} ì†ì‹¤ (ì„ê³„ì¹˜: 30%)")
                logger.warning("ğŸ”„ ìë™ ë™ê¸°í™” íŠ¸ë¦¬ê±° ì¡°ê±´ ì¶©ì¡±, ë™ê¸°í™” ì‹œë„...")
                
                sync_result = self._auto_sync_blacklist_with_is_active()
                if sync_result:
                    logger.info("âœ… ìë™ ë™ê¸°í™” ì„±ê³µ, ì¬ê²€ì¦ ìˆ˜í–‰")
                    # ë™ê¸°í™” í›„ ì¬ê²€ì¦
                    revalidated_tickers = self._validate_active_status_only(pre_filtered_tickers)
                    
                    # ê°œì„  íš¨ê³¼ í™•ì¸
                    improvement = len(revalidated_tickers) - len(validated_tickers)
                    if improvement > 0:
                        logger.info(f"ğŸ“ˆ ë™ê¸°í™” íš¨ê³¼: {improvement}ê°œ í‹°ì»¤ ì¶”ê°€ í™•ë³´ ({len(revalidated_tickers)}ê°œ)")
                        validated_tickers = revalidated_tickers
                    else:
                        logger.warning(f"âš ï¸ ë™ê¸°í™” í›„ì—ë„ ê°œì„  ì—†ìŒ: {len(revalidated_tickers)}ê°œ")
                else:
                    logger.error("âŒ ìë™ ë™ê¸°í™” ì‹¤íŒ¨, ê¸°ì¡´ ê²°ê³¼ ìœ ì§€")
            else:
                efficiency_rate = len(validated_tickers) / len(pre_filtered_tickers)
                logger.info(f"âœ… í•„í„°ë§ íš¨ìœ¨ ì–‘í˜¸: {efficiency_rate:.1%} (ì„ê³„ì¹˜: 70%)")
            
            # === 3ë‹¨ê³„: ê²€ì¦ëœ í‹°ì»¤ë“¤ë¡œë§Œ í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ìˆ˜í–‰ ===
            if not validated_tickers:
                logger.warning("âš ï¸ í™œì„± ìƒíƒœ ê²€ì¦ í†µê³¼ í‹°ì»¤ ì—†ìŒ")
                return pd.DataFrame()

            # ì„±ëŠ¥ ìµœì í™”ëœ ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì •ì +ë™ì  ì§€í‘œ ì¡°íšŒ (ê²€ì¦ëœ í‹°ì»¤ë§Œ ëŒ€ìƒ)
            hybrid_query = """
                SELECT 
                    s.ticker, s.price, s.high_60, s.low_60, s.resistance, s.support, 
                    s.atr, s.adx, s.updated_at,
                    o.rsi_14, o.macd_histogram, o.bb_upper, o.bb_lower, 
                    o.volume_20ma, o.stoch_k, o.current_close, o.ma_50, o.ma_200
                FROM static_indicators s
                LEFT JOIN (
                    SELECT DISTINCT ON (ticker) 
                           ticker, rsi_14, macd_histogram, bb_upper, bb_lower,
                           volume_20ma, stoch_k, close as current_close, ma_50, ma_200
                    FROM ohlcv 
                    ORDER BY ticker, date DESC
                ) o ON s.ticker = o.ticker
                WHERE s.ticker = ANY(%s)
            """
            
            hybrid_result = self.db_mgr.execute_query(hybrid_query, (validated_tickers,))
            
            if not hybrid_result:
                logger.warning("âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨")
                return pd.DataFrame()
            
            # í•˜ì´ë¸Œë¦¬ë“œ DataFrame ìƒì„± (ë‹¨ì¼ ì¿¼ë¦¬ ê²°ê³¼)
            combined_df = pd.DataFrame(hybrid_result, columns=[
                'ticker', 'price', 'high_60', 'low_60', 'resistance', 'support', 
                'atr', 'adx', 'updated_at',
                'rsi_14', 'macd_histogram', 'bb_upper', 'bb_lower', 
                'volume_20ma', 'stoch_k', 'current_close', 'ma_50', 'ma_200'
            ])
            combined_df.set_index('ticker', inplace=True)
            
            # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ (ì •ì /ë™ì  ë¶„ë¦¬í•˜ì—¬ ê²€ì¦)
            static_columns = ['price', 'high_60', 'low_60', 'resistance', 'support', 'atr', 'adx', 'updated_at']
            dynamic_columns = ['rsi_14', 'macd_histogram', 'bb_upper', 'bb_lower', 'volume_20ma', 'stoch_k', 'current_close', 'ma_50', 'ma_200']
            
            static_df = combined_df[static_columns].copy()
            dynamic_df = combined_df[dynamic_columns].copy()
            
            # ë™ì  ë°ì´í„°ê°€ ìˆëŠ” í‹°ì»¤ë§Œ ì¶”ì¶œ (NULLì´ ì•„ë‹Œ ê²½ìš°)
            has_dynamic_data = combined_df[dynamic_columns].notna().any(axis=1)
            dynamic_df = dynamic_df[has_dynamic_data]
            
            from filter_tickers import validate_data_consistency
            validation_result = validate_data_consistency(static_df, dynamic_df)
            
            if not validation_result['is_valid']:
                logger.error("âŒ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨, í•„í„°ë§ ì¤‘ë‹¨")
                return pd.DataFrame()
            
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(combined_df)}ê°œ ì¢…ëª© (ê²€ì¦ëœ í‹°ì»¤ë¡œë§Œ ì¿¼ë¦¬)")
            logger.info(f"ğŸ“Š ë™ì  ë°ì´í„° ë³´ìœ : {has_dynamic_data.sum()}ê°œ ì¢…ëª© ({has_dynamic_data.sum()/len(combined_df)*100:.1f}%)")

            # í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì ìš©
            from filter_tickers import filter_comprehensive_candidates
            filtered_df = filter_comprehensive_candidates(combined_df, config)

            # ê²°ê³¼ ë¡œê¹…
            if filtered_df.empty:
                logger.warning("âš ï¸ ëŒíŒŒ ë§¤ë§¤ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            logger.info(f"âœ… {len(filtered_df)} breakout candidates selected out of {len(combined_df)}")
            logger.info(f"ğŸ“Š ì„ ë³„ëœ ëŒíŒŒ í›„ë³´: {', '.join(filtered_df.index.tolist())}")
            
            # ìƒì„¸ ì •ë³´ ë””ë²„ê·¸ ë¡œê¹…
            if logger.isEnabledFor(logging.DEBUG):
                try:
                    debug_cols = ['price', 'ma_200', 'high_60', 'optional_score']
                    available_cols = [col for col in debug_cols if col in filtered_df.columns]
                    if available_cols:
                        logger.debug(f"ëŒíŒŒ í›„ë³´ ìƒì„¸ ì •ë³´:\n{filtered_df[available_cols]}")
                    
                    # ë³´ì¡° ì¡°ê±´ ìƒì„¸ ì •ë³´
                    if 'optional_details' in filtered_df.columns:
                        for ticker in filtered_df.index:
                            details = filtered_df.loc[ticker, 'optional_details']
                            score = filtered_df.loc[ticker, 'optional_score']
                            logger.debug(f"âœ¨ {ticker}: ì ìˆ˜ {score}, ì¡°ê±´ [{details}]")
                except Exception as e:
                    logger.debug(f"ìƒì„¸ ì •ë³´ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")

            logger.info(f"ğŸ¯ ëŒíŒŒ ë§¤ë§¤ í•„í„°ë§ ì™„ë£Œ: {len(filtered_df)}ê°œ ì¢…ëª© ì„ ë³„")
            
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ í†µí•©
            try:
                import time
                end_time = time.time()
                processing_time = getattr(self, '_filter_start_time', end_time) and (end_time - self._filter_start_time) or 0
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘
                session_metrics = {
                    'total_tickers': len(pre_filtered_tickers),
                    'validated_tickers': len(validated_tickers),
                    'filtered_tickers': len(filtered_df),
                    'processing_time': processing_time,
                    'hybrid_mode_count': has_dynamic_data.sum() if 'has_dynamic_data' in locals() else 0,
                    'static_only_count': len(combined_df) - (has_dynamic_data.sum() if 'has_dynamic_data' in locals() else 0),
                    'data_quality_score': validation_result.get('quality_score', 1.0) if 'validation_result' in locals() else 1.0,
                    'static_weight': config.get('static_weight', 0.6),
                    'dynamic_weight': config.get('dynamic_weight', 0.4),
                    'filter_config': config,
                    'error_count': 0,
                    'session_id': f"filter_{safe_strftime(datetime.now(), '%Y%m%d_%H%M%S')}"
                }
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
                from performance_monitor import get_performance_monitor
                monitor = get_performance_monitor()
                performance_result = monitor.record_filtering_session(session_metrics)
                
                # ì•Œë¦¼ ì‹œìŠ¤í…œ ê²€ì‚¬
                from alert_system import get_alert_system
                alert_system = get_alert_system()
                alerts = alert_system.check_and_send_alerts(session_metrics)
                
                if alerts:
                    logger.info(f"ğŸ“¢ {len(alerts)}ê°œ ì•Œë¦¼ ë°œì†¡ë¨")
                    
            except Exception as monitor_error:
                logger.warning(f"âš ï¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {monitor_error}")
            
            return filtered_df

        except Exception as e:
            logger.error(f"âŒ ëŒíŒŒ ë§¤ë§¤ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì•Œë¦¼ ì‹œìŠ¤í…œì— ì˜¤ë¥˜ ìƒí™© ì „ë‹¬
            try:
                from alert_system import get_alert_system
                alert_system = get_alert_system()
                error_metrics = {
                    'total_tickers': len(pre_filtered_tickers) if 'pre_filtered_tickers' in locals() else 0,
                    'validated_tickers': 0,
                    'filtered_tickers': 0,
                    'processing_time': 0,
                    'data_quality_score': 0.0,
                    'efficiency': 0.0,
                    'error_count': 1,
                    'session_id': f"error_{safe_strftime(datetime.now(), '%Y%m%d_%H%M%S')}"
                }
                alert_system.check_and_send_alerts(error_metrics)
            except:
                pass  # ì•Œë¦¼ ì‹œìŠ¤í…œ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
                
            return pd.DataFrame()

    def process_4h_for_candidates(self, candidates_1d): # 4ì‹œê°„ë´‰ í•„í„°ë§ì„ í†µí•œ ë§ˆì¼“íƒ€ì´ë°
        """
        1ì°¨ í•„í„°ë§ í†µê³¼ ì¢…ëª©ë“¤ì— ëŒ€í•´ 4ì‹œê°„ë´‰ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë§ˆì¼“íƒ€ì´ë° í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            candidates_1d (list): ì¼ë´‰ ê¸°ì¤€ í•„í„°ë§ í†µê³¼ ì¢…ëª© ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: 4ì‹œê°„ë´‰ í•„í„°ë§ê¹Œì§€ í†µê³¼í•œ ìµœì¢… ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘
            logger.info("ğŸ“Š 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ë° ë§ˆì¼“íƒ€ì´ë° í•„í„°ë§ ì‹œì‘")
            
            # ì…ë ¥ ë°ì´í„° ê²€ì¦ ê°•í™”
            if not candidates_1d:
                logger.warning("âš ï¸ 1ì°¨ í•„í„°ë§ í†µê³¼ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì…ë ¥ ë°ì´í„° íƒ€ì… ê²€ì¦
            if not isinstance(candidates_1d, list):
                logger.error(f"âŒ candidates_1dëŠ” list íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ íƒ€ì…: {type(candidates_1d)}")
                return []
            
            # í‹°ì»¤ í˜•ì‹ ê²€ì¦
            valid_tickers = []
            for ticker in candidates_1d:
                if isinstance(ticker, str) and ticker.startswith('KRW-'):
                    valid_tickers.append(ticker)
                else:
                    logger.warning(f"âš ï¸ ì˜ëª»ëœ í‹°ì»¤ í˜•ì‹: {ticker} (íƒ€ì…: {type(ticker)})")
            
            if not valid_tickers:
                logger.error("âŒ ìœ íš¨í•œ í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            candidates_1d = valid_tickers
            logger.info(f"ğŸ“‹ ìœ íš¨í•œ í‹°ì»¤ {len(candidates_1d)}ê°œ í™•ì¸ë¨")
            
            # 1. 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘
            try:
                from data_fetcher import get_ohlcv_4h, calculate_technical_indicators_4h, save_market_data_4h_to_db
            except ImportError as e:
                logger.error(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                return []
            
            market_data_4h = {}
            processing_errors = []
            
            logger.info(f"ğŸ” 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ëŒ€ìƒ: {len(candidates_1d)}ê°œ ì¢…ëª©")
            
            for i, ticker in enumerate(candidates_1d):
                try:
                    # í‹°ì»¤ë³„ ì²˜ë¦¬ ì‹œì‘
                    logger.debug(f"ğŸ”„ [{i+1}/{len(candidates_1d)}] {ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
                    
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ë§¤ìˆ˜ í›„ë³´ê°€ ì„ ì •ëœ ì¢…ëª©ì€ ê°•ì œë¡œ ë°ì´í„° ìˆ˜ì§‘
                    df_4h = get_ohlcv_4h(ticker, limit=250, force_fetch=True)
                    
                    if df_4h is None or df_4h.empty:
                        logger.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ê°•ì œ ìˆ˜ì§‘ ì‹œë„ í›„)")
                        processing_errors.append(f"{ticker}: ê°•ì œ ìˆ˜ì§‘ ì‹¤íŒ¨")
                        continue
                    
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê°•í™”
                    min_required_data = 50  # MA200 ê³„ì‚°ì„ ìœ„í•´ ìµœì†Œ 50ê°œ í•„ìš”
                    if len(df_4h) < min_required_data:
                        logger.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ë¶€ì¡± ({len(df_4h)}ê°œ < {min_required_data}ê°œ)")
                        processing_errors.append(f"{ticker}: ë°ì´í„° ë¶€ì¡±")
                        continue
                    
                    # 2. 4ì‹œê°„ë´‰ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                    df_4h_with_indicators = calculate_technical_indicators_4h(df_4h)
                    
                    if df_4h_with_indicators is None or df_4h_with_indicators.empty:
                        logger.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                        processing_errors.append(f"{ticker}: ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                        continue
                    
                    # ì§€í‘œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                    required_indicators = ['rsi_14', 'macd', 'bb_upper', 'bb_lower']
                    missing_indicators = [ind for ind in required_indicators if ind not in df_4h_with_indicators.columns]
                    
                    if missing_indicators:
                        logger.warning(f"âš ï¸ {ticker} í•„ìˆ˜ ì§€í‘œ ëˆ„ë½: {missing_indicators}")
                        # ëˆ„ë½ëœ ì§€í‘œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (ê²½ê³ ë§Œ)
                    
                    # 3. 4ì‹œê°„ë´‰ OHLCV ë°ì´í„°ë¥¼ DBì— ì €ì¥
                    try:
                        from data_fetcher import save_ohlcv_4h_to_db
                        save_ohlcv_4h_to_db(ticker, df_4h)
                        logger.debug(f"ğŸ’¾ {ticker} 4ì‹œê°„ë´‰ OHLCV DB ì €ì¥ ì™„ë£Œ")
                    except Exception as save_e:
                        logger.error(f"âŒ {ticker} 4ì‹œê°„ë´‰ OHLCV DB ì €ì¥ ì‹¤íŒ¨: {save_e}")
                        # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ë©”ëª¨ë¦¬ì—ëŠ” ì €ì¥í•˜ì—¬ ë¶„ì„ ì§„í–‰
                    
                    # 4. 4ì‹œê°„ë´‰ ë§ˆì¼“íƒ€ì´ë° ì§€í‘œë¥¼ DBì— ì €ì¥
                    try:
                        save_market_data_4h_to_db(ticker, df_4h_with_indicators)
                        logger.debug(f"ğŸ’¾ {ticker} 4ì‹œê°„ë´‰ ë§ˆì¼“íƒ€ì´ë° ì§€í‘œ DB ì €ì¥ ì™„ë£Œ")
                    except Exception as save_e:
                        logger.error(f"âŒ {ticker} 4ì‹œê°„ë´‰ ë§ˆì¼“íƒ€ì´ë° ì§€í‘œ DB ì €ì¥ ì‹¤íŒ¨: {save_e}")
                        # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ë©”ëª¨ë¦¬ì—ëŠ” ì €ì¥í•˜ì—¬ ë¶„ì„ ì§„í–‰
                    
                    # 5. ë©”ëª¨ë¦¬ì— ì €ì¥ (ì´í›„ í•„í„°ë§ì— ì‚¬ìš©)
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ì¤‘ë³µ ì €ì¥ ë°©ì§€
                    if ticker not in market_data_4h:
                        market_data_4h[ticker] = df_4h_with_indicators
                        logger.debug(f"ğŸ’¾ {ticker} 4ì‹œê°„ë´‰ ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ")
                    else:
                        logger.warning(f"âš ï¸ {ticker} ì´ë¯¸ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ ìˆìŒ (ì¤‘ë³µ ë°©ì§€)")
                    
                    logger.info(f"âœ… [{i+1}/{len(candidates_1d)}] {ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ({len(df_4h_with_indicators)}ê°œ ë ˆì½”ë“œ)")
                    
                except Exception as e:
                    error_msg = f"{ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                    logger.error(f"âŒ {error_msg}")
                    processing_errors.append(error_msg)
                    
                    # ê°œë³„ í‹°ì»¤ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ë””ë²„ê¹… ì •ë³´
                    import traceback
                    logger.debug(f"ğŸ” {ticker} ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
                    continue
            
            # 5. 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸
            success_count = len(market_data_4h)
            total_count = len(candidates_1d)
            success_rate = (success_count / total_count * 100) if total_count > 0 else 0
            
            logger.info(f"ğŸ“Š 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_count} ì¢…ëª© ì„±ê³µ ({success_rate:.1f}%)")
            
            if processing_errors:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {len(processing_errors)}ê±´")
                for error in processing_errors[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    logger.warning(f"   - {error}")
                if len(processing_errors) > 5:
                    logger.warning(f"   - ... ì™¸ {len(processing_errors) - 5}ê±´ ë”")
            
            if not market_data_4h:
                logger.warning("âš ï¸ 4ì‹œê°„ë´‰ ë°ì´í„°ê°€ ëª¨ë‘ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return []
            
            # 6. 4ì‹œê°„ë´‰ íƒ€ì´ë° í•„í„°ë§ ì ìš©
            try:
                from filter_tickers import apply_timing_filter_4h
                
                timing_filter_config = self.config.get('timing_filter', {})
                if not timing_filter_config:
                    logger.warning("âš ï¸ íƒ€ì´ë° í•„í„° ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
                    timing_filter_config = {'enabled': True}
                
                # dictë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                if market_data_4h and isinstance(market_data_4h, dict):
                    # ê° í‹°ì»¤ì˜ ìµœì‹  ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ DataFrame ìƒì„±
                    latest_data = {}
                    for ticker, df in market_data_4h.items():
                        if df is not None and not df.empty:
                            latest_data[ticker] = df.iloc[-1]
                    
                    if latest_data:
                        market_df_4h = pd.DataFrame(latest_data).T  # Transpose to get tickers as index
                        final_candidates = apply_timing_filter_4h(market_df_4h, timing_filter_config)
                        
                    else:
                        logger.warning("âš ï¸ 4ì‹œê°„ë´‰ ìµœì‹  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        final_candidates = []
                else:
                    logger.warning("âš ï¸ 4ì‹œê°„ë´‰ ë°ì´í„°ê°€ dict í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")
                    final_candidates = []
                
            except ImportError as e:
                logger.error(f"âŒ íƒ€ì´ë° í•„í„° ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
                # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì„±ê³µ ì¢…ëª© ë°˜í™˜
                final_candidates = list(market_data_4h.keys())
                logger.warning(f"âš ï¸ íƒ€ì´ë° í•„í„°ë§ ìš°íšŒ, ì²˜ë¦¬ ì„±ê³µ ì¢…ëª© {len(final_candidates)}ê°œ ë°˜í™˜")
                
            except Exception as e:
                logger.error(f"âŒ íƒ€ì´ë° í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ìƒìœ„ 3ê°œ ì¢…ëª©ë§Œ ë°˜í™˜
                final_candidates = list(market_data_4h.keys())[:3]
                logger.warning(f"âš ï¸ íƒ€ì´ë° í•„í„°ë§ ì‹¤íŒ¨, ìƒìœ„ {len(final_candidates)}ê°œ ì¢…ëª© ë°˜í™˜")
            
            # 7. ê²°ê³¼ ë¡œê¹…
            logger.info(f"ğŸ“Š 4ì‹œê°„ë´‰ ë§ˆì¼“íƒ€ì´ë° í•„í„°ë§ ê²°ê³¼:")
            logger.info(f"   - ì…ë ¥: {len(candidates_1d)}ê°œ ì¢…ëª©")
            logger.info(f"   - ë°ì´í„° ì²˜ë¦¬ ì„±ê³µ: {len(market_data_4h)}ê°œ ì¢…ëª©")
            logger.info(f"   - ìµœì¢… í†µê³¼: {len(final_candidates)}ê°œ ì¢…ëª©")
            if final_candidates:
                logger.info(f"   - í†µê³¼ ì¢…ëª©: {final_candidates}")
            else:
                logger.warning("âš ï¸ 4ì‹œê°„ë´‰ í•„í„°ë§ í†µê³¼ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # 8. 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ (í•„í„°ë§ ì™„ë£Œ í›„)
            # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] í•„í„°ë§ ì™„ë£Œ í›„ì—ë§Œ ë°ì´í„° ì •ë¦¬
            if final_candidates:
                try:
                    logger.info("ğŸ§¹ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì‹œì‘ (í•„í„°ë§ ì„±ê³µ)")
                    self._cleanup_4h_data()
                    logger.info("âœ… 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
                except Exception as cleanup_error:
                    logger.error(f"âŒ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {cleanup_error}")
            else:
                logger.info("ğŸ“Š í•„í„°ë§ ê²°ê³¼ê°€ ì—†ì–´ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            return final_candidates
            
        except Exception as e:
            logger.error(f"âŒ 4ì‹œê°„ë´‰ ë¶„ì„ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            
            # ì „ì²´ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []

    def _cleanup_4h_data(self):
        """
        4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ í›„ ohlcv_4hì™€ market_data_4h í…Œì´ë¸”ì˜ ëª¨ë“  ë ˆì½”ë“œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        ë©”ëª¨ë¦¬ ìµœì í™”ì™€ ë°ì´í„° ì¼ê´€ì„±ì„ ìœ„í•´ í•„í„°ë§ ì™„ë£Œ í›„ ì¦‰ì‹œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        try:
            logger.info("ğŸ§¹ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì‹œì‘")
            
            # DB ì—°ê²°
            with self.db_mgr.get_connection() as conn:
                with conn.cursor() as cursor:
                    
                    # 1. ohlcv_4h í…Œì´ë¸” ì •ë¦¬
                    cursor.execute("DELETE FROM ohlcv_4h")
                    ohlcv_deleted_count = cursor.rowcount
                    
                    # 2. market_data_4h í…Œì´ë¸” ì •ë¦¬
                    cursor.execute("DELETE FROM market_data_4h")
                    market_data_deleted_count = cursor.rowcount
                    
                    # íŠ¸ëœì­ì…˜ ì»¤ë°‹
                    conn.commit()
                    
                    logger.info(f"âœ… 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ:")
                    logger.info(f"   - ohlcv_4h: {ohlcv_deleted_count}ê°œ ë ˆì½”ë“œ ì‚­ì œ")
                    logger.info(f"   - market_data_4h: {market_data_deleted_count}ê°œ ë ˆì½”ë“œ ì‚­ì œ")
                    logger.info(f"   - ì´ ì‚­ì œ: {ohlcv_deleted_count + market_data_deleted_count}ê°œ ë ˆì½”ë“œ")
                    
        except Exception as e:
            logger.error(f"âŒ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            logger.error(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            raise

    def scan_and_filter_tickers(self) -> list:
        """
        DBì—ì„œ í‹°ì»¤ ë¡œë“œ, ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ìš©, ì›”ë´‰ ë°ì´í„° ê¸¸ì´ í•„í„°, ê±°ë˜ëŒ€ê¸ˆ í•„í„°, ì›”ë´‰ íŒ¨í„´ í•„í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì ìš©í•©ë‹ˆë‹¤.
        Returns:
            list: í•„í„°ë§ëœ í‹°ì»¤ ëª©ë¡
        """
        try:
            logger.info("ğŸ” í‹°ì»¤ ìŠ¤ìº” ë° í•„í„°ë§ ì‹œì‘")
            start_time = time.time()

            # DBì—ì„œ ëª¨ë“  í‹°ì»¤ ì¡°íšŒ
            tickers_rows = self.db_mgr.execute_query("SELECT ticker FROM tickers")
            if not tickers_rows:
                logger.warning("âš ï¸ DBì—ì„œ í‹°ì»¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            # í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            all_tickers = [row[0] for row in tickers_rows]
            logger.info(f"ğŸ“Š DBì—ì„œ {len(all_tickers)}ê°œ í‹°ì»¤ ë¡œë“œë¨ (ì†Œìš”ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ)")

            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ìš©
            blacklist = load_blacklist()
            filtered_tickers = [ticker for ticker in all_tickers if ticker not in blacklist]
            logger.info(f"ğŸš« ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ìš© í›„ {len(filtered_tickers)}ê°œ í‹°ì»¤ ë‚¨ìŒ (ì œì™¸: {len(all_tickers) - len(filtered_tickers)}ê°œ)")

            # í•„í„° ìˆœì„œ: 1) ì›”ë´‰ ë°ì´í„° ê¸¸ì´, 2) ê±°ë˜ëŒ€ê¸ˆ 
            filter_start = time.time()
            from filter_tickers import filter_by_monthly_data_length, filter_by_volume

            # 1. ì›”ë´‰ ë°ì´í„° ê¸¸ì´ í•„í„° (ìµœì†Œ 14ê°œì›”)
            monthly_length_filtered = filter_by_monthly_data_length(filtered_tickers, min_months=14)
            logger.info(f"ğŸ“… ì›”ë´‰ ë°ì´í„° ê¸¸ì´(14ê°œì›”) í•„í„° ì ìš© í›„ {len(monthly_length_filtered)}ê°œ í‹°ì»¤ ë‚¨ìŒ (ì†Œìš”ì‹œê°„: {time.time() - filter_start:.2f}ì´ˆ)")

            # 2. ê±°ë˜ëŒ€ê¸ˆ í•„í„°
            filter_vol_start = time.time()
            volume_filtered = filter_by_volume(monthly_length_filtered)
            logger.info(f"ğŸ’° ê±°ë˜ëŒ€ê¸ˆ í•„í„° ì ìš© í›„ {len(volume_filtered)}ê°œ í‹°ì»¤ ë‚¨ìŒ (ì†Œìš”ì‹œê°„: {time.time() - filter_vol_start:.2f}ì´ˆ)")

            # ê²°ê³¼ ìš”ì•½
            total_time = time.time() - start_time
            logger.info(f"âœ… í‹°ì»¤ ìŠ¤ìº” ë° í•„í„°ë§ ì™„ë£Œ: ì´ {len(all_tickers)}ê°œ ì¤‘ {len(volume_filtered)}ê°œ ì„ íƒ (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            if len(volume_filtered) < 5:
                logger.warning(f"âš ï¸ í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(volume_filtered)}ê°œ). í•„í„° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return volume_filtered
        except Exception as e:
            logger.error(f"âŒ í‹°ì»¤ ìŠ¤ìº” ë° í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []

    def process_daily_ohlcv_and_indicators(self, tickers: list) -> pd.DataFrame:
        """
        í‹°ì»¤ë³„ OHLCV ìˆ˜ì§‘, ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°, ì°¨íŠ¸ ì´ë¯¸ì§€ ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        Args:
            tickers (list): ì²˜ë¦¬í•  í‹°ì»¤ ëª©ë¡
        Returns:
            pd.DataFrame: ìµœì‹  ì§€í‘œê°€ í¬í•¨ëœ DataFrame
        """
        try:
            import pandas as pd
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import time

            start_time = time.time()

            if not tickers:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  í‹°ì»¤ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

            logger.info(f"ğŸ“ˆ {len(tickers)}ê°œ í‹°ì»¤ì˜ ì¼ë´‰ OHLCV ë° ì§€í‘œ ì²˜ë¦¬ ì‹œì‘")

            # ê²°ê³¼ë¥¼ ì €ì¥í•  DataFrameê³¼ ì˜¤ë¥˜ ì¶”ì 
            result_df = pd.DataFrame()
            errors = []
            successful_tickers = []

            # ì§‘í•©í™”í•˜ì—¬ ë¹ ë¥¸ í¬í•¨ ì²´í¬ ì§€ì›
            tickers_set = set(tickers)

            # [ìˆ˜ì •] ìˆœì°¨ ì²˜ë¦¬ì—ì„œ í†µí•© ì²˜ë¦¬ë¡œ ë³€ê²½
            logger.info("ğŸ”§ í†µí•© ì²˜ë¦¬: OHLCV ìˆ˜ì§‘ â†’ ì§€í‘œ ê³„ì‚° â†’ ì €ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬")

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
            def process_ticker(ticker):
                ticker_start = time.time()
                try:
                    # í•„í„° í†µê³¼ í‹°ì»¤ë§Œ ë°ì´í„° ì—…ë°ì´íŠ¸
                    if ticker not in tickers_set:
                        logger.info(f"â­ï¸ {ticker}ëŠ” í•„í„° í†µê³¼ í‹°ì»¤ ëª©ë¡ì— ì—†ìŒ. ì²˜ë¦¬ ê±´ë„ˆëœ€.")
                        return None, ticker

                    # 1. OHLCV ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ update_all_tickers_ohlcv ë¡œì§ í†µí•©)
                    from datetime import date
                    from data_fetcher import get_ohlcv_d, delete_old_ohlcv, save_all_indicators_atomically

                    # ìµœì‹  ë‚ ì§œ í™•ì¸
                    try:
                        row = self.db_mgr.execute_query(
                            "SELECT MAX(date) FROM ohlcv WHERE ticker = %s", (ticker,), fetchone=True
                        )
                        last_date = row[0] if row and row[0] else None
                    except Exception as e:
                        logger.error(f"âŒ {ticker} ê°€ì¥ ìµœì‹  OHLCV ë‚ ì§œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                        last_date = None

                    today = date.today()
                    
                    # ğŸš€ [í•µì‹¬ ìˆ˜ì •] ë°ì´í„° ìˆ˜ì§‘ ë° ìµœì‹  ë°ì´í„° ê°•ì œ ê°±ì‹  ë¡œì§
                    if last_date is None:
                        # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
                        df = get_ohlcv_d(ticker)
                        logger.info(f"ğŸ”„ {ticker} ì „ì²´ OHLCV ë°ì´í„° ìˆ˜ì§‘: {len(df) if df is not None else 0}ê°œ")
                    else:
                        days_diff = (today - last_date).days
                        if days_diff <= 0:
                            logger.info(f"âœ… {ticker} DB ìµœì‹  ìƒíƒœì´ì§€ë§Œ ì˜¤ëŠ˜ ë°ì´í„° ì‹¤ì‹œê°„ ê°±ì‹  í™•ì¸")
                            
                            # ğŸš€ í•µì‹¬ ìˆ˜ì •: ì˜¤ëŠ˜ ë°ì´í„°ëŠ” í•­ìƒ APIì—ì„œ ìµœì‹ ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                            try:
                                from data_fetcher import get_ohlcv_d
                                today_df = get_ohlcv_d(ticker, interval='day', count=1, fetch_latest_only=True)
                                
                                if today_df is not None and not today_df.empty:
                                    # ê¸°ì¡´ DB ë°ì´í„° + ìµœì‹  1ì¼ ë°ì´í„° ê²°í•©
                                    db_df = self.db_mgr.fetch_ohlcv(ticker, days=449)  # 449ì¼ + ì˜¤ëŠ˜ 1ì¼ = 450ì¼
                                    
                                    if not db_df.empty:
                                        # ì¤‘ë³µ ì œê±°í•˜ê³  ë³‘í•©
                                        import pandas as pd
                                        combined_df = pd.concat([db_df, today_df])
                                        combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                                        df = combined_df.sort_index()
                                        logger.info(f"ğŸ”„ {ticker} ìµœì‹  ë°ì´í„° ê°±ì‹  ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ (DB: {len(db_df)}, ìµœì‹ : {len(today_df)})")
                                    else:
                                        df = today_df
                                        logger.info(f"ğŸ”„ {ticker} ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì‹œì‘: {len(df)}ê°œ ë ˆì½”ë“œ")
                                else:
                                    # ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                                    df = self.db_mgr.fetch_ohlcv(ticker, days=450)
                                    logger.warning(f"âš ï¸ {ticker} ìµœì‹  ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©: {len(df) if df is not None else 0}ê°œ")
                            except Exception as api_e:
                                # API ì˜¤ë¥˜ ì‹œ ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©
                                df = self.db_mgr.fetch_ohlcv(ticker, days=450)
                                logger.warning(f"âš ï¸ {ticker} API ì˜¤ë¥˜ë¡œ ê¸°ì¡´ DB ë°ì´í„° ì‚¬ìš©: {str(api_e)}")
                        else:
                            # ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                            df = get_ohlcv_d(ticker, interval='1d', count=days_diff)
                            logger.info(f"ğŸ”„ {ticker} ì¦ë¶„ OHLCV ë°ì´í„° ìˆ˜ì§‘: {len(df) if df is not None else 0}ê°œ")

                    if df is None or df.empty:
                        logger.warning(f"âš ï¸ {ticker} OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                        return None, ticker

                    # 2. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                    from data_fetcher import calculate_unified_indicators
                    indicators_df = calculate_unified_indicators(df, ticker)
                    if indicators_df is None or indicators_df.empty:
                        logger.warning(f"âš ï¸ {ticker} ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                        return None, ticker

                    # 3. [í•µì‹¬ ìˆ˜ì •] í†µí•© ì €ì¥: OHLCV + ì •ì /ë™ì  ì§€í‘œë¥¼ ì›ìì ìœ¼ë¡œ ì €ì¥
                    save_result = save_all_indicators_atomically(ticker, indicators_df, timeframe='1d')
                    
                    if not save_result:
                        logger.error(f"âŒ {ticker} í†µí•© ì €ì¥ ì‹¤íŒ¨")
                        return None, ticker

                    # 4. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
                    delete_old_ohlcv(ticker)

                    # 5. ìµœì‹  ë°ì´í„°ë§Œ ì¶”ì¶œ
                    latest_data = indicators_df.iloc[-1:].copy()
                    latest_data['ticker'] = ticker

                    ticker_duration = time.time() - ticker_start
                    logger.info(f"âœ… {ticker} í†µí•© ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {ticker_duration:.2f}ì´ˆ)")
                    return latest_data, ticker

                except Exception as e:
                    error_msg = f"âŒ {ticker} í†µí•© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
                    logger.error(error_msg)
                    return None, ticker

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ)
            max_workers = min(10, len(tickers))  # ìµœëŒ€ 10ê°œ ìŠ¤ë ˆë“œë¡œ ì œí•œ

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # ëª¨ë“  í‹°ì»¤ì— ëŒ€í•œ ì‘ì—… ì œì¶œ
                future_to_ticker = {executor.submit(process_ticker, ticker): ticker for ticker in tickers}

                # ì‘ì—… ì™„ë£Œ ì‹œ ê²°ê³¼ ì²˜ë¦¬
                for i, future in enumerate(as_completed(future_to_ticker), 1):
                    ticker = future_to_ticker[future]
                    try:
                        data, processed_ticker = future.result()
                        if data is not None:
                            result_df = pd.concat([result_df, data])
                            successful_tickers.append(processed_ticker)
                        else:
                            errors.append(processed_ticker)
                    except Exception as e:
                        logger.error(f"âŒ {ticker} ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                        errors.append(ticker)

                    # ì§„í–‰ìƒí™© ë¡œê¹… (10% ë‹¨ìœ„)
                    if i % max(1, len(tickers) // 10) == 0 or i == len(tickers):
                        progress = (i / len(tickers)) * 100
                        elapsed = time.time() - start_time
                        estimated_total = (elapsed / i) * len(tickers)
                        remaining = max(0, estimated_total - elapsed)
                        logger.info(f"â³ ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(tickers)}) - "
                                   f"ê²½ê³¼: {elapsed:.1f}ì´ˆ, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")

            if result_df.empty:
                logger.warning("âš ï¸ ëª¨ë“  í‹°ì»¤ ì²˜ë¦¬ ì‹¤íŒ¨")
                return pd.DataFrame()

            # ì¸ë±ìŠ¤ ì„¤ì •
            result_df.set_index('ticker', inplace=True)

            # Sequentially generate chart images for successful tickers (JSON ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
            if not self.use_json_instead_of_chart:
                for ticker in successful_tickers:
                    df = self.db_mgr.fetch_ohlcv(ticker, days=400)
                    self.generate_chart_image(ticker, df)

            # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
            total_duration = time.time() - start_time
            logger.info(f"âœ… í†µí•© OHLCV ë° ì§€í‘œ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_tickers)}/{len(tickers)} í‹°ì»¤ ì„±ê³µ "
                        f"(ì†Œìš”ì‹œê°„: {total_duration:.1f}ì´ˆ, í‰ê· : {total_duration/max(1, len(tickers)):.2f}ì´ˆ/í‹°ì»¤)")

            # ì˜¤ë¥˜ ëª©ë¡ ë¡œê¹…
            if errors:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨í•œ í‹°ì»¤ ({len(errors)}ê°œ): {errors[:20]}" +
                               (f" ì™¸ {len(errors)-20}ê°œ" if len(errors) > 20 else ""))

            return result_df

        except Exception as e:
            logger.error(f"âŒ í†µí•© OHLCV ë° ì§€í‘œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return pd.DataFrame()

    def analyze_4h_and_filter(self, candidates_1d: list) -> list:
        """
        4ì‹œê°„ë´‰ ì§€í‘œ ê³„ì‚° ë° í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ìµœì¢… í›„ë³´ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
        
        Args:
            candidates_1d (list): 1ì¼ë´‰ í•„í„°ë§ì„ í†µê³¼í•œ í›„ë³´ í‹°ì»¤ ëª©ë¡ [(í‹°ì»¤, ì ìˆ˜), ...] ë˜ëŠ” [í‹°ì»¤, ...]
        
        Returns:
            list: 4ì‹œê°„ë´‰ í•„í„°ë§ê¹Œì§€ í†µê³¼í•œ ìµœì¢… í›„ë³´ í‹°ì»¤ ëª©ë¡
        """
        try:
            if not candidates_1d:
                logger.warning("âš ï¸ 1ì¼ë´‰ í•„í„°ë§ì„ í†µê³¼í•œ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ì…ë ¥ í˜•íƒœì— ë”°ë¼ í‹°ì»¤ë§Œ ì¶”ì¶œ
            if isinstance(candidates_1d[0], tuple):
                # [(í‹°ì»¤, ì ìˆ˜), ...] í˜•íƒœ
                tickers_only = [ticker for ticker, _ in candidates_1d]
            else:
                # [í‹°ì»¤, ...] í˜•íƒœ
                tickers_only = candidates_1d
            
            logger.info(f"ğŸ” 4ì‹œê°„ë´‰ ë¶„ì„ ë° í•„í„°ë§ ì‹œì‘ (ëŒ€ìƒ: {len(tickers_only)}ê°œ í‹°ì»¤)")
            
            # 4ì‹œê°„ë´‰ ì²˜ë¦¬ ë° í•„í„°ë§ í•¨ìˆ˜ í˜¸ì¶œ
            final_candidates = self.process_4h_for_candidates(tickers_only)
            
            logger.info(f"âœ… 4ì‹œê°„ë´‰ ë¶„ì„ ë° í•„í„°ë§ ì™„ë£Œ: {len(final_candidates)}ê°œ í‹°ì»¤ ìµœì¢… ì„ ì •")
            if final_candidates:
                logger.info(f"ğŸ“Š ìµœì¢… ì„ ì • í‹°ì»¤: {final_candidates}")
            
            return final_candidates
            
        except Exception as e:
            logger.error(f"âŒ 4ì‹œê°„ë´‰ ë¶„ì„ ë° í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def trade_and_report(self, scored_tickers, market_df_updated, market_df_4h, gpt_json_data=None):
        """
        í•„í„°ë§, GPT ë¶„ì„, ë§¤ë§¤ ì‹¤í–‰, í¬íŠ¸í´ë¦¬ì˜¤ ê´€ë¦¬, ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            scored_tickers (list): ì ìˆ˜ê°€ ë§¤ê²¨ì§„ í‹°ì»¤ ëª©ë¡ [(í‹°ì»¤, ì ìˆ˜), ...]
            market_df_updated (pd.DataFrame): ì—…ë°ì´íŠ¸ëœ ë§ˆì¼“ ë°ì´í„°í”„ë ˆì„
            market_df_4h (pd.DataFrame): 4ì‹œê°„ë´‰ ë§ˆì¼“ ë°ì´í„°í”„ë ˆì„ (í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            
        Returns:
            bool: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        try:
            start_time = time.time()
            logger.info("ğŸ’¹ íŠ¸ë ˆì´ë”© ë° ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            
            # ìœ íš¨ì„± ê²€ì‚¬ - market_df_4hëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê²€ì‚¬ì—ì„œ ì œì™¸
            if not scored_tickers or market_df_updated is None or market_df_updated.empty:
                logger.warning("âš ï¸ íŠ¸ë ˆì´ë”© ë° ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return False
                
            # ì§„í–‰ ìƒí™© ì¶”ì 
            step_results = {
                "ëŒíŒŒ_í›„ë³´_ì¤€ë¹„": False,
                "4ì‹œê°„ë´‰_í•„í„°ë§": False,
                "GPT_ë¶„ì„_ë§¤ë§¤": False,
                "í¬íŠ¸í´ë¦¬ì˜¤_ì—…ë°ì´íŠ¸": False,
                "ë§¤ë„_ì¡°ê±´_ì ê²€": False,
                "ë°±í…ŒìŠ¤íŠ¸_ë¦¬í¬íŠ¸": False
            }
            
            # 1. ëŒíŒŒ í›„ë³´ ì¤€ë¹„ (ì´ë¯¸ í•„í„°ë§ ì™„ë£Œë¨)
            step_start = time.time()
            candidates_1d = scored_tickers  # ì´ë¯¸ 1ì°¨ í•„í„°ë§ì´ ì™„ë£Œëœ í›„ë³´ë“¤
            step_time = time.time() - step_start
            
            if not candidates_1d:
                logger.warning("âš ï¸ ëŒíŒŒ ë§¤ë§¤ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            logger.info(f"âœ… 1ë‹¨ê³„: ëŒíŒŒ í›„ë³´ ì¤€ë¹„ ì™„ë£Œ - {len(candidates_1d)}ê°œ í›„ë³´ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
            step_results["ëŒíŒŒ_í›„ë³´_ì¤€ë¹„"] = True
            
            # âœ… GPT ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (ì´ë¯¸ ë¶„ì„ ì™„ë£Œëœ ê²°ê³¼ ì‚¬ìš©)
            step_start_gpt = time.time()
            gpt_results = []  # GPT ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸
            trade_logs = []  # ë§¤ìˆ˜ ì´ë ¥ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸ (ì „ì—­ ì„ ì–¸)
            
            if gpt_json_data and len(gpt_json_data) > 0:
                logger.info("ğŸ”„ ê¸°ì¡´ GPT ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì¤‘")
                
                # ì´ë¯¸ GPT ë¶„ì„ì´ ì™„ë£Œëœ ê²°ê³¼ë¥¼ ì‚¬ìš© (ì¤‘ë³µ ë¶„ì„ ë°©ì§€)
                gpt_results = gpt_json_data
            else:
                logger.warning("âš ï¸ GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                # GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
                gpt_results = []
                for ticker, score in candidates_1d:
                    gpt_results.append({
                        "ticker": ticker,
                        "score": score,
                        "confidence": 0.5,
                        "action": "HOLD",
                        "market_phase": "Unknown",
                        "pattern": "",
                        "reason": "ê¸°ë³¸ê°’",
                        "input_type": "fallback"
                    })
            
            # candidates_1dë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•˜ìœ„ íŒŒì´í”„ë¼ì¸ í˜¸í™˜ì„± ìœ ì§€
            candidates_1d = [(result["ticker"], result["score"]) for result in gpt_results]
            
            # ì¤‘ë³µ ì œê±° (GPT ë¶„ì„ ê²°ê³¼ ì •ë¦¬)
            gpt_results_df = pd.DataFrame(gpt_results)
            gpt_results_df = gpt_results_df.drop_duplicates(subset='ticker')
            gpt_results = gpt_results_df.to_dict(orient='records')
            logger.info(f"[ì¤‘ë³µ ì œê±°] GPT ë¶„ì„ ê²°ê³¼ ì¤‘ë³µ ì œê±° í›„ í‹°ì»¤ ìˆ˜: {len(gpt_results)}")
            
            # GPT ë¶„ì„ ê²°ê³¼ë¥¼ score ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            sorted_results = sorted(gpt_results, key=lambda x: safe_float_convert(x.get("score", 0), context="GPTë¶„ì„ ì •ë ¬"), reverse=True)
            
            # ìƒìœ„ 5ê°œ ì¢…ëª© ë¡œê·¸ ì¶œë ¥
            logger.info("[GPT ë¶„ì„ ê²°ê³¼ ìƒìœ„ 5ê°œ]")
            for i, result in enumerate(sorted_results[:5], 1):
                logger.info(f"{i}. {result['ticker']}: {result['score']}ì  (confidence: {result['confidence']:.2f})")
            
            logger.info(f"âœ… GPT ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start_gpt:.2f}ì´ˆ)")
            
            # GPT ë¶„ì„ ê²°ê³¼ DB ì €ì¥ (ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ ì ìš©)
            try:
                self.save_gpt_analysis_to_db(gpt_results)
                
                # ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ì í†µê³„ ì—…ë°ì´íŠ¸
                if hasattr(self, 'gpt_lifecycle_manager') and self.gpt_lifecycle_manager:
                    cleanup_stats = self.gpt_lifecycle_manager.get_cleanup_stats()
                    logger.info(f"ğŸ“Š GPT ë¼ì´í”„ì‚¬ì´í´ í†µê³„: ì´ ì •ë¦¬ {cleanup_stats.get('total_cleaned', 0)}ê°œ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ GPT ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # GPT ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë§¤ìˆ˜ ì¡°ê±´ í•„í„°ë§ (ë§¤ìˆ˜ ì‹¤í–‰ ì œê±°)
            logger.info("ğŸ” ë§¤ìˆ˜ ì¡°ê±´ í•„í„°ë§ ì‹œì‘")
            buy_candidates = []
            excluded_candidates = []
            
            for result in gpt_results:
                try:
                    # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ê°•í™”
                    ticker = result.get("ticker", "")
                    if not ticker:
                        logger.warning(f"âš ï¸ í‹°ì»¤ ì •ë³´ ëˆ„ë½: {result}")
                        excluded_candidates.append(result)
                        continue
                        
                    score = safe_float_convert(result.get("score", 0), context=f"GPTë¶„ì„ {ticker} score")
                    confidence = safe_float_convert(result.get("confidence", 0), context=f"GPTë¶„ì„ {ticker} confidence")
                    
                    # ì ìˆ˜ì™€ ì‹ ë¢°ë„ ë²”ìœ„ ê²€ì¦
                    if not (0 <= score <= 100):
                        logger.warning(f"âš ï¸ {ticker} ì ìˆ˜ ë²”ìœ„ ì˜¤ë¥˜: {score} (0-100 ë²”ìœ„ ì´ˆê³¼)")
                        excluded_candidates.append(result)
                        continue
                        
                    if not (0 <= confidence <= 1):
                        logger.warning(f"âš ï¸ {ticker} ì‹ ë¢°ë„ ë²”ìœ„ ì˜¤ë¥˜: {confidence} (0-1 ë²”ìœ„ ì´ˆê³¼)")
                        excluded_candidates.append(result)
                        continue
                    
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] action í•„ë“œ íƒ€ì… ì•ˆì „ì„± ë³´ì¥
                    action_raw = result.get("action", "AVOID")
                    
                    # action í•„ë“œ íƒ€ì… ê²€ì¦ ë° ë³€í™˜
                    if isinstance(action_raw, (int, float)):
                        logger.warning(f"âš ï¸ {ticker} action í•„ë“œê°€ ìˆ«ì íƒ€ì…ì…ë‹ˆë‹¤: {action_raw} ({type(action_raw)}) â†’ 'HOLD'ë¡œ ë³€í™˜")
                        action = "HOLD"
                    elif isinstance(action_raw, str):
                        action = action_raw.upper().strip()
                    else:
                        logger.warning(f"âš ï¸ {ticker} action í•„ë“œê°€ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…ì…ë‹ˆë‹¤: {action_raw} ({type(action_raw)}) â†’ 'AVOID'ë¡œ ë³€í™˜")
                        action = "AVOID"
                    
                    # action ê°’ ìœ íš¨ì„± ê²€ì¦
                    valid_actions = ['BUY', 'STRONG_BUY', 'BUY_WEAK', 'SELL', 'STRONG_SELL', 'SELL_WEAK', 'HOLD', 'AVOID', 'NEUTRAL', 'WAIT']
                    if action not in valid_actions:
                        logger.warning(f"âš ï¸ {ticker} ìœ íš¨í•˜ì§€ ì•Šì€ action ê°’: {action} â†’ 'HOLD'ë¡œ ë³€í™˜")
                        action = "HOLD"
                    
                    # ì„¤ì • ê¸°ë°˜ ì—„ê²©í•œ ë§¤ìˆ˜ ì¡°ê±´ ì ìš©
                    try:
                        from config import GPT_FILTERING_CONFIG
                        strict_config = GPT_FILTERING_CONFIG['strict_mode']
                    except ImportError:
                        # fallback ì„¤ì •
                        strict_config = {
                            'min_score': 80,
                            'min_confidence': 0.9,
                            'allowed_actions': ['BUY', 'STRONG_BUY'],
                            'allowed_market_phases': ['Stage1', 'Stage2']
                        }
                    
                    if (score >= strict_config['min_score'] and 
                        confidence >= strict_config['min_confidence'] and 
                        action in strict_config['allowed_actions'] and 
                        result.get("market_phase", "") in strict_config['allowed_market_phases']):
                        buy_candidates.append(result)
                        logger.info(f"âœ… ë§¤ìˆ˜ í›„ë³´ ì„ ì •: {ticker} | ì ìˆ˜: {score} | ì‹ ë¢°ë„: {confidence:.2f} | ì•¡ì…˜: {action}")
                    else:
                        excluded_candidates.append(result)
                        logger.info(f"âŒ ì œì™¸ë¨: {ticker} | ì ìˆ˜: {score} | ì‹ ë¢°ë„: {confidence:.2f} | ì•¡ì…˜: {action}")
                        
                except (ValueError, TypeError) as e:
                    logger.error(f"âŒ ë°ì´í„° íƒ€ì… ì˜¤ë¥˜: {result.get('ticker', 'Unknown')} | ì˜¤ë¥˜: {str(e)}")
                    excluded_candidates.append(result)
            
            logger.info(f"âœ… ë§¤ìˆ˜ í›„ë³´ {len(buy_candidates)}ê°œ, ì œì™¸ëœ ì¢…ëª© {len(excluded_candidates)}ê°œ")
            
            # ìƒì„¸í•œ í•„í„°ë§ ê²°ê³¼ ë¡œê·¸
            if buy_candidates:
                logger.info("ğŸ¯ ìµœì¢… ë§¤ìˆ˜ í›„ë³´ ëª©ë¡:")
                for candidate in buy_candidates:
                    logger.info(f"   - {candidate['ticker']}: ì ìˆ˜ {candidate['score']}, ì‹ ë¢°ë„ {candidate['confidence']:.2f}, ì•¡ì…˜ {candidate.get('action', 'Unknown')}")
            else:
                logger.info("ğŸ“Š ì—„ê²©í•œ í•„í„°ë§ìœ¼ë¡œ ì¸í•´ ë§¤ìˆ˜ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            # 2. 4ì‹œê°„ë´‰ ë¶„ì„ ë° í•„í„°ë§ (ë§¤ìˆ˜ í›„ë³´ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰)
            step_start = time.time()
            
            # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ë§¤ìˆ˜ í›„ë³´ê°€ ì—†ìœ¼ë©´ 4ì‹œê°„ë´‰ ì²˜ë¦¬ ì™„ì „ ê±´ë„ˆë›°ê¸°
            if not buy_candidates:
                logger.info("ğŸ“Š ë§¤ìˆ˜ í›„ë³´ê°€ 0ê°œì´ë¯€ë¡œ 4ì‹œê°„ë´‰ ë¶„ì„ ë° í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                step_time = time.time() - step_start
                logger.info(f"âœ… 2ë‹¨ê³„: 4ì‹œê°„ë´‰ í•„í„°ë§ ê±´ë„ˆëœ€ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
                step_results["4ì‹œê°„ë´‰_í•„í„°ë§"] = True  # ê±´ë„ˆë›´ ê²ƒë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                
                # ë¹ˆ ê²°ê³¼ë¡œ í›„ì† ë‹¨ê³„ ì§„í–‰
                passed_4h = []
                final_candidates = []
                
                # 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ (ê±´ë„ˆë›°ë”ë¼ë„ ì •ë¦¬)
                try:
                    logger.info("ğŸ§¹ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì‹œì‘ (ë§¤ìˆ˜ í›„ë³´ ì—†ìŒ)")
                    self._cleanup_4h_data()
                    logger.info("âœ… 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
                except Exception as cleanup_error:
                    logger.error(f"âŒ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {cleanup_error}")
                
            else:
                # ë§¤ìˆ˜ í›„ë³´ê°€ ìˆì„ ë•Œë§Œ 4ì‹œê°„ë´‰ ì²˜ë¦¬ ì‹¤í–‰
                candidates_1d = [(result["ticker"], result["score"]) for result in buy_candidates]
                passed_4h = self.analyze_4h_and_filter(candidates_1d)
                step_time = time.time() - step_start
                
                if not passed_4h:
                    logger.warning("âš ï¸ 4ì‹œê°„ë´‰ í•„í„°ë§ì„ í†µê³¼í•œ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] í•„í„°ë§ ì‹¤íŒ¨ ì‹œì—ë„ ë°ì´í„° ì •ë¦¬ í›„ False ë°˜í™˜
                    try:
                        logger.info("ğŸ§¹ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì‹œì‘ (í•„í„°ë§ ì‹¤íŒ¨)")
                        self._cleanup_4h_data()
                        logger.info("âœ… 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
                    except Exception as cleanup_error:
                        logger.error(f"âŒ 4ì‹œê°„ë´‰ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {cleanup_error}")
                    
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] í•„í„°ë§ ì‹¤íŒ¨ ì‹œì—ë„ ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬
                    step_results["4ì‹œê°„ë´‰_í•„í„°ë§"] = True  # ì‹¤íŒ¨í•´ë„ ë‹¨ê³„ ì™„ë£Œë¡œ ê°„ì£¼
                    return True  # False ëŒ€ì‹  True ë°˜í™˜í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
                
                # GPT ì ìˆ˜ë¥¼ ìœ ì§€í•œ ì±„ êµì§‘í•© ì¶”ì¶œ
                final_candidates = [(t, s) for (t, s) in candidates_1d if t in passed_4h]
                
                logger.info(f"âœ… 2ë‹¨ê³„: 4ì‹œê°„ë´‰ í•„í„°ë§ ì™„ë£Œ - {len(final_candidates)}ê°œ í›„ë³´ ì„ ì • (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
                step_results["4ì‹œê°„ë´‰_í•„í„°ë§"] = True
                
            # 3. ìµœì¢… ë§¤ìˆ˜ ì‹¤í–‰ (4ì‹œê°„ë´‰ í•„í„°ë§ í†µê³¼í•œ ì¢…ëª©ë§Œ)
            step_start = time.time()
            trade_logs = []  # ë§¤ìˆ˜ ì´ë ¥ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸
            
            if final_candidates:
                logger.info("ğŸ’° ìµœì¢… ë§¤ìˆ˜ ì‹¤í–‰ ì‹œì‘")
                
                # ğŸ”§ [í•µì‹¬ ê°œì„ ] í˜„ì¬ ë³´ìœ  ì¢…ëª© ì¡°íšŒ ë° í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
                current_positions = self.pm.get_current_positions()
                current_tickers = {pos['ticker'] for pos in current_positions}
                
                logger.info(f"ğŸ“Š í˜„ì¬ ë³´ìœ  ì¢…ëª©: {len(current_positions)}ê°œ")
                if current_tickers:
                    logger.info(f"   - ë³´ìœ  ì¢…ëª©: {', '.join(current_tickers)}")
                
                # ğŸ”§ [3ë‹¨ê³„ ê°œì„ ] GPT ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹ ë¢°ë„ ì •ë³´ ì¶”ì¶œ
                gpt_confidence_map = {}
                if gpt_json_data:
                    for result in gpt_json_data:
                        ticker = result.get('ticker')
                        confidence = safe_float_convert(result.get('confidence', 0.5), context=f"GPTë¶„ì„ {ticker} confidence")
                        gpt_confidence_map[ticker] = confidence
                
                # ğŸ”§ [3ë‹¨ê³„ ê°œì„ ] í¬íŠ¸í´ë¦¬ì˜¤ ê¸°ë°˜ ë™ì  ë§¤ìˆ˜ ê¸ˆì•¡ ê³„ì‚°
                total_balance = self.pm.get_total_balance()
                base_amount = min(100000, total_balance * 0.02)  # ìµœëŒ€ 10ë§Œì› ë˜ëŠ” ì´ ìì‚°ì˜ 2%
                
                for ticker, score in final_candidates:
                    try:
                        # ğŸ”§ [í•µì‹¬ ê°œì„ ] ë³´ìœ  ì¢…ëª© í™•ì¸ ë° í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
                        if ticker in current_tickers:
                            logger.info(f"ğŸ”„ {ticker} ì´ë¯¸ ë³´ìœ  ì¤‘ - í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì‹œì‘")
                            
                            # í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
                            pyramid_result = self._check_pyramiding_for_existing_position(ticker, score, gpt_confidence_map.get(ticker, 0.5))
                            
                            logger.info(f"ğŸ” {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ê²°ê³¼: should_pyramid={pyramid_result['should_pyramid']}")
                            logger.info(f"ğŸ” {ticker} ì‚¬ìœ : {pyramid_result['reason']}")
                            
                            if pyramid_result['should_pyramid']:
                                logger.info(f"âœ… {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì¶©ì¡± - ì¶”ê°€ ë§¤ìˆ˜ ì§„í–‰")
                                
                                # ğŸ”§ [ë””ë²„ê¹… ê°•í™”] í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì „ ìƒíƒœ ë¡œê¹…
                                logger.info(f"ğŸ“Š {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì „ trade_logs ê°œìˆ˜: {len(trade_logs)}")
                                
                                # í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰
                                try:
                                    self._execute_pyramiding_buy(ticker, score, gpt_confidence_map.get(ticker, 0.5), trade_logs, total_balance)
                                    logger.info(f"ğŸ“Š {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ í›„ trade_logs ê°œìˆ˜: {len(trade_logs)}")
                                    
                                    # ì‹¤í–‰ ê²°ê³¼ í™•ì¸
                                    if trade_logs:
                                        latest_log = trade_logs[-1]
                                        if latest_log.get('ticker') == ticker:
                                            logger.info(f"âœ… {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ë¡œê·¸ ì¶”ê°€ë¨: {latest_log.get('status')}")
                                        else:
                                            logger.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ë¡œê·¸ ëˆ„ë½")
                                    else:
                                        logger.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ í›„ trade_logsê°€ ë¹„ì–´ìˆìŒ")
                                        
                                except Exception as pyramid_error:
                                    logger.error(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {pyramid_error}")
                                    import traceback
                                    logger.debug(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
                                    
                                    # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ë¡œê·¸ ì¶”ê°€
                                    trade_logs.append({
                                        "ticker": ticker,
                                        "buy_price": 0,
                                        "score": score,
                                        "confidence": gpt_confidence_map.get(ticker, 0.5),
                                        "trade_amount_krw": 0,
                                        "status": "PYRAMIDING_EXCEPTION",
                                        "error_msg": str(pyramid_error)
                                    })
                                    
                            else:
                                logger.info(f"â­ï¸ {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡± - ë§¤ìˆ˜ ê±´ë„ˆëœ€")
                                logger.info(f"   - ì‚¬ìœ : {pyramid_result['reason']}")
                                continue
                        else:
                            # ìƒˆë¡œìš´ ì¢…ëª© ë§¤ìˆ˜ (ê¸°ì¡´ ë¡œì§)
                            logger.info(f"ğŸ†• {ticker} ì‹ ê·œ ì¢…ëª© - ì¼ë°˜ ë§¤ìˆ˜ ì§„í–‰")
                            self._execute_new_position_buy(ticker, score, gpt_confidence_map.get(ticker, 0.5), trade_logs, total_balance)
                            
                    except Exception as e:
                        logger.error(f"âŒ ë§¤ìˆ˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {ticker} | ì˜¤ë¥˜: {str(e)}")
                        
                        # ì˜ˆì™¸ ë°œìƒ ì´ë ¥ ìˆ˜ì§‘
                        trade_logs.append({
                            "ticker": ticker,
                            "buy_price": 0,
                            "score": score,
                            "confidence": gpt_confidence_map.get(ticker, 0.5),
                            "trade_amount_krw": base_amount,
                            "status": "ERROR",
                            "error_msg": str(e)
                        })
                
                # ë§¤ìˆ˜ ì´ë ¥ DB ì €ì¥
                try:
                    self.save_trade_log_to_db(trade_logs)
                except Exception as e:
                    logger.warning(f"âš ï¸ ë§¤ìˆ˜ ì´ë ¥ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            else:
                logger.info("ğŸ“Š ìµœì¢… ë§¤ìˆ˜ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            
            traded_tickers = [log["ticker"] for log in trade_logs if log["status"] == "SUCCESS"]
            step_time = time.time() - step_start
            
            logger.info(f"âœ… 3ë‹¨ê³„: ìµœì¢… ë§¤ìˆ˜ ì‹¤í–‰ ì™„ë£Œ - {len(traded_tickers)}ê°œ í‹°ì»¤ ë§¤ìˆ˜ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
            step_results["GPT_ë¶„ì„_ë§¤ë§¤"] = True
            
            # 4. í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸ ë° ìš”ì•½ ì •ë³´ ì¶œë ¥
            step_start = time.time()
            portfolio_data = self.update_portfolio()
            step_time = time.time() - step_start
            
            logger.info(f"âœ… 4ë‹¨ê³„: í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
            step_results["í¬íŠ¸í´ë¦¬ì˜¤_ì—…ë°ì´íŠ¸"] = True
            
            # 5. ë§¤ë„ ì¡°ê±´ ì ê²€ (PortfolioManager í™œìš©)
            step_start = time.time()
            try:
                from portfolio_manager import PortfolioManager
                portfolio_manager = PortfolioManager(self.upbit)
                portfolio_manager.check_advanced_sell_conditions(portfolio_data)
                step_time = time.time() - step_start
                logger.info(f"âœ… 5ë‹¨ê³„: ë§¤ë„ ì¡°ê±´ ì ê²€ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ)")
                step_results["ë§¤ë„_ì¡°ê±´_ì ê²€"] = True
            except Exception as e:
                step_time = time.time() - step_start
                logger.error(f"âŒ 5ë‹¨ê³„: ë§¤ë„ ì¡°ê±´ ì ê²€ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {step_time:.2f}ì´ˆ): {e}")
                step_results["ë§¤ë„_ì¡°ê±´_ì ê²€"] = False
            
            # 6. ë°±í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸
            step_start = time.time()
            try:
                # ì „ë‹¬ë°›ì€ OHLCV ë°ì´í„° ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìˆ˜ì§‘
                if market_df_4h is not None and not market_df_4h.empty:
                    logger.info("ğŸ“Š ì „ë‹¬ë°›ì€ OHLCV ë°ì´í„°ë¡œ ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰")
                    ohlcv_df = market_df_4h
                else:
                    # OHLCV ë°ì´í„° ìˆ˜ì§‘ (ë°±í…ŒìŠ¤íŒ…ìš©)
                    logger.info("ğŸ“Š ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ OHLCV ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                    ohlcv_data = {}
                    # ğŸ”§ [ê°œì„ ] ë” ë§ì€ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ (ìƒìœ„ 50ê°œë¡œ í™•ëŒ€)
                    test_tickers = list(market_df_updated.index)[:50]  # ìƒìœ„ 50ê°œ ì¢…ëª©ìœ¼ë¡œ í™•ëŒ€
                    logger.info(f"ğŸ“Š ë°±í…ŒìŠ¤íŒ… ëŒ€ìƒ ì¢…ëª©: {len(test_tickers)}ê°œ")
                    
                    for ticker in test_tickers:
                        try:
                            df = self.db_mgr.fetch_ohlcv(ticker, days=200)
                            if df is not None and not df.empty:
                                ohlcv_data[ticker] = df
                        except Exception as e:
                            logger.warning(f"âš ï¸ {ticker} OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                            continue
                    
                    if ohlcv_data and len(ohlcv_data) >= 1:  # ğŸ”§ [ê°œì„ ] ìµœì†Œ 1ê°œ ì¢…ëª© ë°ì´í„°ë¡œ ì™„í™”
                        # OHLCV ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                        ohlcv_df = pd.concat(ohlcv_data.values(), keys=ohlcv_data.keys(), names=['ticker', 'date'])
                        ohlcv_df = ohlcv_df.reset_index()
                        logger.info(f"ğŸ“Š ë°±í…ŒìŠ¤íŒ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(ohlcv_data)}ê°œ ì¢…ëª©, {len(ohlcv_df)}ê°œ ë ˆì½”ë“œ")
                        
                        # ğŸ”§ [ê°œì„ ] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¶”ê°€
                        total_records = len(ohlcv_df)
                        if total_records < 100:
                            logger.warning(f"âš ï¸ ë°±í…ŒìŠ¤íŒ… ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {total_records}ê°œ ë ˆì½”ë“œ (ê¶Œì¥: 100ê°œ ì´ìƒ)")
                        else:
                            logger.info(f"âœ… ë°±í…ŒìŠ¤íŒ… ë°ì´í„° í’ˆì§ˆ í™•ì¸: {total_records}ê°œ ë ˆì½”ë“œ")
                    else:
                        ohlcv_df = None
                        logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ OHLCV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ğŸ”§ [ê°œì„ ] ëŒ€ì²´ ë¡œì§: DBì—ì„œ ì§ì ‘ OHLCV ë°ì´í„° ì¡°íšŒ
                        logger.info("ğŸ”„ ëŒ€ì²´ ë¡œì§: DBì—ì„œ ì§ì ‘ OHLCV ë°ì´í„° ì¡°íšŒ ì‹œë„...")
                        try:
                            from utils import get_db_connection
                            conn = get_db_connection()
                            if conn:
                                # ìµœê·¼ 200ì¼ê°„ì˜ OHLCV ë°ì´í„° ì¡°íšŒ
                                query = """
                                    SELECT ticker, date, open, high, low, close, volume
                                    FROM ohlcv 
                                    WHERE date >= CURRENT_DATE - INTERVAL '200 days'
                                    ORDER BY ticker, date
                                """
                                ohlcv_df = pd.read_sql_query(query, conn)
                                conn.close()
                                
                                if not ohlcv_df.empty:
                                    logger.info(f"âœ… ëŒ€ì²´ ë°ì´í„° ì¡°íšŒ ì„±ê³µ: {len(ohlcv_df)}ê°œ ë ˆì½”ë“œ")
                                    # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
                                    ohlcv_df['date'] = pd.to_datetime(ohlcv_df['date'])
                                else:
                                    logger.warning("âš ï¸ ëŒ€ì²´ ë°ì´í„° ì¡°íšŒ ê²°ê³¼ë„ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                            else:
                                logger.warning("âš ï¸ DB ì—°ê²° ì‹¤íŒ¨ë¡œ ëŒ€ì²´ ë°ì´í„° ì¡°íšŒ ë¶ˆê°€")
                        except Exception as e:
                            logger.warning(f"âš ï¸ ëŒ€ì²´ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
                
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                if ohlcv_df is not None and not ohlcv_df.empty:
                    backtest_success = self.run_backtest_and_report(ohlcv_df, market_df_updated)
                    if backtest_success:
                        logger.info(f"âœ… 7ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
                        step_results["ë°±í…ŒìŠ¤íŠ¸_ë¦¬í¬íŠ¸"] = True
                    else:
                        logger.warning(f"âš ï¸ 7ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
                else:
                    logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ OHLCV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.error(f"âŒ ë°±í…ŒìŠ¤íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŒ…ì„ ê±´ë„ˆë›°ê³  íŒŒì´í”„ë¼ì¸ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                
            # ì‹¤í–‰ ìš”ì•½
            total_time = time.time() - start_time
            success_count = sum(1 for success in step_results.values() if success)
            success_rate = (success_count / len(step_results)) * 100
            
            logger.info(f"âœ… íŠ¸ë ˆì´ë”© ë° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ - {success_count}/{len(step_results)} ë‹¨ê³„ ì„±ê³µ ({success_rate:.1f}%) "
                      f"(ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ)")
            
            # ì‹¤íŒ¨í•œ ë‹¨ê³„ ë¡œê¹…
            failed_steps = [step for step, success in step_results.items() if not success]
            if failed_steps:
                logger.warning(f"âš ï¸ ì‹¤íŒ¨í•œ ë‹¨ê³„: {', '.join(failed_steps)}")
            
            # GPT ë¶„ì„ ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥ ì™„ë£Œ ë©”ì‹œì§€
            logger.info("âœ… GPT ë¶„ì„ ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥ ì™„ë£Œ")
                
            return success_count >= 4  # ê³¼ë°˜ìˆ˜ ì´ìƒ ì„±ê³µí•˜ë©´ ì „ì²´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë ˆì´ë”© ë° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def process_gpt_analysis_chunked(self, gpt_json_data, config):
        """ë©”ëª¨ë¦¬ ìµœì í™”ê°€ ê°•í™”ëœ ì²­í¬ ë‹¨ìœ„ GPT ë¶„ì„ ì²˜ë¦¬"""
        return self.process_gpt_analysis_chunked_enhanced(gpt_json_data, config)
    
    def process_gpt_chunk_with_retry(self, chunk, config):
        """ì¬ì‹œë„ ë¡œì§ì´ ê°•í™”ëœ GPT ì²­í¬ ì²˜ë¦¬"""
        max_retries = config.get('max_retries', 3)
        
        for attempt in range(max_retries):
            try:
                # unified_gpt_analysis_engine í˜¸ì¶œ
                results = unified_gpt_analysis_engine(chunk, config)
                return results
                
            except Exception as e:
                logger.warning(f"âš ï¸ GPT ì²­í¬ ì²˜ë¦¬ ì‹œë„ {attempt+1}/{max_retries} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    raise e
    
    def _save_intermediate_gpt_results(self, results):
        """ì¤‘ê°„ GPT ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        try:
            if self.save_to_db:
                self.save_gpt_analysis_to_db(results)
                logger.debug(f"ğŸ“ ì¤‘ê°„ GPT ê²°ê³¼ {len(results)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì¤‘ê°„ GPT ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _batch_update_trailing_stops_fallback(self, batch_updates):
        """íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì—…ë°ì´íŠ¸ fallback ë©”ì„œë“œ"""
        try:
            if not batch_updates:
                return
            
            logger.info(f"ğŸ”„ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì—…ë°ì´íŠ¸ (fallback): {len(batch_updates)}ê°œ")
            
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                for update in batch_updates:
                    try:
                        # UPSERT ì¿¼ë¦¬ ì‹¤í–‰
                        upsert_query = """
                        INSERT INTO trailing_stops 
                        (ticker, initial_price, activation_price, stop_price, atr_value, created_at, updated_at)
                        VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
                        ON CONFLICT (ticker) DO UPDATE SET
                        activation_price = EXCLUDED.activation_price,
                        stop_price = EXCLUDED.stop_price,
                        atr_value = EXCLUDED.atr_value,
                        updated_at = NOW()
                        """
                        
                        cursor.execute(upsert_query, (
                            update['ticker'],
                            update.get('initial_price', update['stop_price']),  # ì´ˆê¸°ê°€ê²© ì—†ìœ¼ë©´ ìŠ¤íƒ‘ê°€ê²© ì‚¬ìš©
                            update['activation_price'],
                            update['stop_price'],
                            update['atr_value']
                        ))
                        
                    except Exception as e:
                        logger.error(f"âŒ {update['ticker']} íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                conn.commit()
                logger.info(f"âœ… íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _log_gpt_analysis_metrics(self, results: list, config: dict):
        """GPT ë¶„ì„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        total_analyzed = len(results)
        json_count = len([r for r in results if r.get("analysis_method") == "json"])
        chart_count = len([r for r in results if r.get("analysis_method", "").startswith("chart")])
        avg_confidence = sum(r.get("confidence", 0) for r in results) / max(total_analyzed, 1)
        
        logger.info(f"ğŸ“Š GPT ë¶„ì„ ì™„ë£Œ: ì´ {total_analyzed}ê°œ")
        logger.info(f"   - JSON ë¶„ì„: {json_count}ê°œ")
        logger.info(f"   - ì°¨íŠ¸ ë¶„ì„: {chart_count}ê°œ")
        logger.info(f"   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}")
        logger.info(f"   - ë¶„ì„ ëª¨ë“œ: {config.get('mode', 'unknown')}")
        logger.info(f"   - ìºì‹± ì‚¬ìš©: {'Yes' if config.get('enable_caching') else 'No'}")

        # ìƒìœ„ 3ê°œ ì¢…ëª© ìƒì„¸ ì •ë³´
        if results:
            sorted_results = sorted(results, key=lambda x: x.get('score', 0), reverse=True)[:3]
            logger.info("   ğŸ“ˆ ìƒìœ„ 3ê°œ ì¢…ëª©:")
            for i, result in enumerate(sorted_results, 1):
                ticker = result.get('ticker', 'Unknown')
                score = result.get('score', 0)
                confidence = result.get('confidence', 0)
                method = result.get('analysis_method', 'unknown')
                logger.info(f"      {i}. {ticker}: {score}ì  (ì‹ ë¢°ë„: {confidence:.2f}, ë°©ë²•: {method})")

    def update_all_tickers(self):
        """
        ëª¨ë“  í‹°ì»¤ì˜ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ê¸°ìˆ ì  ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ğŸ”§ [ê°œì„ ] ì¤‘ë³µ í˜¸ì¶œ ì œê±°: í•„ìš”í•œ ê²½ìš°ì—ë§Œ í‹°ì»¤ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        
        ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
        1. í‹°ì»¤ ìŠ¤ìº” ë° ê¸°ë³¸ í•„í„°ë§ (DBì—ì„œ ê¸°ì¡´ í‹°ì»¤ ì‚¬ìš©)
        2. ì¼ë´‰ OHLCV ë° ì§€í‘œ ì²˜ë¦¬
        
        Returns:
            tuple: (filtered_tickers, market_df, market_df_4h) - í•„í„°ë§ëœ í‹°ì»¤ ëª©ë¡, ë§ˆì¼“ ë°ì´í„°í”„ë ˆì„, 4ì‹œê°„ë´‰ ë°ì´í„°í”„ë ˆì„
        """
        try:
            logger.info("ğŸš€ í‹°ì»¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì²˜ë¦¬ ì‹œì‘")
            
            # 1. í‹°ì»¤ ìŠ¤ìº” ë° í•„í„°ë§ (DBì—ì„œ ê¸°ì¡´ í‹°ì»¤ ì‚¬ìš©, ì¤‘ë³µ ì—…ë°ì´íŠ¸ ì œê±°)
            filtered_tickers = self.scan_and_filter_tickers()
            if not filtered_tickers:
                logger.warning("âš ï¸ í•„í„°ë§ëœ í‹°ì»¤ê°€ ì—†ì–´ ì „ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return [], pd.DataFrame(), pd.DataFrame()
                
            # 2. ì¼ë´‰ OHLCV ë° ì§€í‘œ ì²˜ë¦¬
            market_df = self.fetch_market_data_internal(filtered_tickers, timeframe='1d')
            if market_df is None or market_df.empty:
                logger.warning("âš ï¸ ì¼ë´‰ ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return filtered_tickers, pd.DataFrame(), pd.DataFrame()
                
            logger.info(f"âœ… í‹°ì»¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(filtered_tickers)}ê°œ í‹°ì»¤")
            return filtered_tickers, market_df, pd.DataFrame()
            
        except Exception as e:
            logger.error(f"âŒ í‹°ì»¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return [], pd.DataFrame(), pd.DataFrame()

    def run(self):
        """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logging.info("ğŸš€ Makenaide íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            
            # ğŸ“ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ (ì„¤ì • ê¸°ë°˜)
            try:
                from utils import cleanup_old_log_files, get_log_file_info
                from config import LOG_MANAGEMENT
                
                # ë¡œê·¸ ê´€ë¦¬ ì„¤ì • í™•ì¸
                if LOG_MANAGEMENT.get('enable_log_cleanup', True) and LOG_MANAGEMENT.get('log_cleanup_on_startup', True):
                    retention_days = LOG_MANAGEMENT.get('retention_days', 7)
                    
                    # ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì‹¤í–‰
                    cleanup_result = cleanup_old_log_files(retention_days=retention_days)
                    if cleanup_result["status"] == "success":
                        logging.info(f"ğŸ—‘ï¸ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleanup_result['deleted_count']}ê°œ íŒŒì¼ ì‚­ì œ (ë³´ê´€ê¸°ê°„: {retention_days}ì¼)")
                    else:
                        logging.warning(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_result.get('message', 'Unknown error')}")
                    
                    # í˜„ì¬ ë¡œê·¸ íŒŒì¼ ìƒíƒœ ì¶œë ¥
                    log_info = get_log_file_info()
                    if log_info["status"] == "success":
                        logging.info(f"ğŸ“Š í˜„ì¬ ë¡œê·¸ íŒŒì¼ ìƒíƒœ: {log_info['total_files']}ê°œ íŒŒì¼, ì´ {log_info['total_size_mb']}MB")
                    else:
                        logging.warning(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {log_info.get('message', 'Unknown error')}")
                else:
                    logging.info("â„¹ï¸ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logging.error(f"âŒ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì—†ì´ íŒŒì´í”„ë¼ì¸ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # 0. DB ì´ˆê¸°í™” í™•ì¸ ë° í…Œì´ë¸” ìƒì„± (ìµœìš°ì„ )
            try:
                logging.info("ğŸ”§ DB ì´ˆê¸°í™” í™•ì¸ ì¤‘...")
                self.init_db()
                logging.info("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logging.error(f"âŒ DB ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ DB ì´ˆê¸°í™” ì‹¤íŒ¨, íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return False
            
            # ğŸš¨ [NEW] Disclaimer ë™ì˜ í™•ì¸ (DB ì´ˆê¸°í™” í›„)
            try:
                from disclaimer_manager import DisclaimerManager
                disclaimer_mgr = DisclaimerManager(self.db_mgr)
                
                if not disclaimer_mgr.ensure_agreement():
                    logging.error("âŒ Disclaimer ë™ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return False
                    
                logging.info("âœ… Disclaimer ë™ì˜ í™•ì¸ ì™„ë£Œ")
                
            except Exception as e:
                logging.error(f"âŒ Disclaimer í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                logging.error("âŒ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return False
            
            # 1. ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ (ì¶”ê°€)
            try:
                validation_success = self._perform_system_validation()
                if not validation_success:
                    logging.warning("âš ï¸ ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆì§€ë§Œ íŒŒì´í”„ë¼ì¸ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                logging.error(f"âŒ ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ ë°ì´í„° ê²€ì¦ ì—†ì´ íŒŒì´í”„ë¼ì¸ì„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # 1. ì‹œì‘ ì „ ë§¤ë„ ì¡°ê±´ ì ê²€
            sell_results = self.pm.check_advanced_sell_conditions()
            if sell_results and sell_results.get('sell_targets'):
                logging.info(f"ğŸ’° ë§¤ë„ ì¡°ê±´ ì¶©ì¡±: {len(sell_results['sell_targets'])}ê±´")
                
            # 2. ìˆ˜ë™ ê°œì… ê°ì§€ (ìƒˆë¡œ ì¶”ê°€)
            try:
                intervention_results = self.pm.detect_manual_interventions()
                if intervention_results.get('total_interventions', 0) > 0:
                    logging.warning(f"âš ï¸ ìˆ˜ë™ ê°œì… {intervention_results['total_interventions']}ê±´ ê°ì§€")
                    
                    # ìˆ˜ë™ ê°œì… ìš”ì•½ ì¶œë ¥
                    for intervention in intervention_results.get('interventions', []):
                        logging.warning(f"   - {intervention['description']}")
                else:
                    logging.info("âœ… ìˆ˜ë™ ê°œì… ê°ì§€ë˜ì§€ ì•ŠìŒ")
                    
            except Exception as e:
                logging.error(f"âŒ ìˆ˜ë™ ê°œì… ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 3. í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.update_portfolio()
            
            # 4. í”¼ë¼ë¯¸ë”© ì¡°ê±´ í™•ì¸
            try:
                self._check_pyramiding_conditions(self.pm)
                logging.info("âœ… í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì™„ë£Œ")
            except Exception as e:
                logging.error(f"âŒ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì‹¤íŒ¨: {e}")
            
            # 5. ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ ì  ì§€í‘œ ì—…ë°ì´íŠ¸ (í†µí•© ì²˜ë¦¬)
            try:
                logging.info("ğŸ“Š ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ ì  ì§€í‘œ ì—…ë°ì´íŠ¸ ì¤‘...")
                # ì¤‘ë³µ í˜¸ì¶œ ì œê±°: scanner.update_tickers() ëŒ€ì‹  update_all_tickers()ë§Œ ì‚¬ìš©
                filtered_tickers, market_df, _ = self.update_all_tickers()
                
                if not filtered_tickers:
                    logging.info("âœ… ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return
                    
            except Exception as e:
                logging.error(f"âŒ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                recovery_success = self._handle_pipeline_error("market_data_collection", e)
                if not recovery_success:
                    logging.error("âŒ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ë³µêµ¬ ì‹¤íŒ¨, íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")
                    return
                else:
                    # ë³µêµ¬ ì„±ê³µ ì‹œ ê¸°ë³¸ í‹°ì»¤ë¡œ ì¬ì‹œë„
                    filtered_tickers, market_df, _ = self.update_all_tickers()
            
            # ğŸŒ¡ï¸ [ì´ë™] 6-1. ì‹œì¥ ì²´ì˜¨ê³„ ê²€ì‚¬ (ìµœì‹  ë°ì´í„° ê¸°ë°˜)
            try:
                logging.info("ğŸŒ¡ï¸ ì‹œì¥ ì²´ì˜¨ê³„ ê²€ì‚¬ ì¤‘... (ìµœì‹  ë°ì´í„° ê¸°ë°˜)")
                from market_sentiment import get_market_sentiment_snapshot
                
                sentiment_result = get_market_sentiment_snapshot()
                
                if not sentiment_result['should_proceed']:
                    logging.warning("âš ï¸ ì‹œì¥ ì¡°ê±´ ë¯¸ì¶©ì¡±ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                    logging.info(f"   - ìƒìŠ¹ì¢…ëª©: {sentiment_result['pct_up']}%")
                    logging.info(f"   - ê±°ë˜ëŒ€ê¸ˆì§‘ì¤‘ë„: {sentiment_result['top10_volume_ratio']}%")
                    logging.info(f"   - MA200ìƒíšŒ: {sentiment_result['ma200_above_ratio']}%")
                    logging.info(f"   - ì¢…í•©ì ìˆ˜: {sentiment_result['sentiment_score']}")
                    logging.info(f"   - ì‹œì¥ìƒí™©: {sentiment_result['market_condition']}")
                    return
                else:
                    logging.info(f"âœ… ì‹œì¥ ì¡°ê±´ ì¶©ì¡±, íŒŒì´í”„ë¼ì¸ ì§„í–‰")
                    logging.info(f"   - ì‹œì¥ìƒí™©: {sentiment_result['market_condition']}")
                    logging.info(f"   - ì¢…í•©ì ìˆ˜: {sentiment_result['sentiment_score']}")
                    logging.info(f"   - ìƒìŠ¹ì¢…ëª©: {sentiment_result['pct_up']}%")
                    logging.info(f"   - ê±°ë˜ëŒ€ê¸ˆì§‘ì¤‘ë„: {sentiment_result['top10_volume_ratio']}%")
                    logging.info(f"   - MA200ìƒíšŒ: {sentiment_result['ma200_above_ratio']}%")
                    
            except Exception as e:
                logging.error(f"âŒ ì‹œì¥ ì²´ì˜¨ê³„ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
                logging.warning("âš ï¸ ì‹œì¥ ì²´ì˜¨ê³„ ê²€ì‚¬ ì—†ì´ íŒŒì´í”„ë¼ì¸ ì§„í–‰")
            
            # 7. í•„í„°ë§ ë° ì¢…ëª© ì„ ë³„ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
            try:
                logging.info("ğŸ” ì¢…ëª© í•„í„°ë§ ì¤‘...")
                filtered_df = self.filter_comprehensive_indicators(market_df)
                
                if filtered_df is None or filtered_df.empty:
                    logging.info("âœ… ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return
                    
            except Exception as e:
                logging.error(f"âŒ ì¢…ëª© í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                recovery_success = self._handle_pipeline_error("filtering", e)
                if not recovery_success:
                    logging.error("âŒ ì¢…ëª© í•„í„°ë§ ë³µêµ¬ ì‹¤íŒ¨, íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")
                    return
                else:
                    # ë³µêµ¬ ì„±ê³µ ì‹œ ê¸°ë³¸ í•„í„°ë§ìœ¼ë¡œ ì§„í–‰
                    filtered_df = pd.DataFrame(index=filtered_tickers[:5])  # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
            
            # 7-1. ì¼ë´‰ ë°ì´í„° ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± (í•„í„°ë§ í†µê³¼ ì¢…ëª©)
            try:
                logging.info("ğŸ“Š ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
                chart_generation_success = True
                
                for ticker in filtered_df.index:
                    try:
                                                # OHLCV ë°ì´í„° ì¡°íšŒ
                        df = self.db_mgr.fetch_ohlcv(ticker, days=400)
                        if df is not None and not df.empty:
                            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì§€í‘œë“¤ í¬í•¨)
                            from data_fetcher import calculate_technical_indicators
                            df_with_indicators = calculate_technical_indicators(df)
                            
                            if df_with_indicators is not None and not df_with_indicators.empty:
                                # ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± (data_fetcher.pyì˜ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ)
                                chart_path = generate_chart_image(ticker, df_with_indicators)
                                if chart_path:
                                    logging.info(f"âœ… {ticker} ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {chart_path}")
                                else:
                                    logging.warning(f"âš ï¸ {ticker} ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                                    chart_generation_success = False
                            else:
                                logging.warning(f"âš ï¸ {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                                chart_generation_success = False
                        else:
                            logging.warning(f"âš ï¸ {ticker} ì¼ë´‰ OHLCV ë°ì´í„° ì—†ìŒ")
                            chart_generation_success = False
                    except Exception as chart_e:
                        logging.error(f"âŒ {ticker} ì¼ë´‰ ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {chart_e}")
                        chart_generation_success = False
                        continue
                
                if chart_generation_success:
                    logging.info(f"âœ… ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {len(filtered_df.index)}ê°œ ì¢…ëª©")
                else:
                    logging.warning("âš ï¸ ì¼ë¶€ ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
                
            except Exception as e:
                logging.error(f"âŒ ì¼ë´‰ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ ì¼ë´‰ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
            
            # ğŸ”„ [ìˆ˜ì •] 8. GPT ë¶„ì„ì„ ìœ„í•œ JSON ìƒì„± ë° GPT ë¶„ì„ ì‹¤í–‰
            try:
                logging.info("ğŸ¤– GPT ë¶„ì„ì„ ìœ„í•œ JSON ìƒì„± ë° ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                
                # GPT ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ì¤€ë¹„ (JSON ë°©ì‹) - ìºì‹± ë¡œì§ ì ìš©
                analysis_candidates = []
                skipped_count = 0
                
                for ticker in filtered_df.index:
                    # ìºì‹± ë¡œì§ ì ìš©: ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ í™•ì¸
                    should_skip, existing_analysis = self._check_gpt_analysis_cache(ticker)
                    
                    if should_skip and existing_analysis:
                        # ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
                        analysis_candidates.append({
                            "ticker": ticker,
                            "base_score": existing_analysis.get('score', 85),
                            "cached_result": existing_analysis,
                            "skip_gpt_call": True
                        })
                        skipped_count += 1
                        logging.info(f"â­ï¸ {ticker} ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš© (score: {existing_analysis.get('score', 85)})")
                    else:
                        # ìƒˆë¡œìš´ JSON ë°ì´í„° ìƒì„±
                        from data_fetcher import generate_gpt_analysis_json
                        json_data = generate_gpt_analysis_json(ticker, days=200)
                        if json_data:
                            analysis_candidates.append({
                                "ticker": ticker,
                                "base_score": 85,
                                "json_data": json_data
                            })
                        else:
                            logging.warning(f"âš ï¸ {ticker} JSON ë°ì´í„° ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë°ì´í„°ë¡œ ì§„í–‰")
                            analysis_candidates.append({
                                "ticker": ticker,
                                "base_score": 85
                            })
                
                logging.info(f"ğŸ“‹ GPT ë¶„ì„ ëŒ€ìƒ: {len(analysis_candidates)}ê°œ ì¢…ëª© (ìºì‹œ ì‚¬ìš©: {skipped_count}ê°œ)")
                
                # GPT ë¶„ì„ ì„¤ì •
                gpt_config = self.get_gpt_config()
                from trend_analyzer import AnalysisConfig, GPTAnalysisOptimizerSingleton
                analysis_config = AnalysisConfig(
                    mode="json",
                    batch_size=gpt_config.get("batch_size", 3),
                    enable_caching=gpt_config.get("enable_caching", True),
                    cache_ttl_minutes=gpt_config.get("cache_ttl_minutes", 720),
                    api_timeout_seconds=gpt_config.get("api_timeout_seconds", 30),
                    max_retries=gpt_config.get("max_retries", 3)
                )
                
                # GPT ë¶„ì„ ìµœì í™”ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                optimizer = GPTAnalysisOptimizerSingleton()
                
                # ìºì‹œëœ ê²°ê³¼ì™€ ìƒˆë¡œìš´ GPT ë¶„ì„ ê²°ê³¼ í†µí•©
                gpt_results = []
                
                # 1. ìºì‹œëœ ê²°ê³¼ ì²˜ë¦¬
                cached_results = [candidate for candidate in analysis_candidates if candidate.get('skip_gpt_call')]
                for cached_candidate in cached_results:
                    cached_result = cached_candidate['cached_result']
                    gpt_results.append({
                        'ticker': cached_candidate['ticker'],
                        'score': cached_result.get('score', 85),
                        'action': cached_result.get('action', 'HOLD'),
                        'confidence': cached_result.get('confidence', 0.7),
                        'market_phase': cached_result.get('market_phase', 'Unknown'),
                        'pattern': cached_result.get('pattern', ''),
                        'reason': cached_result.get('reason', ''),
                        'from_cache': True
                    })
                
                # 2. ìƒˆë¡œìš´ GPT ë¶„ì„ ì‹¤í–‰ (ìºì‹œë˜ì§€ ì•Šì€ ì¢…ëª©ë§Œ)
                new_candidates = [candidate for candidate in analysis_candidates if not candidate.get('skip_gpt_call')]
                
                if new_candidates:
                    from trend_analyzer import _call_gpt_json_batch
                    logging.info(f"ğŸ§  ìƒˆë¡œìš´ GPT JSON ë¶„ì„ ì‹¤í–‰: {len(new_candidates)}ê°œ ì¢…ëª©")
                    new_gpt_results = _call_gpt_json_batch(new_candidates, analysis_config, optimizer)
                    gpt_results.extend(new_gpt_results)
                    logging.info(f"âœ… ìƒˆë¡œìš´ GPT ë¶„ì„ ì™„ë£Œ: {len(new_gpt_results)}ê°œ ê²°ê³¼")
                else:
                    logging.info("â­ï¸ ìƒˆë¡œìš´ GPT ë¶„ì„ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ìºì‹œ ì‚¬ìš©)")
                
                logging.info(f"âœ… ì „ì²´ GPT ë¶„ì„ ì™„ë£Œ: {len(gpt_results)}ê°œ ê²°ê³¼ (ìºì‹œ: {len(cached_results)}ê°œ, ì‹ ê·œ: {len(new_candidates)}ê°œ)")
                
                # GPT ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                if gpt_results:
                    logging.info("ğŸ“Š GPT ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
                    cached_count = 0
                    new_count = 0
                    
                    for result in gpt_results:
                        ticker = result.get('ticker', 'Unknown')
                        score = result.get('score', 0)
                        action = result.get('action', 'Unknown')
                        confidence = result.get('confidence', 0)
                        from_cache = result.get('from_cache', False)
                        
                        if from_cache:
                            cached_count += 1
                            logging.info(f"   - {ticker}: {score}ì , {action}, ì‹ ë¢°ë„: {confidence:.2f} [ìºì‹œ]")
                        else:
                            new_count += 1
                            logging.info(f"   - {ticker}: {score}ì , {action}, ì‹ ë¢°ë„: {confidence:.2f} [ì‹ ê·œ]")
                    
                    logging.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ í†µê³„: ìºì‹œ {cached_count}ê°œ, ì‹ ê·œ {new_count}ê°œ")
                
            except Exception as e:
                logging.error(f"âŒ GPT ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                logging.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                recovery_success = self._handle_pipeline_error("gpt_analysis", e)
                if recovery_success:
                    # GPT ë¶„ì„ ì—†ì´ ê¸°ë³¸ ê±°ë˜ ë¡œì§ìœ¼ë¡œ ì§„í–‰
                    gpt_results = []
                    for ticker in filtered_df.index:
                        gpt_results.append({
                            'ticker': ticker,
                            'action': 'buy',
                            'confidence': 0.7,
                            'score': 75
                        })
                    logging.warning("âš ï¸ GPT ë¶„ì„ ìš°íšŒ, ê¸°ë³¸ ê±°ë˜ ë¡œì§ìœ¼ë¡œ ì§„í–‰")
                else:
                    logging.error("âŒ GPT ë¶„ì„ ë³µêµ¬ ì‹¤íŒ¨, íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")
                    return
            
            # ğŸ”„ [ìˆ˜ì •] 9. GPT ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ (ì¡°ê±´ í†µê³¼ ì¢…ëª©ë§Œ)
            try:
                logging.info("â° GPT ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
                
                # GPT ë¶„ì„ ê²°ê³¼ì—ì„œ ì¡°ê±´ì„ í†µê³¼í•œ ì¢…ëª©ë“¤ ì„ ë³„
                qualified_tickers = []
                if gpt_results:
                    for result in gpt_results:
                        from utils import safe_float_convert
                        score = safe_float_convert(result.get("score", 0), context=f"4ì‹œê°„ë´‰í•„í„° {result.get('ticker', 'Unknown')} score")
                        confidence = safe_float_convert(result.get("confidence", 0), context=f"4ì‹œê°„ë´‰í•„í„° {result.get('ticker', 'Unknown')} confidence")
                        action = result.get("action", "buy")
                        market_phase = result.get("market_phase", "Unknown")

                        # GPT ë¶„ì„ ê²°ê³¼ ì¡°ê±´ í†µê³¼ ì¢…ëª© ì„ ë³„ (ì„¤ì • ê¸°ë°˜)
                        action = result.get("action", "AVOID").upper()
                        try:
                            from config import GPT_FILTERING_CONFIG
                            strict_config = GPT_FILTERING_CONFIG['strict_mode']
                        except ImportError:
                            # fallback ì„¤ì •
                            strict_config = {
                                'min_score': 80,
                                'min_confidence': 0.9,
                                'allowed_actions': ['BUY', 'STRONG_BUY'],
                                'allowed_market_phases': ['Stage1', 'Stage2']
                            }
                        
                        if (score >= strict_config['min_score'] and 
                            confidence >= strict_config['min_confidence'] and 
                            action in strict_config['allowed_actions'] and 
                            market_phase in strict_config['allowed_market_phases']):
                            qualified_tickers.append(result["ticker"])
                
                if qualified_tickers:
                    logging.info(f"ğŸ¯ ì¡°ê±´ í†µê³¼ {len(qualified_tickers)}ê°œ í‹°ì»¤ì˜ 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
                    
                    for ticker in qualified_tickers:
                        try:
                            # 4ì‹œê°„ë´‰ OHLCV ìˆ˜ì§‘
                            from data_fetcher import get_ohlcv_4h, save_ohlcv_4h_to_db
                            df_4h = get_ohlcv_4h(ticker, limit=200, force_fetch=True)
                            
                            if df_4h is not None and not df_4h.empty:
                                # DB ì €ì¥
                                save_ohlcv_4h_to_db(ticker, df_4h)
                                
                                # ë§ˆì¼“íƒ€ì´ë° ì§€í‘œ ê³„ì‚°
                                from data_fetcher import calculate_technical_indicators_4h
                                df_with_indicators = calculate_technical_indicators_4h(df_4h)
                                
                                if df_with_indicators is not None:
                                    # market_data_4h í…Œì´ë¸”ì— ì €ì¥
                                    from data_fetcher import save_market_data_4h_to_db
                                    save_market_data_4h_to_db(ticker, df_with_indicators)
                                    logging.info(f"âœ… {ticker} 4ì‹œê°„ë´‰ ì²˜ë¦¬ ì™„ë£Œ (OHLCV + ì§€í‘œ)")
                                else:
                                    logging.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                            else:
                                logging.warning(f"âš ï¸ {ticker} 4ì‹œê°„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                                
                        except Exception as e:
                            logging.error(f"âŒ {ticker} 4ì‹œê°„ë´‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                else:
                    logging.info("ğŸ“Š ì¡°ê±´ í†µê³¼ ì¢…ëª©ì´ ì—†ì–´ 4ì‹œê°„ë´‰ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    
            except Exception as e:
                logging.error(f"âŒ 4ì‹œê°„ë´‰ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ 4ì‹œê°„ë´‰ ì²˜ë¦¬ ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
            
            # 10. ìµœì¢… ê±°ë˜ ì‹¤í–‰ ë° ë¦¬í¬íŠ¸ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
            if not gpt_results:
                logging.warning("âš ï¸ GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
                
            try:
                logging.info("ğŸ“‹ ê±°ë˜ ë° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
                
                # ë°±í…ŒìŠ¤íŒ…ì„ ìœ„í•œ OHLCV ë°ì´í„° ì¤€ë¹„
                ohlcv_data_for_trading = {}
                trading_tickers = [result.get('ticker', '') for result in gpt_results if result.get('ticker')]
                
                for ticker in trading_tickers[:10]:  # ìƒìœ„ 10ê°œ ì¢…ëª©ë§Œ
                    try:
                        df = self.db_mgr.fetch_ohlcv(ticker, days=200)
                        if df is not None and not df.empty:
                            ohlcv_data_for_trading[ticker] = df
                    except Exception as e:
                        logging.warning(f"âš ï¸ {ticker} OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                        continue
                
                # OHLCV ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                if ohlcv_data_for_trading:
                    ohlcv_df_for_trading = pd.concat(ohlcv_data_for_trading.values(), keys=ohlcv_data_for_trading.keys(), names=['ticker', 'date'])
                    ohlcv_df_for_trading = ohlcv_df_for_trading.reset_index()
                    logging.info(f"ğŸ“Š ê±°ë˜ìš© OHLCV ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(ohlcv_data_for_trading)}ê°œ ì¢…ëª©")
                else:
                    ohlcv_df_for_trading = None
                    logging.warning("âš ï¸ ê±°ë˜ìš© OHLCV ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                
                # GPT ë¶„ì„ ê²°ê³¼ë¥¼ scored_tickers í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                scored_tickers = [(result.get('ticker', ''), result.get('score', 0)) for result in gpt_results if result.get('ticker')]
                self.trade_and_report(scored_tickers, market_df, ohlcv_df_for_trading, gpt_results)
                
            except Exception as e:
                logging.error(f"âŒ ê±°ë˜ ë° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                logging.warning("âš ï¸ ê±°ë˜ ì‹¤í–‰ì€ ê±´ë„ˆë›°ê³  íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            
            logging.info("âœ… Makenaide íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # ì „ì²´ ì‹¤íŒ¨ ì‹œì—ë„ ë³µêµ¬ ì‹œë„
            try:
                self._handle_critical_error(e)
            except:
                pass
            raise

    def _check_pyramiding_conditions(self, portfolio_manager):
        """ë³´ìœ  ì¢…ëª©ì— ëŒ€í•œ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ë° ì‹¤í–‰ (ì•ˆì „ì„± ê°•í™”)"""
        try:
            # í˜„ì¬ ë³´ìœ  ì¢…ëª© ì¡°íšŒ
            current_positions = portfolio_manager.get_current_positions()
            
            if not current_positions:
                logging.info("ğŸ“Š ë³´ìœ  ì¢…ëª©ì´ ì—†ì–´ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            logging.info(f"ğŸ“Š í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ëŒ€ìƒ: {len(current_positions)}ê°œ ë³´ìœ  ì¢…ëª©")
            
            pyramiding_results = []
            
            for position in current_positions:
                ticker = position.get('ticker', '')
                if not ticker:
                    continue
                
                # í‹°ì»¤ í˜•ì‹ í™•ì¸ (KRW- ì ‘ë‘ì‚¬ ì œê±°)
                if ticker.startswith('KRW-'):
                    symbol = ticker[4:]  # KRW- ì œê±°
                else:
                    symbol = ticker
                
                # í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
                try:
                    # ë§¤ìˆ˜ ì •ë³´ë¥¼ portfolio_manager.purchase_infoì— ë“±ë¡
                    # (ì‹¤ì œ ë³´ìœ  ì¢…ëª©ì´ë¯€ë¡œ ê¸°ì¡´ ë§¤ìˆ˜ ì •ë³´ë¡œ ê°„ì£¼)
                    if ticker not in portfolio_manager.purchase_info:
                        # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                        avg_price = self._safe_extract_position_data(position, 'avg_price', 'avg_buy_price')
                        timestamp = self._safe_extract_position_data(position, 'timestamp', 'created_at')
                        quantity = self._safe_extract_position_data(position, 'quantity', 'balance')
                        
                        if avg_price and avg_price > 0:
                            portfolio_manager.purchase_info[ticker] = {
                                'price': float(avg_price),
                                'timestamp': str(timestamp) if timestamp else '',
                                'quantity': float(quantity) if quantity else 0
                            }
                            logging.debug(f"âœ… {ticker} í”¼ë¼ë¯¸ë”© ì •ë³´ ë“±ë¡: í‰ê· ê°€={avg_price}, ìˆ˜ëŸ‰={quantity}")
                        else:
                            logging.warning(f"âš ï¸ {ticker} ìœ íš¨í•˜ì§€ ì•Šì€ í¬ì§€ì…˜ ë°ì´í„°, í”¼ë¼ë¯¸ë”© ê±´ë„ˆëœ€")
                            continue
                    
                    # í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì²´í¬ (ì•ˆì „ì„± ê°•í™”)
                    pyramid_executed = self._safe_check_pyramiding(portfolio_manager, ticker)
                    
                    if pyramid_executed is True:
                        pyramiding_results.append({
                            'ticker': ticker,
                            'status': 'executed',
                            'message': f'{ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ë¨'
                        })
                        logging.info(f"âœ… {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì™„ë£Œ")
                    elif pyramid_executed is False:
                        pyramiding_results.append({
                            'ticker': ticker,
                            'status': 'no_action',
                            'message': f'{ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡±'
                        })
                        logging.debug(f"ğŸ“Š {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡±")
                    else:
                        # None ë°˜í™˜ ì‹œ (ì—ëŸ¬ ìƒí™©)
                        pyramiding_results.append({
                            'ticker': ticker,
                            'status': 'error',
                            'message': f'{ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜'
                        })
                        logging.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ê²°ê³¼ None ë°˜í™˜")
                        
                except Exception as e:
                    pyramiding_results.append({
                        'ticker': ticker,
                        'status': 'error',
                        'message': f'{ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
                    })
                    logging.error(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ë¡œê¹…
                    import traceback
                    logging.debug(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            
            # í”¼ë¼ë¯¸ë”© ê²°ê³¼ ìš”ì•½
            executed_count = sum(1 for r in pyramiding_results if r['status'] == 'executed')
            no_action_count = sum(1 for r in pyramiding_results if r['status'] == 'no_action')
            error_count = sum(1 for r in pyramiding_results if r['status'] == 'error')
            
            logging.info("ğŸ”¼ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì™„ë£Œ:")
            logging.info(f"   - ì‹¤í–‰: {executed_count}ê±´")
            logging.info(f"   - ë¯¸ì‹¤í–‰: {no_action_count}ê±´")
            logging.info(f"   - ì˜¤ë¥˜: {error_count}ê±´")
            
            # í”¼ë¼ë¯¸ë”© ì‹¤í–‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            if executed_count > 0:
                executed_tickers = [r['ticker'] for r in pyramiding_results if r['status'] == 'executed']
                logging.info(f"ğŸ”¼ í”¼ë¼ë¯¸ë”© ì‹¤í–‰ ì¢…ëª©: {', '.join(executed_tickers)}")
            
            # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            if error_count > 0:
                error_details = [f"{r['ticker']}: {r['message']}" for r in pyramiding_results if r['status'] == 'error']
                logging.warning(f"âš ï¸ í”¼ë¼ë¯¸ë”© ì—ëŸ¬ ìƒì„¸: {'; '.join(error_details)}")
            
        except Exception as e:
            logging.error(f"âŒ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            # ìƒì„¸ ì—ëŸ¬ ì •ë³´ ë¡œê¹…
            import traceback
            logging.debug(f"âŒ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            
    def _safe_extract_position_data(self, position: dict, primary_key: str, fallback_key: str = None):
        """í¬ì§€ì…˜ ë°ì´í„°ì—ì„œ ì•ˆì „í•˜ê²Œ ê°’ ì¶”ì¶œ"""
        try:
            # 1ì°¨ í‚¤ë¡œ ì¡°íšŒ
            value = position.get(primary_key)
            if value is not None and value != 0:
                return value
                
            # 2ì°¨ í‚¤ë¡œ ì¡°íšŒ (fallback)
            if fallback_key:
                value = position.get(fallback_key)
                if value is not None and value != 0:
                    return value
                    
            return None
            
        except Exception as e:
            logging.warning(f"âš ï¸ í¬ì§€ì…˜ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
            
    def _safe_check_pyramiding(self, portfolio_manager, ticker: str):
        """ì•ˆì „í•œ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì²´í¬"""
        try:
            # í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì²´í¬ ì‹œë„
            result = portfolio_manager.check_pyramiding(ticker)
            
            # ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦
            if result is None:
                logging.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ê²°ê³¼ê°€ None")
                return None
            elif isinstance(result, bool):
                return result
            else:
                logging.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ì²´í¬ ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
                return None
                
        except Exception as e:
            logging.error(f"âŒ {ticker} ì•ˆì „í•œ í”¼ë¼ë¯¸ë”© ì²´í¬ ì‹¤íŒ¨: {e}")
            return None

    def _initialize_system(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë‹¨ê³„"""
        try:
            # DB ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ)
            if not self.initialized:
                step_start = time.time()
                self.init_db()
                self.initialized = True
                logger.info(f"âœ… DB ì´ˆê¸°í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            else:
                logger.info("âœ… DB ì´ë¯¸ ì´ˆê¸°í™”ë¨ (ê±´ë„ˆëœ€)")
                
            # ğŸ”§ [ê°œì„ ] í‹°ì»¤ ì •ë³´ ì—…ë°ì´íŠ¸ëŠ” update_all_tickers()ì—ì„œ í†µí•© ì²˜ë¦¬
            # ì¤‘ë³µ í˜¸ì¶œ ì œê±°ë¡œ ì„±ëŠ¥ í–¥ìƒ
            logger.info("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (í‹°ì»¤ ì—…ë°ì´íŠ¸ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ì²˜ë¦¬)")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _update_market_data(self):
        """ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì²˜ë¦¬"""
        try:
            step_start = time.time()
            
            # í‹°ì»¤ ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì „ì²˜ë¦¬
            filtered_tickers, market_df, _ = self.update_all_tickers()
            
            if not filtered_tickers or market_df is None or market_df.empty:
                logger.warning("âš ï¸ ì‹œì¥ ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None
                
            logger.info(f"âœ… ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len(filtered_tickers)}ê°œ í‹°ì»¤, ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            return {
                'filtered_tickers': filtered_tickers,
                'market_df': market_df
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
            
    def _execute_trading_pipeline(self, market_data):
        """íŠ¸ë ˆì´ë”© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            step_start = time.time()
            
            # ì¢…í•© ì§€í‘œ í•„í„°ë§
            filtered_df = self.filter_comprehensive_indicators(market_data['market_df'])
            
            if filtered_df is None or filtered_df.empty:
                logger.warning("âš ï¸ ì¢…í•© ì§€í‘œ í•„í„°ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False

            logger.info(f"âœ… ì¢…í•© ì§€í‘œ í•„í„°ë§ ì™„ë£Œ (ì„ ë³„ëœ í‹°ì»¤: {len(filtered_df)}ê°œ, ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")

            # ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            self._generate_chart_images(filtered_df)
            
            # GPT ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            gpt_data = self._prepare_gpt_analysis_data(filtered_df)
            
            # trend_analyzer.pyì˜ unified_gpt_analysis_engine ì‚¬ìš©
            from trend_analyzer import unified_gpt_analysis_engine
            
            # GPT ë¶„ì„ ì„¤ì •
            analysis_config = self.get_gpt_config()
            
            # unified_gpt_analysis_engine í˜¸ì¶œí•˜ì—¬ GPT ë¶„ì„ ì‹¤í–‰
            logger.info("ğŸ§  GPT ë¶„ì„ ì‹¤í–‰ (trend_analyzer.unified_gpt_analysis_engine ì‚¬ìš©)")
            gpt_results = unified_gpt_analysis_engine(gpt_data, analysis_config)
            
            if gpt_results:
                logger.info(f"âœ… GPT ë¶„ì„ ì™„ë£Œ: {len(gpt_results)}ê°œ ê²°ê³¼")
                # ê²°ê³¼ë¥¼ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬
                trading_results = self.trade_and_report(gpt_results, market_data['market_df'], None, gpt_results)
                return trading_results
            else:
                logger.warning("âš ï¸ GPT ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
        except Exception as e:
            logger.error(f"âŒ íŠ¸ë ˆì´ë”© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def _generate_chart_images(self, filtered_df):
        """í•„í„°ë§ í†µê³¼ ì¢…ëª©ì— ëŒ€í•œ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±"""
        if filtered_df.empty:
            return
            
        logger.info("ğŸ“Š í•„í„°ë§ í†µê³¼ ì¢…ëª© ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
        for ticker in filtered_df.index:
            try:
                ohlcv_data = self.get_ohlcv_from_db(ticker, limit=250)
                if ohlcv_data is not None and not ohlcv_data.empty:
                    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì§€í‘œë“¤ í¬í•¨)
                    from data_fetcher import calculate_technical_indicators
                    ohlcv_with_indicators = calculate_technical_indicators(ohlcv_data)
                    
                    if ohlcv_with_indicators is not None and not ohlcv_with_indicators.empty:
                        # data_fetcher.pyì˜ generate_chart_image í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
                        chart_path = generate_chart_image(ticker, ohlcv_with_indicators)
                        logger.info(f"âœ… {ticker} ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {chart_path}")
                    else:
                        logger.warning(f"âš ï¸ {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨")
                else:
                    logger.warning(f"âš ï¸ {ticker} OHLCV ë°ì´í„° ì—†ìŒ, ì°¨íŠ¸ ìƒì„± ê±´ë„ˆëœ€")
            except Exception as e:
                logger.error(f"âŒ {ticker} ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def _prepare_gpt_analysis_data(self, filtered_df):
        """GPT ë¶„ì„ìš© ë°ì´í„° ì¤€ë¹„"""
        # ì¤‘ë³µ í‹°ì»¤ ì œê±°
        filtered_df = filtered_df[~filtered_df.index.duplicated(keep='first')]
        logger.info(f"[ì¤‘ë³µ ì œê±°] GPT ë¶„ì„ ëŒ€ìƒ í‹°ì»¤ ìˆ˜: {len(filtered_df)}")
        
        # ê¸°ì¡´ì˜ ê¸´ GPT ë°ì´í„° ì¤€ë¹„ ë¡œì§ì„ ì—¬ê¸°ë¡œ ì´ë™
        # (í˜„ì¬ ì½”ë“œì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ë™ì¼í•˜ê²Œ ìœ ì§€)
        scored_tickers = [(ticker, 85.0) for ticker in filtered_df.index]
        return scored_tickers

    def _generate_reports(self, market_df):
        """ë¦¬í¬íŠ¸ ìƒì„± - í–¥ìƒëœ ë°±í…ŒìŠ¤íŠ¸ ì—°ë™"""
        try:
            step_start = time.time()
            
            # 1. ì‹¤ì‹œê°„ ì„±ê³¼ ì—…ë°ì´íŠ¸
            try:
                from strategy_analyzer import get_enhanced_analyzer  # í†µí•©ëœ ë²„ì „ ì‚¬ìš©
                analyzer = get_enhanced_analyzer()
                performance_update = analyzer.update_strategy_performance(days=7)
                
                if performance_update:
                    logger.info(f"ğŸ“Š ì‹¤ì‹œê°„ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ìŠ¹ë¥  {performance_update.get('win_rate', 0):.1%}")
                    
                    # ì„±ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ì¶œë ¥
                    recommendation = performance_update.get('recommendation', '')
                    if recommendation:
                        logger.info(f"ğŸ’¡ ì¶”ì²œì‚¬í•­: {recommendation}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹¤ì‹œê°„ ì„±ê³¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
            # 2. ê¸°ì¡´ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if market_df is not None and not market_df.empty:
                backtest_success = self.run_backtest_and_report(None, market_df)
                logger.info(f"âœ… ë°±í…ŒìŠ¤íŠ¸ ë° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
                return backtest_success
            else:
                logger.warning("âš ï¸ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _handle_critical_error(self, error):
        """ì¤‘ìš”í•œ ì˜¤ë¥˜ ì²˜ë¦¬"""
        logger.error(f"âŒ Makenaide ë´‡ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False

    def get_trading_config_value(self, key_path: str, default: Any = None) -> Any:
        """íŠ¸ë ˆì´ë”© ì„¤ì •ê°’ ì¡°íšŒ (ê¸°ì¡´ ì„¤ì • ìš°ì„ , ì—†ìœ¼ë©´ ìƒˆ ì„¤ì •)"""
        if self.trading_config:
            return self.trading_config.get(key_path, default)
        return default

    def update_trading_config_value(self, key_path: str, value: Any) -> bool:
        """íŠ¸ë ˆì´ë”© ì„¤ì •ê°’ ì—…ë°ì´íŠ¸"""
        if self.trading_config:
            return self.trading_config.set(key_path, value)
        return False

    def get_gpt_config(self) -> Dict[str, Any]:
        """GPT ë¶„ì„ ì„¤ì • ì¡°íšŒ"""
        if self.trading_config:
            return self.trading_config.get_gpt_config()
        return {
            'score_threshold': 85,
            'confidence_threshold': 0.9,
            'batch_size': 5,
            'memory_threshold_mb': 500
        }

    def get_risk_config(self) -> Dict[str, Any]:
        """ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì„¤ì • ì¡°íšŒ"""
        if self.trading_config:
            return self.trading_config.get_risk_config()
        return {
            'base_stop_loss': 3.0,
            'base_take_profit': 6.0,
            'max_volatility_multiplier': 3.0,
            'max_position_size': 0.05
        }

    def get_active_tickers_hybrid(self):
        """
        ì „ì²´ í™œì„± í‹°ì»¤ ì¡°íšŒ: ì‹œìŠ¤í…œ ì „ë°˜ì ì¸ í‹°ì»¤ ëª©ë¡ ì œê³µ (ë ˆê±°ì‹œ í˜¸í™˜ì„± ìœ ì§€)
        
        ì£¼ìš” ìš©ë„:
        - ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œ í™œì„± í‹°ì»¤ ëª©ë¡ ì œê³µ
        - ë°±í…ŒìŠ¤íŠ¸ë‚˜ ì „ì²´ ì‹œì¥ ë¶„ì„ìš© í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        - ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
        
        âš ï¸ ì£¼ì˜: filter_comprehensive_indicators()ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        ëŒ€ì‹  _validate_active_status_only()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ í•„í„°ë§ëœ í‹°ì»¤ì˜ í™œì„± ìƒíƒœë§Œ ê²€ì¦
        
        ìš°ì„ ìˆœìœ„:
        1. is_active ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
        2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ê°€ í•„í„°ë§
        3. ë‘ ê²°ê³¼ì˜ êµì§‘í•©ì„ ìµœì¢… í™œìš©
        
        Returns:
            list: í™œì„± ìƒíƒœì´ê³  ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ì „ì²´ í‹°ì»¤ ëª©ë¡
        """
        try:
            # ê²€ì¦ í•¨ìˆ˜ ì‹¤í–‰
            from utils import validate_ticker_filtering_system
            validation = validate_ticker_filtering_system()
            
            logger.info(f"ğŸ“Š ì „ì²´ í‹°ì»¤ í•„í„°ë§ ì‹œìŠ¤í…œ ê²€ì¦ ê²°ê³¼:")
            logger.info(f"   - is_active ì»¬ëŸ¼ ì‚¬ìš© ê°€ëŠ¥: {validation['is_active_available']}")
            logger.info(f"   - ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥: {validation['blacklist_available']}")
            logger.info(f"   - í•„í„°ë§ ì¼ê´€ì„±: {validation['filtering_consistency']}")
            
            tickers_result = self.db_mgr.execute_query("SELECT ticker FROM tickers")
            if not tickers_result:
                logger.warning("âš ï¸ í‹°ì»¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
            if validation["is_active_available"]:
                # is_active ì»¬ëŸ¼ í™œìš©
                active_result = self.db_mgr.execute_query("SELECT ticker FROM tickers WHERE is_active = true")
                active_tickers = {row[0] for row in active_result} if active_result else set()
                logger.info(f"ğŸ“Š is_active í•„í„°ë§ ê²°ê³¼: {len(active_tickers)}ê°œ í‹°ì»¤")
            else:
                # ì „ì²´ í‹°ì»¤ ì¡°íšŒ
                active_tickers = {row[0] for row in tickers_result}
                logger.info(f"ğŸ“Š ì „ì²´ í‹°ì»¤ ì¡°íšŒ: {len(active_tickers)}ê°œ í‹°ì»¤")
            
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ í•„í„°ë§
            if validation["blacklist_available"]:
                blacklist = load_blacklist()
                filtered_tickers = [t for t in active_tickers if t not in blacklist]
                logger.info(f"ğŸ“Š ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ í•„í„°ë§ í›„: {len(filtered_tickers)}ê°œ í‹°ì»¤")
                
                # ì¼ê´€ì„± í™•ì¸ ë° ê²½ê³ 
                if validation["is_active_available"] and validation["consistency_rate"] < 0.8:
                    logger.warning(f"âš ï¸ is_active ì»¬ëŸ¼ê³¼ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¼ê´€ì„± ë‚®ìŒ: {validation['consistency_rate']:.2%}")
                    logger.warning("   ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë™ê¸°í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤: python scanner.py --sync-blacklist")
                    
            else:
                filtered_tickers = list(active_tickers)
                logger.warning("âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨, is_active ê²°ê³¼ë§Œ ì‚¬ìš©")
            
            logger.info(f"âœ… ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì™„ë£Œ: {len(filtered_tickers)}ê°œ í‹°ì»¤ ì„ ë³„")
            return filtered_tickers
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ í‹°ì»¤ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            try:
                tickers_result = self.db_mgr.execute_query("SELECT ticker FROM tickers")
                if tickers_result:
                    tickers = [row[0] for row in tickers_result]
                    blacklist = load_blacklist()
                    filtered_tickers = [t for t in tickers if t not in blacklist]
                    logger.info(f"ğŸ”„ í´ë°± í•„í„°ë§ ì™„ë£Œ: {len(filtered_tickers)}ê°œ í‹°ì»¤")
                    return filtered_tickers
                return []
            except:
                return []

    def _activate_emergency_cleanup(self, all_results, streaming_saver):
        """ê¸´ê¸‰ ì •ë¦¬ ëª¨ë“œ í™œì„±í™”"""
        logger.warning("ğŸš¨ ê¸´ê¸‰ ì •ë¦¬ ëª¨ë“œ í™œì„±í™”")
        
        # 1ë‹¨ê³„: ëª¨ë“  ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì €ì¥
        if all_results:
            streaming_saver.emergency_save(all_results)
            all_results.clear()
            logger.info(f"ğŸ’¾ ê¸´ê¸‰ ì €ì¥ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: ê°•í™”ëœ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            for generation in range(3):
                collected = gc.collect(generation)
            logger.debug(f"ğŸ§¹ GC ì„¸ëŒ€ {generation}: {collected}ê°œ ê°ì²´ ì •ë¦¬")
        
        # 3ë‹¨ê³„: ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬ ìš”ì²­
        try:
            import ctypes
            if hasattr(ctypes, 'windll'):  # Windows
                ctypes.windll.kernel32.SetProcessWorkingSetSize(-1, -1, -1)
            elif hasattr(ctypes, 'CDLL'):  # Unix/Linux
                libc = ctypes.CDLL("libc.so.6")
                libc.malloc_trim(0)
        except:
            pass  # í”Œë«í¼ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë¬´ì‹œ
        
        logger.info("ğŸ§¹ ê¸´ê¸‰ ì •ë¦¬ ëª¨ë“œ ì™„ë£Œ")

    def _resize_remaining_chunks(self, chunks, current_index, new_chunk_size):
        """ë‚¨ì€ ì²­í¬ë“¤ì„ ìƒˆë¡œìš´ í¬ê¸°ë¡œ ì¬ë¶„í• """
        if current_index >= len(chunks) - 1:
            return chunks
        
        # ë‚¨ì€ ë°ì´í„° ìˆ˜ì§‘
        remaining_data = []
        for chunk in chunks[current_index + 1:]:
            remaining_data.extend(chunk)
        
        # ìƒˆë¡œìš´ í¬ê¸°ë¡œ ì¬ë¶„í• 
        new_chunks = [remaining_data[i:i+new_chunk_size] 
                     for i in range(0, len(remaining_data), new_chunk_size)]
        
        return chunks[:current_index + 1] + new_chunks

    def _process_chunk_results(self, chunk_results):
        """ì²­í¬ ê²°ê³¼ í›„ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ìµœì í™”"""
        processed_results = []
        
        for result in chunk_results:
            # âœ… GPT ë¶„ì„ì˜ ëª¨ë“  í•µì‹¬ í•„ë“œ ë³´ì¡´ (DB ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜)
            processed_result = {
                "ticker": result.get("ticker", "Unknown"),
                "score": safe_float_convert(result.get("score", 0), context="GPTë¶„ì„ result score"),
                "confidence": safe_float_convert(result.get("confidence", 0), context="GPTë¶„ì„ result confidence"),
                # âœ… DB ìŠ¤í‚¤ë§ˆì— í•„ìš”í•œ í•„ë“œë“¤ ì¶”ê°€ (trend_analysis í…Œì´ë¸”)
                "action": result.get("action", "HOLD"),
                "market_phase": result.get("market_phase", "Unknown"), 
                "pattern": result.get("pattern", ""),
                "reason": result.get("reason", ""),
                # ê¸°ì¡´ í•„ë“œë“¤ ìœ ì§€
                "input_type": result.get("analysis_method", result.get("input_type", "unknown")),
                "chart_path": result.get("chart_path", f"charts/{result.get('ticker', 'unknown')}.png")
            }
            
            # ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í° í•„ë“œ ì œê±°
            for key in ['raw_response', 'debug_info', 'intermediate_data']:
                if key in result:
                    del result[key]
            
            processed_results.append(processed_result)
        
        return processed_results

    def _generate_fallback_results(self, chunk):
        """ì‹¤íŒ¨í•œ ì²­í¬ì— ëŒ€í•œ ê¸°ë³¸ê°’ ìƒì„±"""
        fallback_results = []
        for ticker_data in chunk:
                    ticker = ticker_data.get("ticker", "Unknown")
                    fallback_results.append({
                        "ticker": ticker,
                        "score": 50.0,
                        "confidence": 0.30,
                # âœ… DB ìŠ¤í‚¤ë§ˆ í•„ë“œë“¤ ì¶”ê°€ (trend_analysis í…Œì´ë¸”)
                "action": "HOLD",
                "market_phase": "Unknown",
                "pattern": "",
                "reason": "ë¶„ì„ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ê°’",
                # ê¸°ì¡´ í•„ë“œë“¤ ìœ ì§€
                        "input_type": "chunk_error",
                        "chart_path": f"charts/{ticker}.png"
                    })
        return fallback_results

    def _handle_pipeline_error(self, stage_name, error):
        """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬"""
        logger.error(f"âŒ {stage_name} ë‹¨ê³„ ì‹¤íŒ¨: {error}")
        
        # ìŠ¤í…Œì´ì§€ë³„ ë³µêµ¬ ë¡œì§
        if stage_name == "market_data_collection":
            logger.info("ğŸ”„ ëŒ€ì²´ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì‹œë„ ì¤‘...")
            try:
                # ê¸°ë³¸ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´ ì‹œë„
                basic_tickers = ["KRW-BTC", "KRW-ETH", "KRW-XRP"]
                filtered_tickers, market_df, _ = self.update_all_tickers()
                if filtered_tickers:
                    logger.info(f"âœ… ëŒ€ì²´ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {len(filtered_tickers)}ê°œ í‹°ì»¤")
                    return True
            except Exception as fallback_error:
                logger.error(f"âŒ ëŒ€ì²´ ë°ì´í„° ìˆ˜ì§‘ë„ ì‹¤íŒ¨: {fallback_error}")
                
        elif stage_name == "filtering":
            logger.info("ğŸ”„ ê¸°ë³¸ í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ë³µêµ¬ ì‹œë„ ì¤‘...")
            try:
                # ìµœì†Œí•œì˜ í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ë³µêµ¬ ì‹œë„
                active_tickers = self.get_active_tickers_hybrid()
                if active_tickers:
                    logger.info(f"âœ… ê¸°ë³¸ í•„í„°ë§ ë³µêµ¬ ì„±ê³µ: {len(active_tickers)}ê°œ í‹°ì»¤")
                    return True
            except Exception as fallback_error:
                logger.error(f"âŒ ê¸°ë³¸ í•„í„°ë§ ë³µêµ¬ë„ ì‹¤íŒ¨: {fallback_error}")
                
        elif stage_name == "gpt_analysis":
            logger.info("ğŸ”„ GPT ë¶„ì„ ìš°íšŒ ì‹œë„ ì¤‘...")
            logger.warning("âš ï¸ GPT ë¶„ì„ ì—†ì´ ê¸°ë³¸ ê±°ë˜ ë¡œì§ìœ¼ë¡œ ì§„í–‰")
            return True
            
        return False

    def _perform_system_validation(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œ ë°ì´í„° ê²€ì¦ ìˆ˜í–‰"""
        try:
            logger.info("ğŸ” ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ ì‹œì‘...")
            
            # 1. static_indicators í…Œì´ë¸” ê²€ì¦
            try:
                self.validate_static_indicators_data()
                logger.info("âœ… static_indicators í…Œì´ë¸” ê²€ì¦ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ static_indicators í…Œì´ë¸” ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # 2. OHLCV ì •ë°€ë„ ê²€ì¦
            try:
                self.validate_ohlcv_precision()
                logger.info("âœ… OHLCV ì •ë°€ë„ ê²€ì¦ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ OHLCV ì •ë°€ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            logger.info("âœ… ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì‹œìŠ¤í…œ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    # get_total_balance í•¨ìˆ˜ëŠ” portfolio_manager.pyë¡œ í†µí•©ë¨
    # ìœ„ì˜ get_total_balance í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
    
    def _validate_market_conditions(self, ticker, current_price, score, confidence):
        """
        ì‹œì¥ ì¡°ê±´ ê²€ì¦ ë©”ì„œë“œ
        """
        try:
            validation_result = {
                'valid': True,
                'reason': 'ê²€ì¦ í†µê³¼'
            }
            
            # ê¸°ë³¸ ê²€ì¦ ì¡°ê±´ë“¤
            if current_price <= 0:
                validation_result['valid'] = False
                validation_result['reason'] = 'í˜„ì¬ê°€ê°€ 0 ì´í•˜ì…ë‹ˆë‹¤'
                return validation_result
            
            if score < 0:
                validation_result['valid'] = False
                validation_result['reason'] = 'ì ìˆ˜ê°€ ìŒìˆ˜ì…ë‹ˆë‹¤'
                return validation_result
            
            if confidence < 0 or confidence > 1:
                validation_result['valid'] = False
                validation_result['reason'] = 'ì‹ ë¢°ë„ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (0~1 ë²”ìœ„)'
                return validation_result
            
            # ì¶”ê°€ ê²€ì¦ ë¡œì§ì€ í•„ìš”ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥
            logger.debug(f"âœ… ì‹œì¥ ì¡°ê±´ ê²€ì¦ í†µê³¼: {ticker} | ê°€ê²©: {current_price} | ì ìˆ˜: {score} | ì‹ ë¢°ë„: {confidence}")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ì¡°ê±´ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'valid': False,
                'reason': f'ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            }
    
    def _analyze_buy_error(self, error_msg, ticker, current_price, trade_amount_krw):
        """
        ë§¤ìˆ˜ ì˜¤ë¥˜ ë¶„ì„ ë©”ì„œë“œ
        """
        try:
            analysis = "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
            error_lower = error_msg.lower()
            
            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
            if "insufficient" in error_lower or "ì”ì•¡" in error_msg:
                analysis = "ì”ì•¡ ë¶€ì¡±"
            elif "minimum" in error_lower or "ìµœì†Œ" in error_msg:
                analysis = "ìµœì†Œ ì£¼ë¬¸ ê¸ˆì•¡ ë¯¸ë‹¬"
            elif "market" in error_lower or "ë§ˆì¼“" in error_msg:
                analysis = "ë§ˆì¼“ ìƒíƒœ ë¬¸ì œ"
            elif "api" in error_lower:
                analysis = "API í˜¸ì¶œ ì˜¤ë¥˜"
            elif "timeout" in error_lower:
                analysis = "íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜"
            elif "rate" in error_lower or "ì œí•œ" in error_msg:
                analysis = "API í˜¸ì¶œ ì œí•œ"
            else:
                analysis = f"ê¸°íƒ€ ì˜¤ë¥˜: {error_msg[:50]}"
            
            logger.debug(f"ğŸ” ë§¤ìˆ˜ ì˜¤ë¥˜ ë¶„ì„: {ticker} | ì˜¤ë¥˜: {error_msg} | ë¶„ì„: {analysis}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ ë§¤ìˆ˜ ì˜¤ë¥˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì˜¤ë¥˜ ë¶„ì„ ì‹¤íŒ¨"

    def calculate_kelly_position_size(self, ticker: str, score: float, confidence: float, 
                                    current_price: float, total_balance: float) -> dict:
        """
        ì¼ˆë¦¬ ê³µì‹ ê¸°ë°˜ í¬ì§€ì…˜ ì‚¬ì´ì§• ê³„ì‚°
        
        Args:
            ticker: í‹°ì»¤ ì‹¬ë³¼
            score: GPT ë¶„ì„ ì ìˆ˜
            confidence: GPT ë¶„ì„ ì‹ ë¢°ë„
            current_price: í˜„ì¬ê°€
            total_balance: ì´ ìì‚°
            
        Returns:
            dict: ì¼ˆë¦¬ ê¸°ë°˜ í¬ì§€ì…˜ ì‚¬ì´ì§• ê²°ê³¼
        """
        try:
            # 1. ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì¡°íšŒ (ATR, ì§€ì§€/ì €í•­ì„  ë“±)
            market_data = self._get_market_data_for_kelly(ticker)
            if not market_data:
                logger.warning(f"âš ï¸ {ticker} ì¼ˆë¦¬ ê³„ì‚°ì„ ìœ„í•œ ì‹œì¥ ë°ì´í„° ì—†ìŒ")
                return self._get_default_kelly_result(total_balance)
            
            # 2. ATR ê¸°ë°˜ ë¦¬ìŠ¤í¬ ê³„ì‚°
            atr = market_data.get('atr', 0)
            if atr <= 0:
                logger.warning(f"âš ï¸ {ticker} ATR ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {atr}")
                return self._get_default_kelly_result(total_balance)
            
            # 3. ì†ì ˆê°€ ë° ëª©í‘œê°€ ê³„ì‚°
            stop_loss = current_price - (atr * 2.5)  # 2.5x ATR ì†ì ˆ
            target_price = current_price + (atr * 4.0)  # 4.0x ATR ëª©í‘œ (ë¦¬ìŠ¤í¬ ëŒ€ë¹„ 1.6:1)
            
            # 4. ìŠ¹ë¥  ì¶”ì • (ì ìˆ˜ì™€ ì‹ ë¢°ë„ ê¸°ë°˜)
            # ì ìˆ˜ 50ì  ê¸°ì¤€ìœ¼ë¡œ ìŠ¹ë¥  ì¶”ì • (40-80% ë²”ìœ„)
            base_win_rate = 0.4 + (score / 100.0) * 0.4  # 40-80% ë²”ìœ„
            # ì‹ ë¢°ë„ë¡œ ìŠ¹ë¥  ì¡°ì •
            estimated_win_rate = base_win_rate * confidence
            estimated_win_rate = max(0.3, min(estimated_win_rate, 0.8))  # 30-80% ë²”ìœ„
            
            # 5. í‰ê·  ìˆ˜ìµ/ì†ì‹¤ ë¹„ìœ¨ ê³„ì‚°
            avg_win = (target_price - current_price) / current_price
            avg_loss = (current_price - stop_loss) / current_price
            
            # 6. ì¼ˆë¦¬ ê³µì‹ ì ìš©: f = (bp - q) / b
            # b = ìŠ¹ë¦¬ì‹œ ìˆ˜ìµë¥ , p = ìŠ¹ë¥ , q = íŒ¨ë°° í™•ë¥ 
            if avg_loss > 0 and avg_win > 0:
                kelly_fraction = (avg_win * estimated_win_rate - (1 - estimated_win_rate)) / avg_win
                # ì¼ˆë¦¬ ë¹„ìœ¨ì„ 0-25% ë²”ìœ„ë¡œ ì œí•œ (ë³´ìˆ˜ì  ì ‘ê·¼)
                kelly_fraction = max(0, min(kelly_fraction, 0.25))
            else:
                kelly_fraction = 0.01  # ê¸°ë³¸ê°’
            
            # 7. ATR ê¸°ë°˜ ë³€ë™ì„± ì¡°ì • (ê°•í™”ëœ ë¡œì§)
            atr_ratio = atr / current_price
            
            # ë³€ë™ì„±ì— ë”°ë¥¸ í¬ì§€ì…˜ í¬ê¸° ì¡°ì • (ë” ì„¸ë°€í•œ ì¡°ì •)
            if atr_ratio > 0.05:  # 5% ì´ìƒ ë³€ë™ì„± (ê³ ë³€ë™ì„±)
                volatility_adjustment = 0.5  # 50% ì¶•ì†Œ
            elif atr_ratio > 0.03:  # 3-5% ë³€ë™ì„± (ì¤‘ë³€ë™ì„±)
                volatility_adjustment = 0.7  # 30% ì¶•ì†Œ
            elif atr_ratio > 0.02:  # 2-3% ë³€ë™ì„± (ì €ë³€ë™ì„±)
                volatility_adjustment = 0.9  # 10% ì¶•ì†Œ
            elif atr_ratio > 0.01:  # 1-2% ë³€ë™ì„± (ë§¤ìš° ë‚®ì€ ë³€ë™ì„±)
                volatility_adjustment = 1.1  # 10% ì¦ê°€
            else:  # 1% ë¯¸ë§Œ ë³€ë™ì„± (ê·¹íˆ ë‚®ì€ ë³€ë™ì„±)
                volatility_adjustment = 1.3  # 30% ì¦ê°€
            
            # 8. ìµœì¢… í¬ì§€ì…˜ í¬ê¸° ê³„ì‚°
            final_position_size = kelly_fraction * volatility_adjustment * confidence
            final_position_size = max(0.005, min(final_position_size, 0.15))  # 0.5-15% ë²”ìœ„
            
            # 9. ì‹¤ì œ ë§¤ìˆ˜ ê¸ˆì•¡ ê³„ì‚°
            position_amount_krw = total_balance * final_position_size
            
            # 10. ìµœì†Œ/ìµœëŒ€ ê¸ˆì•¡ ì œí•œ
            from utils import MIN_KRW_ORDER, TAKER_FEE_RATE
            min_amount_with_fee = MIN_KRW_ORDER / (1 + TAKER_FEE_RATE)
            max_amount = min(200000, total_balance * 0.05)  # ìµœëŒ€ 20ë§Œì› ë˜ëŠ” ì´ ìì‚°ì˜ 5%
            
            position_amount_krw = max(min_amount_with_fee, min(position_amount_krw, max_amount))
            
            # 11. ìˆ˜ìˆ˜ë£Œë¥¼ í¬í•¨í•œ ì‹¤ì œ ì£¼ë¬¸ ê¸ˆì•¡
            actual_order_amount = position_amount_krw * (1 + TAKER_FEE_RATE)
            
            logger.info(f"ğŸ’° {ticker} ì¼ˆë¦¬ ê³µì‹ ê³„ì‚° ì™„ë£Œ:")
            logger.info(f"   - ì¼ˆë¦¬ ë¹„ìœ¨: {kelly_fraction:.3f} ({kelly_fraction*100:.1f}%)")
            logger.info(f"   - ATR ë¹„ìœ¨: {atr_ratio:.2%} (ë³€ë™ì„±)")
            logger.info(f"   - ë³€ë™ì„± ì¡°ì •: {volatility_adjustment:.3f}")
            logger.info(f"   - ìµœì¢… í¬ì§€ì…˜: {final_position_size:.3f} ({final_position_size*100:.1f}%)")
            logger.info(f"   - ì˜ˆìƒ ìŠ¹ë¥ : {estimated_win_rate:.1%}")
            logger.info(f"   - ë¦¬ìŠ¤í¬/ë¦¬ì›Œë“œ: 1:{avg_win/avg_loss:.2f}")
            logger.info(f"   - ë§¤ìˆ˜ ê¸ˆì•¡: {position_amount_krw:,.0f}ì›")
            
            return {
                'position_amount_krw': position_amount_krw,
                'actual_order_amount': actual_order_amount,
                'kelly_fraction': kelly_fraction,
                'volatility_adjustment': volatility_adjustment,
                'final_position_size': final_position_size,
                'estimated_win_rate': estimated_win_rate,
                'risk_reward_ratio': avg_win / avg_loss if avg_loss > 0 else 0,
                'stop_loss': stop_loss,
                'target_price': target_price,
                'atr': atr
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ì¼ˆë¦¬ ê³µì‹ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_default_kelly_result(total_balance)
    
    def _get_market_data_for_kelly(self, ticker: str) -> dict:
        """ì¼ˆë¦¬ ê³„ì‚°ì„ ìœ„í•œ ì‹œì¥ ë°ì´í„° ì¡°íšŒ"""
        try:
            # static_indicatorsì—ì„œ ATR ë° ê¸°íƒ€ ì§€í‘œ ì¡°íšŒ
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT atr, adx, price, high_60, low_60
                    FROM static_indicators 
                    WHERE ticker = %s
                """, (ticker,))
                
                result = cursor.fetchone()
                if result:
                    atr, adx, price, high_60, low_60 = result
                    return {
                        'atr': atr or 0,
                        'adx': adx or 25,
        
                        'price': price or 0,
                        'high_60': high_60 or 0,
                        'low_60': low_60 or 0
                    }
                else:
                    logger.warning(f"âš ï¸ {ticker} static_indicators ë°ì´í„° ì—†ìŒ")
                return None
                
        except Exception as e:
            logger.error(f"âŒ {ticker} ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None

    def _get_default_kelly_result(self, total_balance: float) -> dict:
        """ê¸°ë³¸ ì¼ˆë¦¬ ê³„ì‚° ê²°ê³¼ (ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)"""
        from utils import MIN_KRW_ORDER, TAKER_FEE_RATE
        
        min_amount_with_fee = MIN_KRW_ORDER / (1 + TAKER_FEE_RATE)
        actual_order_amount = min_amount_with_fee * (1 + TAKER_FEE_RATE)
        
        return {
            'position_amount_krw': min_amount_with_fee,
            'actual_order_amount': actual_order_amount,
            'kelly_fraction': 0.01,
            'volatility_adjustment': 1.0,
            'final_position_size': 0.01,
            'estimated_win_rate': 0.5,
            'risk_reward_ratio': 1.0,
            'stop_loss': 0,
            'target_price': 0,
            'atr': 0
        }
    
    def get_technical_data_batch(self, tickers: list) -> dict:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í‹°ì»¤ì˜ ê¸°ìˆ ì  ì§€í‘œë¥¼ í•œ ë²ˆì— ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)"""
        import time
        start_time = time.time()
        logger.info(f"ğŸ“Š ë°°ì¹˜ ê¸°ìˆ ì  ì§€í‘œ ì¡°íšŒ ì‹œì‘: {len(tickers)}ê°œ í‹°ì»¤")
        
        if not tickers:
            return {}
            
        try:
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                # ë‹¨ì¼ ë°°ì¹˜ ì¿¼ë¦¬ë¡œ ëª¨ë“  í‹°ì»¤ì˜ ë°ì´í„° ì¡°íšŒ
                placeholders = ','.join(['%s'] * len(tickers))
                
                batch_query = f"""
                    SELECT 
                        s.ticker,
                        s.price, s.atr, s.adx, s.volume_change_7_30, s.supertrend_signal,
                        o.close, o.rsi_14, o.ma_50, o.ma_200, o.bb_upper, o.bb_lower
                    FROM static_indicators s
                    LEFT JOIN LATERAL (
                        SELECT close, rsi_14, ma_50, ma_200, bb_upper, bb_lower
                        FROM ohlcv 
                        WHERE ticker = s.ticker 
                        ORDER BY date DESC 
                        LIMIT 1
                    ) o ON true
                    WHERE s.ticker IN ({placeholders})
                """
                
                cursor.execute(batch_query, tickers)
                results = cursor.fetchall()
                
                # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                batch_data = {}
                for row in results:
                    ticker = row[0]
                    s_price, s_atr, s_adx, s_volume_change, s_supertrend = row[1:6]
                    o_close, o_rsi, o_ma50, o_ma200, o_bb_upper, o_bb_lower = row[6:]
                    
                    # ê¸°ë³¸ê°’ ì„¤ì •
                    price = float(s_price or 0)
                    if price == 0 and o_close:
                        price = float(o_close)
                    
                    batch_data[ticker] = {
                        'price': price,
                        'rsi_14': float(o_rsi or 50),
                        'ma_50': float(o_ma50 or 0),
                        'ma_200': float(o_ma200 or 0),
                        'bb_upper': float(o_bb_upper or 0),
                        'bb_lower': float(o_bb_lower or 0),
                        'atr': float(s_atr or 0),
                        'adx': float(s_adx or 25),
                        'volume_change_7_30': float(s_volume_change or 0),
                        'supertrend_signal': s_supertrend or 'neutral'
                    }
                
                execution_time = time.time() - start_time
                query_count_saved = len(tickers) * 2 - 1  # ê¸°ì¡´: í‹°ì»¤ë‹¹ 2ì¿¼ë¦¬, ìµœì í™”: 1ì¿¼ë¦¬
                logger.info(f"âœ… ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: {len(batch_data)}ê°œ í‹°ì»¤ ({execution_time:.2f}ì´ˆ)")
                logger.info(f"ğŸ’° DB ì¿¼ë¦¬ ìµœì í™”: {query_count_saved}ê°œ ì¿¼ë¦¬ ì ˆì•½ ({query_count_saved/len(tickers)*2*100:.0f}% ê°ì†Œ)")
                return batch_data
                
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"âŒ ë°°ì¹˜ ê¸°ìˆ ì  ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)} (ì†Œìš”ì‹œê°„: {execution_time:.2f}ì´ˆ)")
            return {}
    
    def _get_default_technical_data(self) -> dict:
        """ê¸°ë³¸ ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° (ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)"""
        return {
            'rsi_14': 50,
            'macd_signal': 'neutral',
            'ma_alignment': 'neutral',
            'bb_upper': 0,
            'bb_lower': 0,
            'trend_strength': 0.3,
            'volume_momentum': 0.5,
            'adx': 25,
            'price': 0
        }
    
    def _get_technical_data_for_integration(self, ticker: str) -> dict:
        """í†µí•© í¬ì§€ì…˜ ì‚¬ì´ì§•ì„ ìœ„í•œ ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì¡°íšŒ (ìˆ˜ì •ë¨)"""
        try:
            # ğŸ”§ [ìˆ˜ì •] ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ìœ¼ë¡œ ì¿¼ë¦¬ ë³€ê²½
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                # ìµœì í™”ëœ ë‹¨ì¼ JOIN ì¿¼ë¦¬ë¡œ ëª¨ë“  ì§€í‘œ í•œë²ˆì— ì¡°íšŒ
                cursor.execute("""
                    SELECT 
                        s.price, s.atr, s.adx, s.volume_change_7_30, s.supertrend_signal,
                        o.close, o.rsi_14, o.ma_50, o.ma_200, o.bb_upper, o.bb_lower
                    FROM static_indicators s
                    LEFT JOIN (
                        SELECT ticker, close, rsi_14, ma_50, ma_200, bb_upper, bb_lower
                        FROM ohlcv 
                        WHERE ticker = %s 
                        ORDER BY date DESC 
                        LIMIT 1
                    ) o ON s.ticker = o.ticker
                    WHERE s.ticker = %s
                """, (ticker, ticker))
                
                combined_result = cursor.fetchone()
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                price = 0
                rsi_14 = 50
                ma_50 = 0
                ma_200 = 0
                bb_upper = 0
                bb_lower = 0
                atr = 0
                adx = 25
                volume_change_7_30 = 0
                supertrend_signal = 'neutral'
                
                # ìµœì í™”ëœ í†µí•© ê²°ê³¼ ì²˜ë¦¬ (2ê°œ ì¿¼ë¦¬ â†’ 1ê°œ ì¿¼ë¦¬ë¡œ 50% ê°ì†Œ)
                if combined_result:
                    s_price, s_atr, s_adx, s_volume_change, s_supertrend, o_close, o_rsi, o_ma50, o_ma200, o_bb_upper, o_bb_lower = combined_result
                    
                    # static_indicators ë°ì´í„°
                    price = float(s_price or 0)
                    atr = float(s_atr or 0)
                    adx = float(s_adx or 25)
                    volume_change_7_30 = float(s_volume_change or 0)
                    supertrend_signal = s_supertrend or 'neutral'
                    
                    # ohlcv ë°ì´í„° ì²˜ë¦¬
                    if o_close:
                        if price == 0:  # static_indicatorsì— priceê°€ ì—†ìœ¼ë©´ ohlcvì˜ close ì‚¬ìš©
                            price = float(o_close)
                        rsi_14 = float(o_rsi or 50)
                        ma_50 = float(o_ma50 or 0)
                        ma_200 = float(o_ma200 or 0)
                        bb_upper = float(o_bb_upper or 0)
                        bb_lower = float(o_bb_lower or 0)
                
                # MACD ì‹ í˜¸ íŒë‹¨ (ohlcvì— macd ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©)
                macd_signal_type = 'neutral'
                
                # ì´ë™í‰ê·  ì •ë ¬ íŒë‹¨
                if price and ma_50 and ma_200 and price > 0 and ma_50 > 0 and ma_200 > 0:
                    if price > ma_50 > ma_200:
                        ma_alignment = 'bullish'
                    elif price < ma_50 < ma_200:
                        ma_alignment = 'bearish'
                    else:
                        ma_alignment = 'neutral'
                else:
                    ma_alignment = 'neutral'
                
                # ADX ê¸°ë°˜ ì¶”ì„¸ ê°•ë„ íŒë‹¨
                if adx and adx > 0:
                    if adx > 25:
                        trend_strength = min(adx / 50.0, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    else:
                        trend_strength = 0.3  # ì•½í•œ ì¶”ì„¸
                else:
                    trend_strength = 0.3
                
                # ë³¼ë¥¨ ë³€í™” ê¸°ë°˜ ëª¨ë©˜í…€ íŒë‹¨
                if volume_change_7_30 and volume_change_7_30 > 0:
                    volume_momentum = min(volume_change_7_30 / 100.0, 1.0)  # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                else:
                    volume_momentum = 0.5
                
                return {
                    'rsi_14': rsi_14 or 50,
                    'macd_signal': macd_signal_type,
                    'ma_alignment': ma_alignment,
                    'bb_upper': bb_upper or 0,
                    'bb_lower': bb_lower or 0,
                    'trend_strength': trend_strength,
                    'volume_momentum': volume_momentum,
                    'adx': adx or 25,
                    'price': price or 0
                }
                    
        except Exception as e:
            logger.error(f"âŒ {ticker} ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return self._get_default_technical_data()
    
    def _get_market_conditions_for_integration(self) -> dict:
        """í†µí•© í¬ì§€ì…˜ ì‚¬ì´ì§•ì„ ìœ„í•œ ì‹œì¥ ìƒí™© ë°ì´í„° ì¡°íšŒ (í™•ì¥)"""
        try:
            # ì‹œì¥ ì „ì²´ ìƒí™© ë¶„ì„
            with self.get_db_connection_safe() as conn:
                cursor = conn.cursor()
                
                # 1. ì „ì²´ ì‹œì¥ ë³€ë™ì„± ë¶„ì„ (ATR ê¸°ë°˜)
                cursor.execute("""
                    SELECT AVG(atr) as avg_atr, STDDEV(atr) as atr_std
                    FROM static_indicators 
                    WHERE atr IS NOT NULL AND atr > 0
                """)
                atr_result = cursor.fetchone()
                
                if atr_result and atr_result[0]:
                    avg_atr, atr_std = atr_result
                    if atr_std and atr_std > 0:
                        # í˜„ì¬ ATRì´ í‰ê·  ëŒ€ë¹„ ì–´ëŠ ì •ë„ì¸ì§€ ê³„ì‚°
                        atr_z_score = (avg_atr - atr_std) / atr_std if atr_std > 0 else 0
                        if atr_z_score > 1.5:
                            market_volatility = 'high'
                        elif atr_z_score < -1.5:
                            market_volatility = 'low'
                        else:
                            market_volatility = 'normal'
                    else:
                        market_volatility = 'normal'
                else:
                    market_volatility = 'normal'
                
                # 2. ì „ì²´ ì‹œì¥ ì¶”ì„¸ ê°•ë„ ë¶„ì„ (MA200 ê¸°ìš¸ê¸° ê¸°ë°˜)
                cursor.execute("""
                    SELECT 
                           COUNT(*) as total_count,
                           COUNT(CASE WHEN adx > 20 THEN 1 END) as strong_trend_count,
                           COUNT(CASE WHEN adx <= 20 THEN 1 END) as weak_trend_count
                    FROM static_indicators 
                    WHERE adx IS NOT NULL
                """)
                trend_result = cursor.fetchone()
                
                if trend_result and trend_result[0] is not None:
                    avg_slope, positive_count, total_count = trend_result
                    if total_count > 0:
                        # ìƒìŠ¹ ì¢…ëª© ë¹„ìœ¨
                        bullish_ratio = positive_count / total_count
                        # ì¶”ì„¸ ê°•ë„ (0-1 ë²”ìœ„)
                        trend_strength = min(max(bullish_ratio, 0.3), 0.8)
                    else:
                        trend_strength = 0.5
                else:
                    trend_strength = 0.5
                
                # 3. ì‹œì¥ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (ADX ê¸°ë°˜)
                cursor.execute("""
                    SELECT AVG(adx) as avg_adx
                    FROM static_indicators 
                    WHERE adx IS NOT NULL AND adx > 0
                """)
                adx_result = cursor.fetchone()
                
                if adx_result and adx_result[0]:
                    avg_adx = adx_result[0]
                    if avg_adx > 30:
                        market_sentiment = 'strong_trend'
                    elif avg_adx > 20:
                        market_sentiment = 'moderate_trend'
                    else:
                        market_sentiment = 'weak_trend'
                else:
                    market_sentiment = 'neutral'
                
                # 4. ê±°ë˜ëŸ‰ íŠ¸ë Œë“œ ë¶„ì„
                cursor.execute("""
                    SELECT AVG(volume_change_7_30) as avg_volume_change
                    FROM static_indicators 
                    WHERE volume_change_7_30 IS NOT NULL
                """)
                volume_result = cursor.fetchone()
                
                if volume_result and volume_result[0]:
                    avg_volume_change = volume_result[0]
                    if avg_volume_change > 50:
                        volume_trend = 'high'
                    elif avg_volume_change < -20:
                        volume_trend = 'low'
                    else:
                        volume_trend = 'normal'
                else:
                    volume_trend = 'normal'
                
                return {
                    'market_volatility': market_volatility,
                    'trend_strength': round(trend_strength, 3),
                    'market_sentiment': market_sentiment,
                    'volume_trend': volume_trend,
                    'avg_atr': round(avg_atr, 4) if atr_result and atr_result[0] else 0.02,
                    'bullish_ratio': round(bullish_ratio, 3) if 'bullish_ratio' in locals() else 0.5,
                    'avg_adx': round(avg_adx, 1) if adx_result and adx_result[0] else 25,
                    'avg_volume_change': round(avg_volume_change, 1) if volume_result and volume_result[0] else 0
                }
                
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ìƒí™© ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                'market_volatility': 'normal',
                'trend_strength': 0.5,
                'market_sentiment': 'neutral',
                'volume_trend': 'normal',
                'avg_atr': 0.02,
                'bullish_ratio': 0.5,
                'avg_adx': 25,
                'avg_volume_change': 0
            }

    def get_ohlcv_from_db(self, ticker: str, limit: int = 450) -> pd.DataFrame:
        """DBì—ì„œ OHLCV ë°ì´í„°ë¥¼ ìµœê·¼ ë‚ ì§œìˆœìœ¼ë¡œ ì •í™•íˆ ì¡°íšŒ"""
        try:
            with self.get_db_connection_safe() as conn:
                # ìµœê·¼ ë°ì´í„°ë¶€í„° ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¡°íšŒ í›„ ë‹¤ì‹œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
                query = """
                SELECT date, open, high, low, close, volume
                FROM (
                    SELECT date, open, high, low, close, volume
                    FROM ohlcv 
                    WHERE ticker = %s 
                    ORDER BY date DESC 
                    LIMIT %s
                ) subquery
                ORDER BY date ASC
                """
                
                df = pd.read_sql_query(query, conn, params=[ticker, limit])
                
                if not df.empty:
                    df['date'] = pd.to_datetime(df['date'])
                    df.set_index('date', inplace=True)
                    
                    # ê²€ì¦ ë¡œê·¸
                    from utils import safe_strftime
                    start_date = safe_strftime(df.index[0])
                    end_date = safe_strftime(df.index[-1])
                    logger.debug(f"ğŸ” {ticker} DB ì¡°íšŒ ì™„ë£Œ: {len(df)}ê°œ ({start_date} ~ {end_date})")
                    
                return df
                
        except Exception as e:
            logger.error(f"âŒ {ticker} DB ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    def _check_pyramiding_for_existing_position(self, ticker: str, score: float, confidence: float) -> dict:
        """
        ê¸°ì¡´ ë³´ìœ  ì¢…ëª©ì— ëŒ€í•œ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            score (float): GPT ë¶„ì„ ì ìˆ˜
            confidence (float): GPT ë¶„ì„ ì‹ ë¢°ë„
            
        Returns:
            dict: í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ê²°ê³¼
        """
        try:
            # í˜„ì¬ê°€ ì¡°íšŒ ë¨¼ì € ìˆ˜í–‰
            current_price = get_current_price_safe(ticker)
            if not current_price:
                return {
                    'should_pyramid': False,
                    'reason': 'í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨',
                    'type': 'price_fetch_failed'
                }
            
            # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ìƒì„¸ ë¶„ì„ì„ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ ì‹¤ì œ ì¡°ê±´ í™•ì¸
            pyramid_analysis = self._analyze_pyramiding_conditions(ticker, current_price, score, confidence)
            
            # ë¶„ì„ ê²°ê³¼ì—ì„œ ì¶©ì¡±ëœ ì¡°ê±´ ê°œìˆ˜ í™•ì¸
            met_conditions_count = pyramid_analysis.get('met_conditions', 0)
            total_conditions = pyramid_analysis.get('total_conditions', 4)
            
            # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ì¡°ê±´ ì¶©ì¡± íŒë‹¨ì„ reason í…ìŠ¤íŠ¸ë¡œë„ í™•ì¸
            reason_text = pyramid_analysis.get('reason', '')
            conditions_met_by_reason = "í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì¶©ì¡±:" in reason_text
            
            # ğŸ”§ [ìˆ˜ì •] ìµœì†Œ 2ê°œ ì¡°ê±´ ì¶©ì¡± ì‹œ í”¼ë¼ë¯¸ë”© ì‹¤í–‰ ê²°ì •
            should_pyramid_by_analysis = met_conditions_count >= 2 or conditions_met_by_reason
            
            logger.info(f"ğŸ” {ticker} ì¡°ê±´ ë¶„ì„ ìƒì„¸: ì¶©ì¡±={met_conditions_count}, ì „ì²´={total_conditions}, reason_check={conditions_met_by_reason}, ì‹¤í–‰ê²°ì •={should_pyramid_by_analysis}")
            logger.info(f"ğŸ” {ticker} ë¶„ì„ ì‚¬ìœ : {pyramid_analysis.get('reason', 'Unknown')}")
            
            if should_pyramid_by_analysis:
                # ì¡°ê±´ì´ ì¶©ì¡±ë˜ì—ˆìœ¼ë©´ í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ë‹ˆì €ë¥¼ í†µí•´ ì‹¤ì œ ì‹¤í–‰
                logger.info(f"âœ… {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¶„ì„ ê²°ê³¼: ì¡°ê±´ ì¶©ì¡± í™•ì¸ë¨")
                
                # ğŸ”§ [ìˆ˜ì •] ì‹¤ì œ í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì‹œë„ - ìë™ ì‹¤í–‰ ë¨¼ì € ì‹œë„
                pyramid_executed = self.pm.check_pyramiding(ticker)
                
                if pyramid_executed:
                    return {
                        'should_pyramid': True,
                        'reason': f'í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì¶©ì¡± ë° ìë™ ì‹¤í–‰ ì™„ë£Œ: {pyramid_analysis["reason"]}',
                        'type': 'pyramiding_executed',
                        'details': pyramid_analysis
                    }
                else:
                    # ğŸ”§ [í•µì‹¬ ìˆ˜ì •] ìë™ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œì—ë„ ì¡°ê±´ì€ ì¶©ì¡±ë˜ì—ˆìœ¼ë¯€ë¡œ ìˆ˜ë™ ì‹¤í–‰ ì§„í–‰
                    logger.warning(f"âš ï¸ {ticker} í”¼ë¼ë¯¸ë”© ìë™ ì‹¤í–‰ ì‹¤íŒ¨, í•˜ì§€ë§Œ ì¡°ê±´ ì¶©ì¡±ìœ¼ë¡œ ìˆ˜ë™ ì‹¤í–‰ ì§„í–‰")
                    return {
                        'should_pyramid': True,
                        'reason': f'í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì¶©ì¡± (ìˆ˜ë™ ì‹¤í–‰): {pyramid_analysis["reason"]}',
                        'type': 'manual_execution_needed',
                        'details': pyramid_analysis
                    }
            else:
                # ì¡°ê±´ ë¯¸ì¶©ì¡±
                logger.debug(f"ğŸ“Š {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡±: {met_conditions_count}/{total_conditions}ê°œ ì¡°ê±´ë§Œ ì¶©ì¡±")
                return {
                    'should_pyramid': False,
                    'reason': pyramid_analysis['reason'],
                    'type': 'conditions_not_met',
                    'details': pyramid_analysis
                }
                
        except Exception as e:
            logger.error(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'should_pyramid': False,
                'reason': f'í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€ ì‹¤íŒ¨: {str(e)}',
                'type': 'error'
            }

    def _analyze_pyramiding_conditions(self, ticker: str, current_price: float, score: float, confidence: float) -> dict:
        """
        í”¼ë¼ë¯¸ë”© ì¡°ê±´ ìƒì„¸ ë¶„ì„
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            current_price (float): í˜„ì¬ê°€
            score (float): GPT ë¶„ì„ ì ìˆ˜
            confidence (float): GPT ë¶„ì„ ì‹ ë¢°ë„
            
        Returns:
            dict: í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¶„ì„ ê²°ê³¼
        """
        try:
            # í˜„ì¬ ë³´ìœ  í¬ì§€ì…˜ ì •ë³´ ì¡°íšŒ
            current_positions = self.pm.get_current_positions()
            position_info = next((pos for pos in current_positions if pos['ticker'] == ticker), None)
            
            if not position_info:
                return {
                    'reason': 'ë³´ìœ  í¬ì§€ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨',
                    'conditions_checked': []
                }
            
            avg_price = position_info.get('avg_buy_price', 0)
            current_value = position_info.get('value', 0)
            
            # í”¼ë¼ë¯¸ë”© ì¡°ê±´ë“¤ ì ê²€
            conditions_checked = []
            
            # 1. ì €í•­ì„  ëŒíŒŒ ì¡°ê±´
            resistance_breakout = self._check_resistance_breakout(ticker, current_price)
            conditions_checked.append({
                'condition': 'ì €í•­ì„  ëŒíŒŒ',
                'met': resistance_breakout['met'],
                'details': resistance_breakout['details']
            })
            
            # 2. ì „ê³ ì  ëŒíŒŒ ì¡°ê±´
            high_breakout = self._check_high_breakout(ticker, current_price)
            conditions_checked.append({
                'condition': 'ì „ê³ ì  ëŒíŒŒ',
                'met': high_breakout['met'],
                'details': high_breakout['details']
            })
            
            # 3. ê¸°ìˆ ì  ì§€í‘œ ì¡°ê±´
            technical_conditions = self._check_technical_pyramiding_conditions(ticker, current_price)
            conditions_checked.append({
                'condition': 'ê¸°ìˆ ì  ì§€í‘œ',
                'met': technical_conditions['met'],
                'details': technical_conditions['details']
            })
            
            # 4. ìˆ˜ìµë¥  ì¡°ê±´
            return_rate = ((current_price - avg_price) / avg_price * 100) if avg_price > 0 else 0
            profit_condition = return_rate >= 5.0  # 5% ì´ìƒ ìˆ˜ìµ ì‹œì—ë§Œ í”¼ë¼ë¯¸ë”© ê³ ë ¤
            conditions_checked.append({
                'condition': 'ìˆ˜ìµë¥  ì¡°ê±´',
                'met': profit_condition,
                'details': f'í˜„ì¬ ìˆ˜ìµë¥ : {return_rate:.1f}% (ê¸°ì¤€: 5.0%)'
            })
            
            # ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€ íŒë‹¨
            met_conditions = [c for c in conditions_checked if c['met']]
            unmet_conditions = [c for c in conditions_checked if not c['met']]
            
            if len(met_conditions) >= 2:  # ìµœì†Œ 2ê°œ ì¡°ê±´ ì¶©ì¡± ì‹œ í”¼ë¼ë¯¸ë”© ê³ ë ¤
                reason = f"í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì¶©ì¡±: {', '.join([c['condition'] for c in met_conditions])}"
            else:
                reason = f"í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¯¸ì¶©ì¡±: {', '.join([c['condition'] for c in unmet_conditions])}"
            
            return {
                'reason': reason,
                'conditions_checked': conditions_checked,
                'met_conditions': len(met_conditions),
                'total_conditions': len(conditions_checked)
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'reason': f'í”¼ë¼ë¯¸ë”© ì¡°ê±´ ë¶„ì„ ì‹¤íŒ¨: {str(e)}',
                'conditions_checked': [],
                'met_conditions': 0,
                'total_conditions': 4
            }

    def _check_resistance_breakout(self, ticker: str, current_price: float) -> dict:
        """
        ì €í•­ì„  ëŒíŒŒ ì¡°ê±´ ì ê²€
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            current_price (float): í˜„ì¬ê°€
            
        Returns:
            dict: ì €í•­ì„  ëŒíŒŒ ì¡°ê±´ ì ê²€ ê²°ê³¼
        """
        try:
            # ìµœê·¼ OHLCV ë°ì´í„° ì¡°íšŒ
            ohlcv_data = self.get_ohlcv_from_db(ticker, limit=50)
            if ohlcv_data is None or ohlcv_data.empty:
                return {
                    'met': False,
                    'details': 'OHLCV ë°ì´í„° ì—†ìŒ'
                }
            
            # ìµœê·¼ ê³ ì ë“¤ ê³„ì‚° (20ì¼ ê¸°ì¤€)
            recent_highs = ohlcv_data['high'].rolling(window=20).max()
            current_high = recent_highs.iloc[-1]
            
            # ì €í•­ì„  ëŒíŒŒ ì—¬ë¶€ (í˜„ì¬ê°€ê°€ ìµœê·¼ ê³ ì ì„ 1% ì´ìƒ ëŒíŒŒ)
            breakout_threshold = current_high * 1.01
            is_breakout = current_price > breakout_threshold
            
            return {
                'met': is_breakout,
                'details': f'í˜„ì¬ê°€: {current_price:,.0f}ì›, ì €í•­ì„ : {breakout_threshold:,.0f}ì›, ëŒíŒŒ: {is_breakout}'
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ì €í•­ì„  ëŒíŒŒ ì¡°ê±´ ì ê²€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'met': False,
                'details': f'ì ê²€ ì‹¤íŒ¨: {str(e)}'
            }

    def _check_high_breakout(self, ticker: str, current_price: float) -> dict:
        """
        ì „ê³ ì  ëŒíŒŒ ì¡°ê±´ ì ê²€
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            current_price (float): í˜„ì¬ê°€
            
        Returns:
            dict: ì „ê³ ì  ëŒíŒŒ ì¡°ê±´ ì ê²€ ê²°ê³¼
        """
        try:
            # ìµœê·¼ OHLCV ë°ì´í„° ì¡°íšŒ (ë” ê¸´ ê¸°ê°„)
            ohlcv_data = self.get_ohlcv_from_db(ticker, limit=100)
            if ohlcv_data is None or ohlcv_data.empty:
                return {
                    'met': False,
                    'details': 'OHLCV ë°ì´í„° ì—†ìŒ'
                }
            
            # ìµœê·¼ 100ì¼ ê³ ì 
            historical_high = ohlcv_data['high'].max()
            
            # ì „ê³ ì  ëŒíŒŒ ì—¬ë¶€ (í˜„ì¬ê°€ê°€ ì „ê³ ì ì„ 0.5% ì´ìƒ ëŒíŒŒ)
            breakout_threshold = historical_high * 1.005
            is_breakout = current_price > breakout_threshold
            
            return {
                'met': is_breakout,
                'details': f'í˜„ì¬ê°€: {current_price:,.0f}ì›, ì „ê³ ì : {historical_high:,.0f}ì›, ëŒíŒŒ: {is_breakout}'
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ì „ê³ ì  ëŒíŒŒ ì¡°ê±´ ì ê²€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'met': False,
                'details': f'ì ê²€ ì‹¤íŒ¨: {str(e)}'
            }

    def _check_technical_pyramiding_conditions(self, ticker: str, current_price: float) -> dict:
        """
        ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ í”¼ë¼ë¯¸ë”© ì¡°ê±´ ì ê²€
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            current_price (float): í˜„ì¬ê°€
            
        Returns:
            dict: ê¸°ìˆ ì  ì§€í‘œ ì¡°ê±´ ì ê²€ ê²°ê³¼
        """
        try:
            # ì‹œì¥ ë°ì´í„° ì¡°íšŒ
            market_data = self._get_market_data_for_kelly(ticker)
            if not market_data:
                return {
                    'met': False,
                    'details': 'ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨'
                }
            
            conditions_met = []
            conditions_failed = []
            
            # 1. Supertrend ë§¤ìˆ˜ ì‹ í˜¸
            supertrend_signal = market_data.get('supertrend_signal', '')
            if supertrend_signal == 'bull':
                conditions_met.append('Supertrend ë§¤ìˆ˜')
            else:
                conditions_failed.append(f'Supertrend: {supertrend_signal}')
            
            # 2. ADX ê°•ë„ (25 ì´ìƒ)
            adx = market_data.get('adx', 0)
            if adx > 25:
                conditions_met.append('ADX ê°•í•¨')
            else:
                conditions_failed.append(f'ADX: {adx:.1f}')
            
            # 3. RSI ê³¼ë§¤ìˆ˜ ë°©ì§€ (75 ë¯¸ë§Œ)
            rsi = market_data.get('rsi', 50)
            if rsi < 75:
                conditions_met.append('RSI ì •ìƒ')
            else:
                conditions_failed.append(f'RSI ê³¼ë§¤ìˆ˜: {rsi:.1f}')
            
            # 4. MA20 ìƒìŠ¹
            ma20 = market_data.get('ma20', current_price)
            if current_price > ma20:
                conditions_met.append('MA20 ìƒìŠ¹')
            else:
                conditions_failed.append(f'MA20 í•˜ë½: {current_price:,.0f} < {ma20:,.0f}')
            
            # ìµœì†Œ 3ê°œ ì¡°ê±´ ì¶©ì¡± ì‹œ í†µê³¼
            is_met = len(conditions_met) >= 3
            
            return {
                'met': is_met,
                'details': f'ì¶©ì¡±: {conditions_met}, ë¯¸ì¶©ì¡±: {conditions_failed}'
            }
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ê¸°ìˆ ì  ì§€í‘œ ì¡°ê±´ ì ê²€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                'met': False,
                'details': f'ì ê²€ ì‹¤íŒ¨: {str(e)}'
            }

    def _execute_pyramiding_buy(self, ticker: str, score: float, confidence: float, trade_logs: list, total_balance: float):
        """
        í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            score (float): GPT ë¶„ì„ ì ìˆ˜
            confidence (float): GPT ë¶„ì„ ì‹ ë¢°ë„
            trade_logs (list): ê±°ë˜ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
            total_balance (float): ì´ ìì‚°
        """
        try:
            current_price = get_current_price_safe(ticker)
            if not current_price or current_price <= 0:
                logger.warning(f"âš ï¸ {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨")
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": 0,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": 0,
                    "status": "PRICE_FETCH_FAILED",
                    "error_msg": "í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨"
                })
                return
            
            # í”¼ë¼ë¯¸ë”© ì „ìš© í¬ì§€ì…˜ ì‚¬ì´ì§• (ê¸°ì¡´ë³´ë‹¤ ì‘ì€ í¬ê¸°)
            pyramid_position_size = 0.01  # ì´ ìì‚°ì˜ 1% (ê¸°ë³¸ í”¼ë¼ë¯¸ë”© í¬ê¸°)
            trade_amount_krw = total_balance * pyramid_position_size
            
            # ìµœì†Œ/ìµœëŒ€ ê¸ˆì•¡ ì œí•œ ì ìš©
            from utils import MIN_KRW_ORDER, TAKER_FEE_RATE
            min_amount_with_fee = MIN_KRW_ORDER / (1 + TAKER_FEE_RATE)
            max_amount = min(50000, total_balance * 0.02)  # í”¼ë¼ë¯¸ë”©ì€ ìµœëŒ€ 5ë§Œì› ë˜ëŠ” 2%
            trade_amount_krw = max(min_amount_with_fee, min(trade_amount_krw, max_amount))
            
            # ìˆ˜ìˆ˜ë£Œë¥¼ í¬í•¨í•œ ì‹¤ì œ ì£¼ë¬¸ ê¸ˆì•¡
            actual_order_amount = trade_amount_krw * (1 + TAKER_FEE_RATE)
            
            logger.info(f"ğŸ”¼ í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹œë„: {ticker} | ì ìˆ˜: {score} | ì‹ ë¢°ë„: {confidence:.2f} | í¬ì§€ì…˜: {pyramid_position_size:.1%} | ê¸ˆì•¡: {trade_amount_krw:,.0f}ì›")
            
            # ë§¤ìˆ˜ ì‹¤í–‰
            from trade_executor import buy_asset
            buy_result = buy_asset(
                upbit_client=self.upbit,
                ticker=ticker,
                current_price=current_price,
                trade_amount_krw=actual_order_amount,
                gpt_confidence=confidence,
                gpt_reason=f"í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ - GPT ë¶„ì„ ì ìˆ˜: {score}ì , ì‹ ë¢°ë„: {confidence:.2f}"
            )
            
            if buy_result.get("status") in ["SUCCESS", "SUCCESS_PARTIAL", "SUCCESS_PARTIAL_NO_AVG", "SUCCESS_NO_AVG_PRICE"]:
                buy_price = buy_result.get('price', current_price)
                status_msg = "í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì„±ê³µ" if buy_result.get("status") == "SUCCESS" else "í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ë¶€ë¶„ ì²´ê²° ì„±ê³µ"
                logger.info(f"ğŸ”¼ {status_msg}: {ticker} | ì²´ê²°ê°€: {buy_price:.2f} | ê¸ˆì•¡: {trade_amount_krw:,.0f}ì›")
                
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": buy_price,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": trade_amount_krw,
                    "status": "PYRAMIDING_SUCCESS",
                    "error_msg": None
                })
            else:
                error_msg = buy_result.get('error', 'Unknown')
                logger.warning(f"âš ï¸ í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤íŒ¨: {ticker} | ì˜¤ë¥˜: {error_msg}")
                
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": current_price,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": trade_amount_krw,
                    "status": "PYRAMIDING_FAILED",
                    "error_msg": error_msg
                })
                
        except Exception as e:
            logger.error(f"âŒ {ticker} í”¼ë¼ë¯¸ë”© ë§¤ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            trade_logs.append({
                "ticker": ticker,
                "buy_price": 0,
                "score": score,
                "confidence": confidence,
                "trade_amount_krw": 0,
                "status": "PYRAMIDING_ERROR",
                "error_msg": str(e)
            })

    def _execute_new_position_buy(self, ticker: str, score: float, confidence: float, trade_logs: list, total_balance: float):
        """
        ì‹ ê·œ ì¢…ëª© ë§¤ìˆ˜ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§)
        
        Args:
            ticker (str): í‹°ì»¤ëª…
            score (float): GPT ë¶„ì„ ì ìˆ˜
            confidence (float): GPT ë¶„ì„ ì‹ ë¢°ë„
            trade_logs (list): ê±°ë˜ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
            total_balance (float): ì´ ìì‚°
        """
        try:
            current_price = get_current_price_safe(ticker)
            if not current_price or current_price <= 0:
                logger.warning(f"âš ï¸ {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨")
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": 0,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": 0,
                    "status": "PRICE_FETCH_FAILED",
                    "error_msg": "í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨"
                })
                return
            
            # ê¸°ì¡´ì˜ í†µí•© í¬ì§€ì…˜ ì‚¬ì´ì§• ë¡œì§ ì‚¬ìš©
            kelly_result = self.calculate_kelly_position_size(
                ticker=ticker,
                score=score,
                confidence=confidence,
                current_price=current_price,
                total_balance=total_balance
            )
            
            # ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° ìˆ˜ì§‘
            technical_data = self._get_technical_data_for_integration(ticker)
            market_conditions = self._get_market_conditions_for_integration()
            
            # í†µí•© í¬ì§€ì…˜ ì‚¬ì´ì§• ê³„ì‚°
            try:
                from strategy_analyzer import calculate_integrated_position_size
                
                kelly_params = {
                    'kelly_fraction': kelly_result['kelly_fraction'],
                    'estimated_win_rate': kelly_result['estimated_win_rate'],
                    'risk_reward_ratio': kelly_result['risk_reward_ratio']
                }
                
                atr_params = {
                    'atr': kelly_result['atr'],
                    'current_price': current_price
                }
                
                integrated_result = calculate_integrated_position_size(
                    technical_data=technical_data,
                    kelly_params=kelly_params,
                    atr_params=atr_params,
                    market_conditions=market_conditions
                )
                
                integrated_position_size = integrated_result['final_position_size']
                trade_amount_krw = total_balance * integrated_position_size
                
            except Exception as e:
                logger.warning(f"âš ï¸ {ticker} í†µí•© í¬ì§€ì…˜ ì‚¬ì´ì§• ì‹¤íŒ¨, ê¸°ë³¸ ì¼ˆë¦¬ ê³µì‹ ì‚¬ìš©: {e}")
                integrated_position_size = kelly_result['final_position_size']
                trade_amount_krw = total_balance * integrated_position_size
            
            # ìµœì†Œ/ìµœëŒ€ ê¸ˆì•¡ ì œí•œ ì ìš©
            from utils import MIN_KRW_ORDER, TAKER_FEE_RATE
            min_amount_with_fee = MIN_KRW_ORDER / (1 + TAKER_FEE_RATE)
            max_amount = min(200000, total_balance * 0.05)
            trade_amount_krw = max(min_amount_with_fee, min(trade_amount_krw, max_amount))
            
            # ìˆ˜ìˆ˜ë£Œë¥¼ í¬í•¨í•œ ì‹¤ì œ ì£¼ë¬¸ ê¸ˆì•¡
            actual_order_amount = trade_amount_krw * (1 + TAKER_FEE_RATE)
            
            # ì‹œì¥ ìƒí™© ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
            market_validation = self._validate_market_conditions(ticker, current_price, score, confidence)
            if not market_validation['valid']:
                logger.warning(f"âš ï¸ ì‹œì¥ ì¡°ê±´ ê²€ì¦ ì‹¤íŒ¨: {ticker} | ì‚¬ìœ : {market_validation['reason']}")
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": current_price,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": trade_amount_krw,
                    "status": "MARKET_VALIDATION_FAILED",
                    "error_msg": market_validation['reason']
                })
                return
            
            logger.info(f"ğŸ†• ì‹ ê·œ ë§¤ìˆ˜ ì‹œë„: {ticker} | ì ìˆ˜: {score} | ì‹ ë¢°ë„: {confidence:.2f} | í†µí•©í¬ì§€ì…˜: {integrated_position_size:.1%} | ê¸ˆì•¡: {trade_amount_krw:,.0f}ì›")
            
            # ë§¤ìˆ˜ ì‹¤í–‰
            from trade_executor import buy_asset
            buy_result = buy_asset(
                upbit_client=self.upbit,
                ticker=ticker,
                current_price=current_price,
                trade_amount_krw=actual_order_amount,
                gpt_confidence=confidence,
                gpt_reason=f"ì‹ ê·œ ë§¤ìˆ˜ - GPT ë¶„ì„ ì ìˆ˜: {score}ì , ì‹ ë¢°ë„: {confidence:.2f}"
            )
            
            if buy_result.get("status") in ["SUCCESS", "SUCCESS_PARTIAL", "SUCCESS_PARTIAL_NO_AVG", "SUCCESS_NO_AVG_PRICE"]:
                buy_price = buy_result.get('price', current_price)
                status_msg = "ì‹ ê·œ ë§¤ìˆ˜ ì„±ê³µ" if buy_result.get("status") == "SUCCESS" else "ì‹ ê·œ ë§¤ìˆ˜ ë¶€ë¶„ ì²´ê²° ì„±ê³µ"
                logger.info(f"ğŸ’° {status_msg}: {ticker} | ì²´ê²°ê°€: {buy_price:.2f} | ê¸ˆì•¡: {trade_amount_krw:,.0f}ì›")
                
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": buy_price,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": trade_amount_krw,
                    "status": "SUCCESS",
                    "error_msg": None
                })
            else:
                error_msg = buy_result.get('error', 'Unknown')
                logger.warning(f"âš ï¸ ì‹ ê·œ ë§¤ìˆ˜ ì‹¤íŒ¨: {ticker} | ì˜¤ë¥˜: {error_msg}")
                
                error_analysis = self._analyze_buy_error(error_msg, ticker, current_price, trade_amount_krw)
                
                trade_logs.append({
                    "ticker": ticker,
                    "buy_price": current_price,
                    "score": score,
                    "confidence": confidence,
                    "trade_amount_krw": trade_amount_krw,
                    "status": "FAILED",
                    "error_msg": f"{error_msg} | ë¶„ì„: {error_analysis}"
                })
                
        except Exception as e:
            logger.error(f"âŒ {ticker} ì‹ ê·œ ë§¤ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            trade_logs.append({
                "ticker": ticker,
                "buy_price": 0,
                "score": score,
                "confidence": confidence,
                "trade_amount_krw": 0,
                "status": "ERROR",
                "error_msg": str(e)
            })

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import time
    
    start_time = time.time()
    
    try:
        logger.info("="*50)
        logger.info("ğŸš€ makenaide ì‹œì‘")
        logger.info("="*50)
        
        # MakenaideBot ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        try:
            bot = MakenaideBot()
        except Exception as e:
            logger.error(f"âŒ MakenaideBot ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

        try:
            success = bot.run()
        except Exception as e:
            logger.error(f"âŒ MakenaideBot ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
        
        # ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
        total_time = time.time() - start_time
        hours, remainder = divmod(total_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{int(hours)}ì‹œê°„ {int(minutes)}ë¶„ {seconds:.1f}ì´ˆ" if hours > 0 else f"{int(minutes)}ë¶„ {seconds:.1f}ì´ˆ"
        
        if success:
            logger.info("="*50)
            logger.info(f"âœ… makenaide ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ (ì´ ì†Œìš”ì‹œê°„: {time_str})")
            logger.info("="*50)
        else:
            logger.warning("="*50)
            logger.warning(f"âš ï¸ makenaide ì‹¤í–‰ ì¤‘ ì¼ë¶€ ê³¼ì • ì‹¤íŒ¨ (ì´ ì†Œìš”ì‹œê°„: {time_str})")
            logger.warning("="*50)
        
    except Exception as e:
        logger.error("="*50)
        logger.error(f"âŒ makenaide ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        logger.error("="*50)
        return False
        
    return True

if __name__ == "__main__":
    main()