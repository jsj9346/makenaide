#!/usr/bin/env python3
"""
ğŸ“Š Phase 1: Selective Data Collection Lambda
- Phase 0ì—ì„œ ë°›ì€ í‹°ì»¤ ëª©ë¡ì— ëŒ€í•´ ì„ íƒì  í•„í„°ë§ ì ìš©
- ì›”ë´‰ ë°ì´í„° í•„í„°ë§ (14ê°œì›” ì´ìƒ)
- ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ (24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ 3ì–µì› ì´ìƒ)
- OHLCV ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
- static_indicators í…Œì´ë¸”ì— ì €ì¥
"""

import json
import os
import logging
import time
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np

# Layerì—ì„œ ê³µí†µ ìœ í‹¸ë¦¬í‹° import
try:
    from makenaide_utils import (
        setup_lambda_logger, get_db_connection_params, 
        save_to_s3, load_from_s3, trigger_next_phase,
        create_lambda_response, LambdaTimer
    )
except ImportError:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© fallback
    def setup_lambda_logger(phase_name):
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def get_db_connection_params():
        return {
            'host': os.getenv("PG_HOST"),
            'port': os.getenv("PG_PORT", "5432"),
            'dbname': os.getenv("PG_DATABASE"),
            'user': os.getenv("PG_USER"),
            'password': os.getenv("PG_PASSWORD")
        }

logger = setup_lambda_logger("Phase1-DataCollection")

# ìƒìˆ˜ ì •ì˜
ONE_HMIL_KRW = 100_000_000  # 1ì–µì›
DEFAULT_MIN_TRADE_VOLUME = ONE_HMIL_KRW * 3  # 3ì–µì›

class SelectiveDataCollector:
    """
    ì„ íƒì  ë°ì´í„° ìˆ˜ì§‘ê¸°
    ê¸°ì¡´ makenaide.pyì˜ í•„í„°ë§ ë¡œì§ì„ ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì— ìµœì í™”
    """
    
    def __init__(self):
        self.s3_bucket = os.getenv('S3_BUCKET', 'makenaide-serverless-data')
        self.db_params = get_db_connection_params()
        
    def get_input_tickers(self) -> List[str]:
        """Phase 0ì—ì„œ ìƒì„±ëœ í‹°ì»¤ ëª©ë¡ì„ S3ì—ì„œ ë¡œë“œ"""
        try:
            logger.info("ğŸ“¥ Phase 0 ê²°ê³¼ ë¡œë“œ ì¤‘...")
            
            phase0_data = load_from_s3(self.s3_bucket, 'phase0/updated_tickers.json')
            
            if not phase0_data or 'tickers' not in phase0_data:
                logger.error("âŒ Phase 0 ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            tickers = phase0_data['tickers']
            logger.info(f"âœ… Phase 0ì—ì„œ {len(tickers)}ê°œ í‹°ì»¤ ë¡œë“œ ì™„ë£Œ")
            
            return tickers
            
        except Exception as e:
            logger.error(f"âŒ Phase 0 ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def filter_by_monthly_data_length(self, tickers: List[str], min_months: int = 14) -> List[str]:
        """
        ì›”ë´‰ ë°ì´í„° ê¸¸ì´ë¡œ í•„í„°ë§
        ê¸°ì¡´ filter_tickers.pyì˜ filter_by_monthly_data_length()ì™€ ë™ì¼
        """
        try:
            import pyupbit
            import time
            
            logger.info(f"ğŸ“… ì›”ë´‰ ë°ì´í„° ê¸¸ì´ í•„í„°ë§ ì‹œì‘ (ìµœì†Œ {min_months}ê°œì›”)")
            
            filtered_tickers = []
            failed_tickers = []
            
            for i, ticker in enumerate(tickers):
                try:
                    # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (ì´ˆë‹¹ 10íšŒ)
                    if i > 0 and i % 10 == 0:
                        time.sleep(1)
                    
                    # ì›”ë´‰ ë°ì´í„° ì¡°íšŒ
                    monthly_data = pyupbit.get_ohlcv(ticker, interval="month", count=200)
                    
                    if monthly_data is not None and len(monthly_data) >= min_months:
                        filtered_tickers.append(ticker)
                        logger.debug(f"âœ… {ticker}: {len(monthly_data)}ê°œì›” ë°ì´í„° (í†µê³¼)")
                    else:
                        failed_tickers.append(ticker)
                        data_length = len(monthly_data) if monthly_data is not None else 0
                        logger.debug(f"âŒ {ticker}: {data_length}ê°œì›” ë°ì´í„° (ë¶€ì¡±)")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ {ticker} ì›”ë´‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    failed_tickers.append(ticker)
            
            logger.info(f"ğŸ“… ì›”ë´‰ í•„í„°ë§ ì™„ë£Œ: {len(filtered_tickers)}ê°œ í†µê³¼, {len(failed_tickers)}ê°œ ì œì™¸")
            
            return filtered_tickers
            
        except Exception as e:
            logger.error(f"âŒ ì›”ë´‰ ë°ì´í„° í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return tickers  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def filter_by_volume(self, tickers: List[str], min_trade_price_krw: int = DEFAULT_MIN_TRADE_VOLUME) -> List[str]:
        """
        24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆìœ¼ë¡œ í•„í„°ë§
        ê¸°ì¡´ filter_tickers.pyì˜ filter_by_volume()ì™€ ë™ì¼
        """
        try:
            import pyupbit
            import requests
            
            logger.info(f"ğŸ’° 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì‹œì‘ (ìµœì†Œ {min_trade_price_krw:,}ì›)")
            
            # Upbit REST APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì¡°íšŒ
            try:
                url = "https://api.upbit.com/v1/ticker"
                params = {"markets": ",".join(tickers)}
                response = requests.get(url, params=params)
                
                if response.status_code != 200:
                    logger.warning(f"âš ï¸ Upbit API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                    raise Exception("API í˜¸ì¶œ ì‹¤íŒ¨")
                
                ticker_data = response.json()
                
            except Exception as e:
                logger.warning(f"âš ï¸ REST API ì‹¤íŒ¨, pyupbit fallback ì‚¬ìš©: {e}")
                # pyupbit fallback
                ticker_data = pyupbit.get_current_price(tickers)
                if isinstance(ticker_data, dict):
                    ticker_data = [{'market': k, 'acc_trade_price_24h': v} for k, v in ticker_data.items()]
            
            # ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§
            filtered_tickers = []
            for data in ticker_data:
                if isinstance(data, dict):
                    market = data.get('market')
                    trade_volume = data.get('acc_trade_price_24h', 0)
                    
                    if trade_volume and trade_volume >= min_trade_price_krw:
                        filtered_tickers.append(market)
                        logger.debug(f"âœ… {market}: {trade_volume:,.0f}ì› (í†µê³¼)")
                    else:
                        logger.debug(f"âŒ {market}: {trade_volume:,.0f}ì› (ë¶€ì¡±)")
            
            logger.info(f"ğŸ’° ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì™„ë£Œ: {len(filtered_tickers)}ê°œ í†µê³¼")
            
            return filtered_tickers
            
        except Exception as e:
            logger.error(f"âŒ ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return tickers  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def collect_ohlcv_data(self, ticker: str, days: int = 200) -> Optional[pd.DataFrame]:
        """
        ë‹¨ì¼ í‹°ì»¤ì˜ OHLCV ë°ì´í„° ìˆ˜ì§‘
        ê¸°ì¡´ data_fetcher.pyì˜ get_ohlcv_d()ì™€ ìœ ì‚¬
        """
        try:
            import pyupbit
            
            # ì¼ë´‰ ë°ì´í„° ì¡°íšŒ
            ohlcv_data = pyupbit.get_ohlcv(ticker, interval="day", count=days)
            
            if ohlcv_data is None or ohlcv_data.empty:
                logger.warning(f"âš ï¸ {ticker}: OHLCV ë°ì´í„° ì—†ìŒ")
                return None
            
            # ë°ì´í„° ê²€ì¦
            if len(ohlcv_data) < 50:  # ìµœì†Œ 50ì¼ ë°ì´í„° í•„ìš”
                logger.warning(f"âš ï¸ {ticker}: ë°ì´í„° ë¶€ì¡± ({len(ohlcv_data)}ì¼)")
                return None
            
            # ì»¬ëŸ¼ëª… í‘œì¤€í™”
            ohlcv_data.columns = ['open', 'high', 'low', 'close', 'volume']
            ohlcv_data.index.name = 'date'
            
            logger.debug(f"âœ… {ticker}: {len(ohlcv_data)}ì¼ OHLCV ë°ì´í„° ìˆ˜ì§‘")
            
            return ohlcv_data
            
        except Exception as e:
            logger.error(f"âŒ {ticker} OHLCV ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def calculate_technical_indicators(self, ohlcv_data: pd.DataFrame) -> Dict[str, Any]:
        """
        ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        ê¸°ì¡´ data_fetcher.pyì˜ calculate_technical_indicators()ì™€ ìœ ì‚¬
        """
        try:
            import pandas_ta as ta
            
            # ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
            indicators = {}
            
            # ì´ë™í‰ê· 
            indicators['ma5'] = ohlcv_data['close'].rolling(5).mean().iloc[-1]
            indicators['ma10'] = ohlcv_data['close'].rolling(10).mean().iloc[-1]
            indicators['ma20'] = ohlcv_data['close'].rolling(20).mean().iloc[-1]
            indicators['ma60'] = ohlcv_data['close'].rolling(60).mean().iloc[-1]
            indicators['ma120'] = ohlcv_data['close'].rolling(120).mean().iloc[-1]
            indicators['ma200'] = ohlcv_data['close'].rolling(200).mean().iloc[-1]
            
            # RSI
            rsi = ta.rsi(ohlcv_data['close'], length=14)
            indicators['rsi'] = rsi.iloc[-1] if rsi is not None else None
            
            # MACD
            macd = ta.macd(ohlcv_data['close'])
            if macd is not None:
                indicators['macd'] = macd['MACD_12_26_9'].iloc[-1]
                indicators['macd_signal'] = macd['MACDs_12_26_9'].iloc[-1]
                indicators['macd_histogram'] = macd['MACDh_12_26_9'].iloc[-1]
            
            # Bollinger Bands
            bb = ta.bbands(ohlcv_data['close'], length=20)
            if bb is not None:
                indicators['bb_upper'] = bb['BBU_20_2.0'].iloc[-1]
                indicators['bb_middle'] = bb['BBM_20_2.0'].iloc[-1]
                indicators['bb_lower'] = bb['BBL_20_2.0'].iloc[-1]
            
            # ADX
            adx = ta.adx(ohlcv_data['high'], ohlcv_data['low'], ohlcv_data['close'])
            if adx is not None:
                indicators['adx'] = adx['ADX_14'].iloc[-1]
            
            # ê±°ë˜ëŸ‰ ê´€ë ¨
            indicators['volume_ma20'] = ohlcv_data['volume'].rolling(20).mean().iloc[-1]
            indicators['volume_ratio'] = ohlcv_data['volume'].iloc[-1] / indicators['volume_ma20'] if indicators['volume_ma20'] > 0 else 1
            
            # ê°€ê²© ê´€ë ¨
            indicators['current_price'] = ohlcv_data['close'].iloc[-1]
            indicators['high_52w'] = ohlcv_data['high'].tail(252).max()  # 52ì£¼ ìµœê³ ê°€
            indicators['low_52w'] = ohlcv_data['low'].tail(252).min()   # 52ì£¼ ìµœì €ê°€
            
            # NaN ê°’ ì²˜ë¦¬
            for key, value in indicators.items():
                if pd.isna(value):
                    indicators[key] = None
            
            return indicators
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def process_single_ticker(self, ticker: str) -> Optional[Dict[str, Any]]:
        """
        ë‹¨ì¼ í‹°ì»¤ ì²˜ë¦¬ (OHLCV ìˆ˜ì§‘ + ì§€í‘œ ê³„ì‚°)
        """
        try:
            # OHLCV ë°ì´í„° ìˆ˜ì§‘
            ohlcv_data = self.collect_ohlcv_data(ticker)
            if ohlcv_data is None:
                return None
            
            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            indicators = self.calculate_technical_indicators(ohlcv_data)
            if not indicators:
                return None
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            result = {
                'ticker': ticker,
                'processed_at': datetime.now().isoformat(),
                'data_points': len(ohlcv_data),
                **indicators
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ {ticker} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def process_tickers_parallel(self, filtered_tickers: List[str], max_workers: int = 5) -> List[Dict[str, Any]]:
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ í‹°ì»¤ì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì§€í‘œ ê³„ì‚°
        """
        logger.info(f"âš¡ï¸ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {len(filtered_tickers)}ê°œ í‹°ì»¤ ({max_workers}ê°œ ì›Œì»¤)")
        
        successful_results = []
        failed_tickers = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_ticker = {
                executor.submit(self.process_single_ticker, ticker): ticker 
                for ticker in filtered_tickers
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_ticker):
                ticker = future_to_ticker[future]
                try:
                    result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                    if result:
                        successful_results.append(result)
                        logger.debug(f"âœ… {ticker} ì²˜ë¦¬ ì™„ë£Œ")
                    else:
                        failed_tickers.append(ticker)
                        logger.debug(f"âŒ {ticker} ì²˜ë¦¬ ì‹¤íŒ¨")
                        
                except Exception as e:
                    failed_tickers.append(ticker)
                    logger.warning(f"âš ï¸ {ticker} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}")
        
        logger.info(f"âš¡ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(successful_results)}ê°œ, ì‹¤íŒ¨ {len(failed_tickers)}ê°œ")
        
        return successful_results
    
    def save_to_database(self, processed_data: List[Dict[str, Any]]) -> bool:
        """
        ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ static_indicators í…Œì´ë¸”ì— ì €ì¥
        """
        if not processed_data:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        try:
            logger.info(f"ğŸ—„ï¸ DB ì €ì¥ ì‹œì‘: {len(processed_data)}ê°œ ë ˆì½”ë“œ")
            
            conn = psycopg2.connect(**self.db_params)
            cursor = conn.cursor()
            
            try:
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë‹¹ì¼ ë°ì´í„°ë§Œ)
                today = datetime.now().date()
                cursor.execute("""
                    DELETE FROM static_indicators 
                    WHERE DATE(created_at) = %s
                """, (today,))
                
                deleted_count = cursor.rowcount
                logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {deleted_count}ê°œ ë ˆì½”ë“œ")
                
                # ìƒˆë¡œìš´ ë°ì´í„° ì‚½ì…
                insert_query = """
                    INSERT INTO static_indicators (
                        ticker, ma5, ma10, ma20, ma60, ma120, ma200,
                        rsi, macd, macd_signal, macd_histogram,
                        bb_upper, bb_middle, bb_lower, adx,
                        volume_ma20, volume_ratio, current_price,
                        high_52w, low_52w, data_points,
                        created_at, updated_at
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s,
                        %s, %s, %s, %s,
                        %s, %s, %s, %s,
                        %s, %s, %s,
                        %s, %s, %s,
                        NOW(), NOW()
                    )
                """
                
                insert_data = []
                for data in processed_data:
                    insert_data.append((
                        data['ticker'],
                        data.get('ma5'), data.get('ma10'), data.get('ma20'),
                        data.get('ma60'), data.get('ma120'), data.get('ma200'),
                        data.get('rsi'), data.get('macd'), data.get('macd_signal'), data.get('macd_histogram'),
                        data.get('bb_upper'), data.get('bb_middle'), data.get('bb_lower'), data.get('adx'),
                        data.get('volume_ma20'), data.get('volume_ratio'), data.get('current_price'),
                        data.get('high_52w'), data.get('low_52w'), data.get('data_points', 0)
                    ))
                
                cursor.executemany(insert_query, insert_data)
                
                # ì»¤ë°‹
                conn.commit()
                
                logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(insert_data)}ê°œ ë ˆì½”ë“œ ì‚½ì…")
                
                return True
                
            finally:
                cursor.close()
                conn.close()
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def save_results_to_s3(self, filtered_tickers: List[str], processed_data: List[Dict[str, Any]]) -> bool:
        """Phase 1 ê²°ê³¼ë¥¼ S3ì— ì €ì¥"""
        try:
            result_data = {
                'timestamp': datetime.now().isoformat(),
                'phase': 'selective_data_collection',
                'input_tickers': len(filtered_tickers),
                'processed_tickers': len(processed_data),
                'filtered_tickers': filtered_tickers,
                'success_rate': len(processed_data) / len(filtered_tickers) if filtered_tickers else 0,
                'status': 'success'
            }
            
            # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ì¶”ê°€
            if processed_data:
                prices = [d.get('current_price', 0) for d in processed_data if d.get('current_price')]
                result_data['stats'] = {
                    'avg_price': sum(prices) / len(prices) if prices else 0,
                    'max_price': max(prices) if prices else 0,
                    'min_price': min(prices) if prices else 0
                }
            
            return save_to_s3(
                self.s3_bucket,
                'phase1/filtered_tickers_with_data.json',
                result_data
            )
            
        except Exception as e:
            logger.error(f"âŒ S3 ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

def lambda_handler(event, context):
    """
    AWS Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜
    """
    
    with LambdaTimer("Phase 1: Selective Data Collection"):
        try:
            # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
            collector = SelectiveDataCollector()
            
            # 1. Phase 0 ê²°ê³¼ ë¡œë“œ
            logger.info("ğŸ“¥ 1ë‹¨ê³„: Phase 0 ê²°ê³¼ ë¡œë“œ")
            input_tickers = collector.get_input_tickers()
            
            if not input_tickers:
                raise Exception("Phase 0 ê²°ê³¼ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. ì›”ë´‰ ë°ì´í„° ê¸¸ì´ í•„í„°ë§
            logger.info("ğŸ“… 2ë‹¨ê³„: ì›”ë´‰ ë°ì´í„° í•„í„°ë§")
            monthly_filtered = collector.filter_by_monthly_data_length(input_tickers, min_months=14)
            
            if not monthly_filtered:
                raise Exception("ì›”ë´‰ í•„í„°ë§ í›„ ë‚¨ì€ í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 3. ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§
            logger.info("ğŸ’° 3ë‹¨ê³„: ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§")
            volume_filtered = collector.filter_by_volume(monthly_filtered)
            
            if not volume_filtered:
                raise Exception("ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ í›„ ë‚¨ì€ í‹°ì»¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 4. OHLCV ë°ì´í„° ìˆ˜ì§‘ ë° ì§€í‘œ ê³„ì‚°
            logger.info("ğŸ“Š 4ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° ì§€í‘œ ê³„ì‚°")
            processed_data = collector.process_tickers_parallel(volume_filtered)
            
            if not processed_data:
                raise Exception("ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # 5. DB ì €ì¥
            logger.info("ğŸ—„ï¸ 5ë‹¨ê³„: DB ì €ì¥")
            db_success = collector.save_to_database(processed_data)
            
            # 6. S3 ê²°ê³¼ ì €ì¥
            logger.info("ğŸ’¾ 6ë‹¨ê³„: S3 ê²°ê³¼ ì €ì¥")
            s3_success = collector.save_results_to_s3(volume_filtered, processed_data)
            
            # 7. ë‹¤ìŒ ë‹¨ê³„ íŠ¸ë¦¬ê±°
            logger.info("ğŸš€ 7ë‹¨ê³„: ë‹¤ìŒ ë‹¨ê³„ íŠ¸ë¦¬ê±°")
            trigger_success = trigger_next_phase('selective_data_collection', 'comprehensive_filtering')
            
            # ì„±ê³µ ì‘ë‹µ
            return create_lambda_response(200, 'selective_data_collection', {
                'input_tickers': len(input_tickers),
                'monthly_filtered': len(monthly_filtered),
                'volume_filtered': len(volume_filtered),
                'processed_data': len(processed_data),
                'db_saved': db_success,
                's3_saved': s3_success,
                'next_phase_triggered': trigger_success
            })
            
        except Exception as e:
            logger.error(f"âŒ Phase 1 ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return create_lambda_response(500, 'selective_data_collection', error=str(e))

# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ.setdefault('S3_BUCKET', 'makenaide-serverless-data')
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = lambda_handler({}, {})
    print(json.dumps(result, indent=2, ensure_ascii=False))