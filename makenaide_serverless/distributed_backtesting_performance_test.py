#!/usr/bin/env python3
"""
ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

Lambda ê¸°ë°˜ ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  
ìˆœì°¨ ì²˜ë¦¬ì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

Author: Performance Testing System
Version: 1.0.0
"""

import json
import logging
import boto3
import time
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
import uuid
import statistics
from pathlib import Path
import csv

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class DistributedBacktestingPerformanceTest:
    """ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, region_name: str = 'ap-northeast-2'):
        self.region = region_name
        self.lambda_client = boto3.client('lambda', region_name=region_name)
        self.sqs_client = boto3.client('sqs', region_name=region_name)
        self.cloudwatch = boto3.client('cloudwatch', region_name=region_name)
        
        self.resource_prefix = "makenaide-distributed-backtest"
        self.worker_function = "makenaide-distributed-backtest-worker"
        self.collector_function = "makenaide-backtest-result-collector"
        
        # í URL ì¡°íšŒ
        self.job_queue_url = self._get_queue_url(f"{self.resource_prefix}-job-queue")
        self.result_queue_url = self._get_queue_url(f"{self.resource_prefix}-result-queue")
        
        logger.info(f"ğŸš€ ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™” (ë¦¬ì „: {region_name})")
    
    def run_sequential_performance_test(self, test_jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìˆœì°¨ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            logger.info(f"ğŸŒ ìˆœì°¨ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_jobs)}ê°œ ì‘ì—…")
            start_time = datetime.now()
            
            results = []
            processing_times = []
            
            for i, job in enumerate(test_jobs):
                job_start = time.time()
                
                # Lambda ì§ì ‘ í˜¸ì¶œ (ìˆœì°¨ ì²˜ë¦¬)
                response = self.lambda_client.invoke(
                    FunctionName=self.worker_function,
                    InvocationType='RequestResponse',
                    Payload=json.dumps({'job_data': job})
                )
                
                job_end = time.time()
                job_duration = job_end - job_start
                processing_times.append(job_duration)
                
                # ì‘ë‹µ íŒŒì‹±
                result_payload = json.loads(response['Payload'].read())
                results.append(result_payload)
                
                if i % 10 == 0:  # 10ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê·¸
                    logger.info(f"   ìˆœì°¨ ì²˜ë¦¬ ì§„í–‰: {i+1}/{len(test_jobs)} ({job_duration:.2f}ì´ˆ)")
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            sequential_metrics = {
                'test_type': 'sequential',
                'total_jobs': len(test_jobs),
                'total_duration_seconds': total_duration,
                'average_job_duration': statistics.mean(processing_times),
                'min_job_duration': min(processing_times),
                'max_job_duration': max(processing_times),
                'median_job_duration': statistics.median(processing_times),
                'jobs_per_second': len(test_jobs) / total_duration,
                'successful_jobs': len([r for r in results if r.get('statusCode') == 200]),
                'failed_jobs': len([r for r in results if r.get('statusCode') != 200]),
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat()
            }
            
            logger.info(f"âœ… ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {total_duration:.2f}ì´ˆ, {sequential_metrics['jobs_per_second']:.2f} jobs/sec")
            
            return sequential_metrics
            
        except Exception as e:
            logger.error(f"âŒ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def run_distributed_performance_test(self, test_jobs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë¶„ì‚° ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            logger.info(f"âš¡ ë¶„ì‚° ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_jobs)}ê°œ ì‘ì—…")
            start_time = datetime.now()
            
            # 1. ëª¨ë“  ì‘ì—…ì„ SQS íì— ì „ì†¡
            job_submission_start = time.time()
            sent_jobs = self._send_jobs_to_queue(test_jobs)
            job_submission_end = time.time()
            submission_duration = job_submission_end - job_submission_start
            
            logger.info(f"ğŸ“¤ ì‘ì—… ì „ì†¡ ì™„ë£Œ: {len(sent_jobs)}ê°œ ({submission_duration:.2f}ì´ˆ)")
            
            # 2. ê²°ê³¼ ëª¨ë‹ˆí„°ë§
            monitoring_results = self._monitor_distributed_processing(len(sent_jobs))
            
            end_time = datetime.now()
            total_duration = (end_time - start_time).total_seconds()
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            distributed_metrics = {
                'test_type': 'distributed',
                'total_jobs': len(test_jobs),
                'sent_jobs': len(sent_jobs),
                'total_duration_seconds': total_duration,
                'job_submission_duration': submission_duration,
                'processing_duration': monitoring_results.get('processing_duration', 0),
                'jobs_per_second': len(sent_jobs) / total_duration,
                'successful_jobs': monitoring_results.get('successful_jobs', 0),
                'failed_jobs': monitoring_results.get('failed_jobs', 0),
                'concurrent_workers': monitoring_results.get('concurrent_workers', 0),
                'average_worker_duration': monitoring_results.get('average_worker_duration', 0),
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'monitoring_details': monitoring_results
            }
            
            logger.info(f"âœ… ë¶„ì‚° ì²˜ë¦¬ ì™„ë£Œ: {total_duration:.2f}ì´ˆ, {distributed_metrics['jobs_per_second']:.2f} jobs/sec")
            
            return distributed_metrics
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì‚° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def run_scalability_test(self) -> Dict[str, Any]:
        """í™•ì¥ì„± í…ŒìŠ¤íŠ¸ - ë‹¤ì–‘í•œ ì‘ì—… í¬ê¸°ë¡œ ì„±ëŠ¥ ì¸¡ì •"""
        try:
            logger.info("ğŸ“ˆ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # í…ŒìŠ¤íŠ¸ í¬ê¸° ì •ì˜ (ì‘ì—… ê°œìˆ˜)
            test_sizes = [5, 10, 25, 50, 100]
            scalability_results = []
            
            for size in test_sizes:
                logger.info(f"\nğŸ“Š í™•ì¥ì„± í…ŒìŠ¤íŠ¸: {size}ê°œ ì‘ì—…")
                
                # í…ŒìŠ¤íŠ¸ ì‘ì—… ìƒì„±
                test_jobs = self._generate_test_jobs(size, job_type='SCALABILITY')
                
                # ë¶„ì‚° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
                distributed_result = self.run_distributed_performance_test(test_jobs)
                
                # ê²°ê³¼ ì €ì¥
                scalability_results.append({
                    'job_count': size,
                    'distributed_metrics': distributed_result
                })
                
                # í í´ë¦¬ì–´ë§ ëŒ€ê¸°
                time.sleep(10)
            
            # í™•ì¥ì„± ë¶„ì„
            scalability_analysis = self._analyze_scalability(scalability_results)
            
            return {
                'test_type': 'scalability',
                'test_results': scalability_results,
                'scalability_analysis': scalability_analysis,
                'test_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def run_comprehensive_performance_comparison(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        try:
            logger.info("ğŸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            test_start = datetime.now()
            
            # í…ŒìŠ¤íŠ¸ ì„¤ì •
            test_job_count = 50  # ì ë‹¹í•œ í¬ê¸°ë¡œ ë¹„êµ
            test_jobs = self._generate_test_jobs(test_job_count, job_type='COMPARISON')
            
            logger.info(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì‘ì—… ìƒì„± ì™„ë£Œ: {test_job_count}ê°œ")
            
            # 1. ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            logger.info("\nğŸŒ 1ë‹¨ê³„: ìˆœì°¨ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •")
            sequential_results = self.run_sequential_performance_test(test_jobs.copy())
            
            # ì‹œìŠ¤í…œ ì •ë¦¬ ì‹œê°„
            logger.info("â³ ì‹œìŠ¤í…œ ì •ë¦¬ ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
            time.sleep(30)
            
            # 2. ë¶„ì‚° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            logger.info("\nâš¡ 2ë‹¨ê³„: ë¶„ì‚° ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •")
            distributed_results = self.run_distributed_performance_test(test_jobs.copy())
            
            # 3. ì„±ëŠ¥ ë¹„êµ ë¶„ì„
            logger.info("\nğŸ“Š 3ë‹¨ê³„: ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
            comparison_analysis = self._analyze_performance_comparison(
                sequential_results, distributed_results
            )
            
            test_end = datetime.now()
            total_test_duration = (test_end - test_start).total_seconds()
            
            # ì¢…í•© ê²°ê³¼
            comprehensive_results = {
                'test_type': 'comprehensive_comparison',
                'test_job_count': test_job_count,
                'total_test_duration_seconds': total_test_duration,
                'sequential_results': sequential_results,
                'distributed_results': distributed_results,
                'performance_comparison': comparison_analysis,
                'test_start_time': test_start.isoformat(),
                'test_end_time': test_end.isoformat()
            }
            
            # ê²°ê³¼ ë¦¬í¬íŠ¸
            self._generate_performance_report(comprehensive_results)
            
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•© ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _generate_test_jobs(self, count: int, job_type: str = 'PERFORMANCE_TEST') -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ìš© ë°±í…ŒìŠ¤íŠ¸ ì‘ì—… ìƒì„±"""
        test_jobs = []
        
        strategies = ['Momentum_Strategy', 'Mean_Reversion', 'Breakout_Strategy', 'Volume_Strategy']
        
        for i in range(count):
            strategy = strategies[i % len(strategies)]
            
            job = {
                'job_id': f'{job_type.lower()}-{i+1}-{int(time.time())}-{str(uuid.uuid4())[:8]}',
                'job_type': 'SINGLE_STRATEGY',
                'strategy_name': strategy,
                'parameters': {
                    'position_size_method': 'percent',
                    'position_size_value': 0.05 + (i % 10) * 0.01,  # 0.05~0.14
                    'stop_loss_pct': 0.03 + (i % 5) * 0.01,  # 0.03~0.07
                    'take_profit_pct': 0.08 + (i % 8) * 0.01,  # 0.08~0.15
                    'max_positions': 5 + (i % 10)  # 5~14
                },
                'data_range': {
                    'start_date': '2024-01-01',
                    'end_date': '2024-02-01'  # 1ê°œì›” ë°ì´í„°
                },
                'test_metadata': {
                    'test_type': job_type,
                    'test_index': i + 1,
                    'created_at': datetime.now().isoformat()
                }
            }
            
            test_jobs.append(job)
        
        return test_jobs
    
    def _send_jobs_to_queue(self, jobs: List[Dict[str, Any]]) -> List[str]:
        """ì‘ì—…ë“¤ì„ SQS íì— ì „ì†¡"""
        sent_job_ids = []
        
        try:
            for job in jobs:
                response = self.sqs_client.send_message(
                    QueueUrl=self.job_queue_url,
                    MessageBody=json.dumps(job),
                    MessageAttributes={
                        'job_type': {
                            'StringValue': job.get('job_type', 'SINGLE_STRATEGY'),
                            'DataType': 'String'
                        },
                        'test': {
                            'StringValue': 'performance_test',
                            'DataType': 'String'
                        }
                    }
                )
                sent_job_ids.append(job['job_id'])
            
            return sent_job_ids
            
        except Exception as e:
            logger.error(f"âŒ ì‘ì—… ì „ì†¡ ì‹¤íŒ¨: {e}")
            return sent_job_ids
    
    def _monitor_distributed_processing(self, expected_jobs: int, timeout: int = 300) -> Dict[str, Any]:
        """ë¶„ì‚° ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§"""
        try:
            logger.info(f"ğŸ‘ï¸ ë¶„ì‚° ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {expected_jobs}ê°œ ì‘ì—… ëŒ€ê¸°")
            
            start_time = time.time()
            completed_jobs = 0
            failed_jobs = 0
            processing_times = []
            
            # CloudWatch ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘ì 
            monitoring_start = datetime.now()
            
            while time.time() - start_time < timeout:
                # ê²°ê³¼ íì—ì„œ ë©”ì‹œì§€ í™•ì¸
                try:
                    response = self.sqs_client.receive_message(
                        QueueUrl=self.result_queue_url,
                        MaxNumberOfMessages=10,
                        WaitTimeSeconds=5,
                        MessageAttributeNames=['All']
                    )
                    
                    messages = response.get('Messages', [])
                    
                    for message in messages:
                        try:
                            result_data = json.loads(message['Body'])
                            
                            if result_data.get('status') == 'COMPLETED':
                                completed_jobs += 1
                                processing_times.append(
                                    result_data.get('execution_time_seconds', 0)
                                )
                            else:
                                failed_jobs += 1
                            
                            # ë©”ì‹œì§€ ì‚­ì œ
                            self.sqs_client.delete_message(
                                QueueUrl=self.result_queue_url,
                                ReceiptHandle=message['ReceiptHandle']
                            )
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ ê²°ê³¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    # ì§„í–‰ ìƒí™© ë¡œê·¸
                    total_processed = completed_jobs + failed_jobs
                    if total_processed > 0 and total_processed % 10 == 0:
                        logger.info(f"   ì²˜ë¦¬ ì§„í–‰: {total_processed}/{expected_jobs} "
                                  f"(ì„±ê³µ: {completed_jobs}, ì‹¤íŒ¨: {failed_jobs})")
                    
                    # ëª¨ë“  ì‘ì—… ì™„ë£Œ í™•ì¸
                    if total_processed >= expected_jobs:
                        logger.info("âœ… ëª¨ë“  ë¶„ì‚° ì‘ì—… ì™„ë£Œ")
                        break
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ê²°ê³¼ í í´ë§ ì‹¤íŒ¨: {e}")
                    time.sleep(2)
            
            end_time = time.time()
            processing_duration = end_time - start_time
            
            # CloudWatchì—ì„œ ë™ì‹œ ì‹¤í–‰ ë©”íŠ¸ë¦­ ì¡°íšŒ
            concurrent_metrics = self._get_lambda_concurrent_metrics(monitoring_start)
            
            monitoring_results = {
                'processing_duration': processing_duration,
                'successful_jobs': completed_jobs,
                'failed_jobs': failed_jobs,
                'total_processed': completed_jobs + failed_jobs,
                'completion_rate': (completed_jobs + failed_jobs) / expected_jobs,
                'average_worker_duration': statistics.mean(processing_times) if processing_times else 0,
                'concurrent_workers': concurrent_metrics.get('max_concurrent_executions', 0),
                'timeout_reached': processing_duration >= timeout
            }
            
            return monitoring_results
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì‚° ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _get_lambda_concurrent_metrics(self, start_time: datetime) -> Dict[str, Any]:
        """Lambda ë™ì‹œ ì‹¤í–‰ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            end_time = datetime.now()
            
            response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/Lambda',
                MetricName='ConcurrentExecutions',
                Dimensions=[
                    {
                        'Name': 'FunctionName',
                        'Value': self.worker_function
                    }
                ],
                StartTime=start_time,
                EndTime=end_time,
                Period=60,  # 1ë¶„ ê°„ê²©
                Statistics=['Maximum', 'Average']
            )
            
            datapoints = response.get('Datapoints', [])
            
            if datapoints:
                max_concurrent = max(dp['Maximum'] for dp in datapoints)
                avg_concurrent = statistics.mean(dp['Average'] for dp in datapoints)
                
                return {
                    'max_concurrent_executions': max_concurrent,
                    'average_concurrent_executions': avg_concurrent,
                    'datapoints_count': len(datapoints)
                }
            else:
                return {'max_concurrent_executions': 0, 'average_concurrent_executions': 0}
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë™ì‹œ ì‹¤í–‰ ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'max_concurrent_executions': 0, 'average_concurrent_executions': 0}
    
    def _analyze_scalability(self, scalability_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í™•ì¥ì„± ë¶„ì„"""
        try:
            job_counts = []
            throughputs = []
            durations = []
            
            for result in scalability_results:
                if 'error' not in result['distributed_metrics']:
                    job_counts.append(result['job_count'])
                    throughputs.append(result['distributed_metrics']['jobs_per_second'])
                    durations.append(result['distributed_metrics']['total_duration_seconds'])
            
            if not job_counts:
                return {'error': 'ë¶„ì„í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # ì„ í˜• í™•ì¥ì„± ë¶„ì„
            linear_efficiency = []
            for i, count in enumerate(job_counts):
                if i == 0:
                    linear_efficiency.append(1.0)  # ê¸°ì¤€ì 
                else:
                    expected_duration = durations[0] * (count / job_counts[0])
                    actual_duration = durations[i]
                    efficiency = expected_duration / actual_duration
                    linear_efficiency.append(efficiency)
            
            analysis = {
                'job_count_range': f"{min(job_counts)}-{max(job_counts)}",
                'throughput_range': f"{min(throughputs):.2f}-{max(throughputs):.2f} jobs/sec",
                'average_throughput': statistics.mean(throughputs),
                'peak_throughput': max(throughputs),
                'linear_efficiency': {
                    'average': statistics.mean(linear_efficiency),
                    'minimum': min(linear_efficiency),
                    'efficiency_scores': dict(zip(job_counts, linear_efficiency))
                },
                'scalability_trend': 'increasing' if throughputs[-1] > throughputs[0] else 'decreasing'
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ í™•ì¥ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _analyze_performance_comparison(self, sequential: Dict[str, Any], distributed: Dict[str, Any]) -> Dict[str, Any]:
        """ìˆœì°¨ vs ë¶„ì‚° ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        try:
            if 'error' in sequential or 'error' in distributed:
                return {'error': 'ë¹„êµë¥¼ ìœ„í•œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
            
            # ì„±ëŠ¥ ê°œì„  ê³„ì‚°
            throughput_improvement = (
                distributed['jobs_per_second'] / sequential['jobs_per_second']
            ) if sequential['jobs_per_second'] > 0 else 0
            
            duration_improvement = (
                sequential['total_duration_seconds'] / distributed['total_duration_seconds']
            ) if distributed['total_duration_seconds'] > 0 else 0
            
            # íš¨ìœ¨ì„± ë¶„ì„
            sequential_efficiency = sequential['total_jobs'] / sequential['total_duration_seconds']
            distributed_efficiency = distributed['total_jobs'] / distributed['total_duration_seconds']
            
            comparison = {
                'throughput_improvement_ratio': throughput_improvement,
                'throughput_improvement_percentage': (throughput_improvement - 1) * 100,
                'duration_improvement_ratio': duration_improvement,
                'duration_improvement_percentage': (duration_improvement - 1) * 100,
                'performance_summary': {
                    'sequential_throughput': sequential['jobs_per_second'],
                    'distributed_throughput': distributed['jobs_per_second'],
                    'sequential_duration': sequential['total_duration_seconds'],
                    'distributed_duration': distributed['total_duration_seconds'],
                    'concurrent_workers': distributed.get('concurrent_workers', 0)
                },
                'efficiency_analysis': {
                    'sequential_efficiency': sequential_efficiency,
                    'distributed_efficiency': distributed_efficiency,
                    'efficiency_gain': distributed_efficiency / sequential_efficiency if sequential_efficiency > 0 else 0
                },
                'success_rates': {
                    'sequential_success_rate': sequential['successful_jobs'] / sequential['total_jobs'] if sequential['total_jobs'] > 0 else 0,
                    'distributed_success_rate': distributed['successful_jobs'] / distributed['total_jobs'] if distributed['total_jobs'] > 0 else 0
                }
            }
            
            # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
            if throughput_improvement >= 2.0:
                performance_grade = 'A+ (Excellent)'
            elif throughput_improvement >= 1.5:
                performance_grade = 'A (Very Good)'
            elif throughput_improvement >= 1.2:
                performance_grade = 'B (Good)'
            elif throughput_improvement >= 1.0:
                performance_grade = 'C (Fair)'
            else:
                performance_grade = 'D (Poor)'
            
            comparison['performance_grade'] = performance_grade
            
            return comparison
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def _generate_performance_report(self, results: Dict[str, Any]):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = Path(f'distributed_backtesting_performance_report_{timestamp}.json')
            
            # JSON ë¦¬í¬íŠ¸ ì €ì¥
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            # CSV ìš”ì•½ ì €ì¥
            csv_file = Path(f'performance_summary_{timestamp}.csv')
            comparison = results.get('performance_comparison', {})
            
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Metric', 'Sequential', 'Distributed', 'Improvement'])
                writer.writerow([
                    'Throughput (jobs/sec)',
                    f"{results['sequential_results']['jobs_per_second']:.2f}",
                    f"{results['distributed_results']['jobs_per_second']:.2f}",
                    f"{comparison.get('throughput_improvement_percentage', 0):.1f}%"
                ])
                writer.writerow([
                    'Duration (seconds)',
                    f"{results['sequential_results']['total_duration_seconds']:.2f}",
                    f"{results['distributed_results']['total_duration_seconds']:.2f}",
                    f"{comparison.get('duration_improvement_percentage', 0):.1f}%"
                ])
                writer.writerow([
                    'Success Rate',
                    f"{comparison.get('success_rates', {}).get('sequential_success_rate', 0)*100:.1f}%",
                    f"{comparison.get('success_rates', {}).get('distributed_success_rate', 0)*100:.1f}%",
                    ""
                ])
            
            logger.info(f"ğŸ“‹ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ:")
            logger.info(f"   ğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
            logger.info(f"   ğŸ“Š ìš”ì•½ CSV: {csv_file}")
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _get_queue_url(self, queue_name: str) -> str:
        """í URL ì¡°íšŒ"""
        try:
            response = self.sqs_client.get_queue_url(QueueName=queue_name)
            return response['QueueUrl']
        except Exception as e:
            logger.warning(f"âš ï¸ í URL ì¡°íšŒ ì‹¤íŒ¨ ({queue_name}): {e}")
            return ""

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    try:
        performance_tester = DistributedBacktestingPerformanceTest()
        
        # ì¢…í•© ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        logger.info("ğŸ¯ ì¢…í•© ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        results = performance_tester.run_comprehensive_performance_comparison()
        
        if 'error' not in results:
            comparison = results.get('performance_comparison', {})
            
            print(f"\nğŸ‰ ë¶„ì‚° ë°±í…ŒìŠ¤íŒ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print("=" * 50)
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
            print(f"   ğŸ“ í…ŒìŠ¤íŠ¸ ì‘ì—… ìˆ˜: {results.get('test_job_count', 0)}ê°œ")
            print(f"   â±ï¸  ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {results.get('total_test_duration_seconds', 0):.1f}ì´ˆ")
            
            if comparison:
                print(f"\nâš¡ ì„±ëŠ¥ ê°œì„  ê²°ê³¼:")
                print(f"   ğŸš€ ì²˜ë¦¬ëŸ‰ ê°œì„ : {comparison.get('throughput_improvement_percentage', 0):.1f}%")
                print(f"   â° ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•: {comparison.get('duration_improvement_percentage', 0):.1f}%")
                print(f"   ğŸ“ˆ ì„±ëŠ¥ ë“±ê¸‰: {comparison.get('performance_grade', 'N/A')}")
                
                summary = comparison.get('performance_summary', {})
                print(f"\nğŸ“ˆ ìƒì„¸ ë©”íŠ¸ë¦­:")
                print(f"   ìˆœì°¨ ì²˜ë¦¬: {summary.get('sequential_throughput', 0):.2f} jobs/sec")
                print(f"   ë¶„ì‚° ì²˜ë¦¬: {summary.get('distributed_throughput', 0):.2f} jobs/sec")
                print(f"   ë™ì‹œ ì›Œì»¤: {summary.get('concurrent_workers', 0)}ê°œ")
                
            # ì¶”ê°€ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì œì•ˆ
            print(f"\nğŸ” ì¶”ê°€ í…ŒìŠ¤íŠ¸:")
            print("   python distributed_backtesting_performance_test.py --scalability")
            
        else:
            print(f"âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {results['error']}")
    
    except Exception as e:
        print(f"âŒ ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--scalability':
        # í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("ğŸ“ˆ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        tester = DistributedBacktestingPerformanceTest()
        scalability_results = tester.run_scalability_test()
        
        if 'error' not in scalability_results:
            analysis = scalability_results.get('scalability_analysis', {})
            print(f"ğŸ“Š í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
            print(f"   ì²˜ë¦¬ëŸ‰ ë²”ìœ„: {analysis.get('throughput_range', 'N/A')}")
            print(f"   í‰ê·  ì²˜ë¦¬ëŸ‰: {analysis.get('average_throughput', 0):.2f} jobs/sec")
            print(f"   ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {analysis.get('peak_throughput', 0):.2f} jobs/sec")
            
            efficiency = analysis.get('linear_efficiency', {})
            print(f"   í‰ê·  ì„ í˜• íš¨ìœ¨ì„±: {efficiency.get('average', 0):.2f}")
        else:
            print(f"âŒ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {scalability_results['error']}")
    else:
        main()