#!/usr/bin/env python3
"""
AWS Lambda í•¨ìˆ˜: Makenaide Orchestrator (ìµœì í™” ë²„ì „)
ê¸°ëŠ¥: ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì¡°ì • ë° ê° Lambda í•¨ìˆ˜ í˜¸ì¶œ

ğŸ¯ íŒŒì´í”„ë¼ì¸ ìˆœì„œ:
1. í‹°ì»¤ ìŠ¤ìº” (makenaide-ticker-scanner)
2. OHLCV ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸° (SQS í ëª¨ë‹ˆí„°ë§)
3. ê²°ê³¼ ì§‘ê³„ ë° ë¦¬í¬íŠ¸
4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

ğŸš€ ìµœì í™” íŠ¹ì§•:
- ë¹„ë™ê¸° Lambda í˜¸ì¶œ
- SQS í ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
- ìƒì„¸í•œ ì‹¤í–‰ ë¡œê·¸
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
"""

import json
import boto3
import logging
import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
lambda_client = boto3.client('lambda')
sqs_client = boto3.client('sqs')
cloudwatch = boto3.client('cloudwatch')

class MakenaideOrchestrator:
    """Makenaide íŒŒì´í”„ë¼ì¸ ì¡°ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.execution_id = f"exec_{int(time.time())}"
        
        # Lambda í•¨ìˆ˜ ì„¤ì •
        self.functions = {
            'ticker_scanner': 'makenaide-ticker-scanner',
            'ohlcv_collector': 'makenaide-ohlcv-collector',
            'api_gateway': 'makenaide-api-gateway'
        }
        
        # SQS í URL
        self.sqs_queue_url = 'https://sqs.ap-northeast-2.amazonaws.com/901361833359/makenaide-ohlcv-collection'
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
        self.timeouts = {
            'ticker_scan': 300,      # 5ë¶„
            'ohlcv_collection': 1800, # 30ë¶„
            'total_pipeline': 2400   # 40ë¶„
        }
        
        # ì‹¤í–‰ ê²°ê³¼ ì¶”ì 
        self.results = {
            'pipeline_start': self.start_time.isoformat(),
            'execution_id': self.execution_id,
            'steps': {},
            'metrics': {},
            'errors': []
        }
    
    def log_step(self, step_name: str, status: str, details: Dict = None):
        """ë‹¨ê³„ë³„ ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡"""
        step_info = {
            'status': status,
            'timestamp': datetime.now().isoformat(),
            'duration': (datetime.now() - self.start_time).total_seconds()
        }
        
        if details:
            step_info.update(details)
            
        self.results['steps'][step_name] = step_info
        
        logger.info(f"ğŸ“‹ {step_name}: {status}")
        if details:
            for key, value in details.items():
                logger.info(f"   - {key}: {value}")
    
    def send_custom_metric(self, metric_name: str, value: float, unit: str = 'Count'):
        """CloudWatch ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì „ì†¡"""
        try:
            cloudwatch.put_metric_data(
                Namespace='Makenaide/Pipeline',
                MetricData=[
                    {
                        'MetricName': metric_name,
                        'Value': value,
                        'Unit': unit,
                        'Timestamp': datetime.now(),
                        'Dimensions': [
                            {
                                'Name': 'ExecutionId',
                                'Value': self.execution_id
                            }
                        ]
                    }
                ]
            )
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”íŠ¸ë¦­ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def invoke_lambda_function(self, function_name: str, payload: Dict = None) -> Dict:
        """Lambda í•¨ìˆ˜ ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            logger.info(f"ğŸš€ {function_name} í˜¸ì¶œ ì‹œì‘")
            
            response = lambda_client.invoke(
                FunctionName=function_name,
                InvocationType='RequestResponse',
                Payload=json.dumps(payload or {})
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_payload = json.loads(response['Payload'].read())
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = (datetime.now() - self.start_time).total_seconds()
            
            if response['StatusCode'] == 200:
                logger.info(f"âœ… {function_name} ì‹¤í–‰ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
                return {
                    'success': True,
                    'response': response_payload,
                    'execution_time': execution_time,
                    'status_code': response['StatusCode']
                }
            else:
                logger.error(f"âŒ {function_name} ì‹¤í–‰ ì‹¤íŒ¨: {response['StatusCode']}")
                return {
                    'success': False,
                    'error': f"HTTP {response['StatusCode']}",
                    'response': response_payload,
                    'execution_time': execution_time
                }
                
        except Exception as e:
            logger.error(f"âŒ {function_name} í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e),
                'execution_time': (datetime.now() - self.start_time).total_seconds()
            }
    
    def wait_for_sqs_processing(self, max_wait_minutes: int = 30) -> Dict:
        """SQS í ì²˜ë¦¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°"""
        logger.info(f"â³ SQS í ì²˜ë¦¬ ëŒ€ê¸° ì‹œì‘ (ìµœëŒ€ {max_wait_minutes}ë¶„)")
        
        start_wait = datetime.now()
        max_wait_time = timedelta(minutes=max_wait_minutes)
        
        initial_messages = None
        stable_count = 0
        required_stable_checks = 3  # 3íšŒ ì—°ì† ì•ˆì • í™•ì¸
        
        while datetime.now() - start_wait < max_wait_time:
            try:
                # í ì†ì„± í™•ì¸
                response = sqs_client.get_queue_attributes(
                    QueueUrl=self.sqs_queue_url,
                    AttributeNames=['ApproximateNumberOfMessages', 'ApproximateNumberOfMessagesNotVisible']
                )
                
                visible_messages = int(response['Attributes'].get('ApproximateNumberOfMessages', 0))
                processing_messages = int(response['Attributes'].get('ApproximateNumberOfMessagesNotVisible', 0))
                total_messages = visible_messages + processing_messages
                
                if initial_messages is None:
                    initial_messages = total_messages
                    logger.info(f"ğŸ“Š ì´ˆê¸° ë©”ì‹œì§€ ìˆ˜: {initial_messages}")
                
                logger.info(f"ğŸ“ˆ í ìƒíƒœ: ëŒ€ê¸°={visible_messages}, ì²˜ë¦¬ì¤‘={processing_messages}, ì´={total_messages}")
                
                # íê°€ ë¹„ì–´ìˆê³  ì•ˆì •ì ì¸ ìƒíƒœì¸ì§€ í™•ì¸
                if total_messages == 0:
                    stable_count += 1
                    logger.info(f"âœ… í ì•ˆì • ìƒíƒœ í™•ì¸ {stable_count}/{required_stable_checks}")
                    
                    if stable_count >= required_stable_checks:
                        wait_time = (datetime.now() - start_wait).total_seconds()
                        logger.info(f"ğŸ‰ SQS ì²˜ë¦¬ ì™„ë£Œ í™•ì¸ ({wait_time:.2f}ì´ˆ)")
                        
                        return {
                            'success': True,
                            'wait_time': wait_time,
                            'initial_messages': initial_messages,
                            'final_messages': total_messages
                        }
                else:
                    stable_count = 0
                
                # 30ì´ˆ ëŒ€ê¸°
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"âŒ SQS ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
                time.sleep(30)
        
        # íƒ€ì„ì•„ì›ƒ
        wait_time = (datetime.now() - start_wait).total_seconds()
        logger.warning(f"âš ï¸ SQS ì²˜ë¦¬ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ ({wait_time:.2f}ì´ˆ)")
        
        return {
            'success': False,
            'error': 'timeout',
            'wait_time': wait_time,
            'initial_messages': initial_messages
        }
    
    def get_db_summary(self) -> Dict:
        """DB ìƒíƒœ ìš”ì•½ ì¡°íšŒ (API Gateway ê²½ìœ )"""
        try:
            logger.info("ğŸ“Š DB ìƒíƒœ ìš”ì•½ ì¡°íšŒ")
            
            # API Gateway Lambda í˜¸ì¶œë¡œ DB ìƒíƒœ ì¡°íšŒ
            result = self.invoke_lambda_function(
                self.functions['api_gateway'],
                {
                    'httpMethod': 'GET',
                    'path': '/db/summary',
                    'headers': {'Content-Type': 'application/json'}
                }
            )
            
            if result['success'] and 'body' in result['response']:
                summary_data = json.loads(result['response']['body'])
                logger.info("âœ… DB ìƒíƒœ ìš”ì•½ ì¡°íšŒ ì™„ë£Œ")
                return {
                    'success': True,
                    'data': summary_data
                }
            else:
                logger.warning("âš ï¸ DB ìƒíƒœ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨")
                return {
                    'success': False,
                    'error': 'DB summary query failed'
                }
                
        except Exception as e:
            logger.error(f"âŒ DB ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def execute_pipeline(self) -> Dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ Makenaide íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
            logger.info(f"ğŸ“‹ ì‹¤í–‰ ID: {self.execution_id}")
            
            # 1ë‹¨ê³„: í‹°ì»¤ ìŠ¤ìº”
            self.log_step("ticker_scan", "ì‹œì‘")
            step_start = datetime.now()
            
            ticker_result = self.invoke_lambda_function(
                self.functions['ticker_scanner']
            )
            
            step_duration = (datetime.now() - step_start).total_seconds()
            
            if ticker_result['success']:
                # ì‘ë‹µì—ì„œ í‹°ì»¤ ìˆ˜ì§‘ ì •ë³´ ì¶”ì¶œ
                if 'body' in ticker_result['response']:
                    ticker_data = json.loads(ticker_result['response']['body'])
                    processed_tickers = ticker_data.get('volume_filtered', 0)
                    
                    self.log_step("ticker_scan", "ì™„ë£Œ", {
                        'processed_tickers': processed_tickers,
                        'duration': step_duration,
                        'update_result': ticker_data.get('update_result', {})
                    })
                    
                    # ë©”íŠ¸ë¦­ ì „ì†¡
                    self.send_custom_metric('TickersScanned', processed_tickers)
                    self.send_custom_metric('TickerScanDuration', step_duration, 'Seconds')
                    
                    if processed_tickers == 0:
                        logger.warning("âš ï¸ ì²˜ë¦¬í•  í‹°ì»¤ê°€ ì—†ì–´ OHLCV ìˆ˜ì§‘ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                        self.log_step("ohlcv_collection", "ê±´ë„ˆëœ€", {
                            'reason': 'ì²˜ë¦¬í•  í‹°ì»¤ ì—†ìŒ'
                        })
                    else:
                        # 2ë‹¨ê³„: OHLCV ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸°
                        self.log_step("ohlcv_collection", "ëŒ€ê¸°")
                        step_start = datetime.now()
                        
                        sqs_result = self.wait_for_sqs_processing(max_wait_minutes=30)
                        step_duration = (datetime.now() - step_start).total_seconds()
                        
                        if sqs_result['success']:
                            self.log_step("ohlcv_collection", "ì™„ë£Œ", {
                                'wait_time': sqs_result['wait_time'],
                                'initial_messages': sqs_result['initial_messages'],
                                'duration': step_duration
                            })
                            
                            # ë©”íŠ¸ë¦­ ì „ì†¡
                            self.send_custom_metric('OHLCVCollectionTime', sqs_result['wait_time'], 'Seconds')
                            self.send_custom_metric('ProcessedMessages', sqs_result.get('initial_messages', 0))
                        else:
                            self.log_step("ohlcv_collection", "íƒ€ì„ì•„ì›ƒ", {
                                'wait_time': sqs_result['wait_time'],
                                'error': sqs_result.get('error', 'timeout'),
                                'duration': step_duration
                            })
                            self.results['errors'].append("OHLCV ìˆ˜ì§‘ íƒ€ì„ì•„ì›ƒ")
                else:
                    self.log_step("ticker_scan", "ì‹¤íŒ¨", {
                        'error': 'ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨',
                        'duration': step_duration
                    })
                    self.results['errors'].append("í‹°ì»¤ ìŠ¤ìº” ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨")
            else:
                self.log_step("ticker_scan", "ì‹¤íŒ¨", {
                    'error': ticker_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
                    'duration': step_duration
                })
                self.results['errors'].append(f"í‹°ì»¤ ìŠ¤ìº” ì‹¤íŒ¨: {ticker_result.get('error')}")
            
            # 3ë‹¨ê³„: DB ìƒíƒœ ìš”ì•½
            self.log_step("db_summary", "ì‹œì‘")
            step_start = datetime.now()
            
            db_result = self.get_db_summary()
            step_duration = (datetime.now() - step_start).total_seconds()
            
            if db_result['success']:
                self.log_step("db_summary", "ì™„ë£Œ", {
                    'duration': step_duration,
                    'summary': db_result.get('data', {})
                })
            else:
                self.log_step("db_summary", "ì‹¤íŒ¨", {
                    'error': db_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
                    'duration': step_duration
                })
                self.results['errors'].append(f"DB ìš”ì•½ ì‹¤íŒ¨: {db_result.get('error')}")
            
            # 4ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì •ë¦¬
            total_duration = (datetime.now() - self.start_time).total_seconds()
            
            self.results.update({
                'pipeline_end': datetime.now().isoformat(),
                'total_duration': total_duration,
                'success': len(self.results['errors']) == 0,
                'execution_summary': {
                    'ticker_scan_success': ticker_result['success'],
                    'ohlcv_collection_attempted': processed_tickers > 0 if 'processed_tickers' in locals() else False,
                    'db_summary_success': db_result['success'],
                    'total_errors': len(self.results['errors'])
                }
            })
            
            # ìµœì¢… ë©”íŠ¸ë¦­ ì „ì†¡
            self.send_custom_metric('PipelineDuration', total_duration, 'Seconds')
            self.send_custom_metric('PipelineSuccess', 1 if self.results['success'] else 0)
            self.send_custom_metric('PipelineErrors', len(self.results['errors']))
            
            logger.info(f"ğŸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ ({total_duration:.2f}ì´ˆ)")
            logger.info(f"ğŸ“Š ì„±ê³µ: {self.results['success']}, ì˜¤ë¥˜: {len(self.results['errors'])}ê°œ")
            
            return self.results
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            
            total_duration = (datetime.now() - self.start_time).total_seconds()
            self.results.update({
                'pipeline_end': datetime.now().isoformat(),
                'total_duration': total_duration,
                'success': False,
                'fatal_error': str(e)
            })
            
            # ì˜¤ë¥˜ ë©”íŠ¸ë¦­ ì „ì†¡
            self.send_custom_metric('PipelineFatalError', 1)
            self.send_custom_metric('PipelineDuration', total_duration, 'Seconds')
            
            return self.results

def lambda_handler(event, context):
    """Lambda ë©”ì¸ í•¸ë“¤ëŸ¬"""
    try:
        logger.info("ğŸ­ Makenaide Orchestrator ì‹œì‘")
        logger.info(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().isoformat()}")
        
        # ì´ë²¤íŠ¸ ì •ë³´ ë¡œê¹…
        if event:
            logger.info(f"ğŸ“¨ ì´ë²¤íŠ¸: {json.dumps(event, default=str)}")
        
        # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹¤í–‰
        orchestrator = MakenaideOrchestrator()
        results = orchestrator.execute_pipeline()
        
        # ì‘ë‹µ ìƒì„±
        response = {
            'statusCode': 200 if results['success'] else 500,
            'body': json.dumps({
                'message': 'Makenaide íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ',
                'success': results['success'],
                'execution_id': results['execution_id'],
                'total_duration': results['total_duration'],
                'steps_completed': len(results['steps']),
                'errors_count': len(results['errors']),
                'detailed_results': results,
                'timestamp': datetime.now().isoformat(),
                'version': 'orchestrator_v1.0'
            }, indent=2)
        }
        
        if results['success']:
            logger.info("ğŸ‰ Makenaide Orchestrator ì„±ê³µ ì™„ë£Œ")
        else:
            logger.error(f"âŒ Makenaide Orchestrator ì‹¤íŒ¨: {len(results['errors'])}ê°œ ì˜¤ë¥˜")
        
        return response
        
    except Exception as e:
        logger.error(f"âŒ Orchestrator ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e),
                'message': 'Orchestrator ì‹¤í–‰ ì‹¤íŒ¨',
                'timestamp': datetime.now().isoformat(),
                'version': 'orchestrator_v1.0'
            })
        } 