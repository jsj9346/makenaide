#!/usr/bin/env python3
"""
ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ ë° í†µí•© Lambda í•¨ìˆ˜

SQS ê²°ê³¼ íì—ì„œ ë¶„ì‚° ì²˜ë¦¬ëœ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  
DynamoDBì™€ S3ì— í†µí•© ì €ì¥í•˜ëŠ” Lambda í•¨ìˆ˜ì…ë‹ˆë‹¤.

Author: Result Collection System
Version: 1.0.0
"""

import json
import logging
import boto3
import os
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from decimal import Decimal
import traceback

# ë¡œê±° ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

class BacktestResultCollector:
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ ë° í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.sqs_client = boto3.client('sqs')
        self.dynamodb = boto3.resource('dynamodb')
        self.s3_client = boto3.client('s3')
        
        # í™˜ê²½ ë³€ìˆ˜
        self.result_queue_url = os.environ.get('RESULT_QUEUE_URL')
        self.s3_bucket = os.environ.get('S3_BUCKET', 'makenaide-backtest-data')
        self.results_table_name = os.environ.get('RESULTS_TABLE', 'makenaide-distributed-backtest-results')
        self.jobs_table_name = os.environ.get('JOBS_TABLE', 'makenaide-distributed-backtest-jobs')
        
        # DynamoDB í…Œì´ë¸”
        try:
            self.results_table = self.dynamodb.Table(self.results_table_name)
            self.jobs_table = self.dynamodb.Table(self.jobs_table_name)
            logger.info("âœ… DynamoDB í…Œì´ë¸” ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ DynamoDB í…Œì´ë¸” ì—°ê²° ì‹¤íŒ¨: {e}")
            self.results_table = None
            self.jobs_table = None
        
        self.collector_id = os.environ.get('AWS_LAMBDA_LOG_STREAM_NAME', str(uuid.uuid4()))
        
        logger.info(f"ğŸ”§ ResultCollector ì´ˆê¸°í™”: {self.collector_id}")
    
    def process_result_messages(self, sqs_messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """SQS ë©”ì‹œì§€ì—ì„œ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì²˜ë¦¬"""
        try:
            processed_count = 0
            aggregated_results = {}
            errors = []
            
            logger.info(f"ğŸ“Š ê²°ê³¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {len(sqs_messages)}ê°œ")
            
            for message in sqs_messages:
                try:
                    # SQS ë©”ì‹œì§€ êµ¬ì¡° íŒŒì‹±
                    message_body = message.get('body') or message.get('Body')
                    result_data = json.loads(message_body)
                    
                    job_id = result_data.get('job_id', 'unknown')
                    logger.info(f"ğŸ“¤ ê²°ê³¼ ì²˜ë¦¬ ì¤‘: {job_id}")
                    
                    # ê°œë³„ ê²°ê³¼ ì €ì¥
                    self._store_individual_result(result_data)
                    
                    # ì§‘ê³„ ë°ì´í„° ëˆ„ì 
                    self._aggregate_result(result_data, aggregated_results)
                    
                    # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
                    if self.jobs_table:
                        self._update_job_status(job_id, result_data)
                    
                    processed_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ê°œë³„ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    errors.append(str(e))
                    continue
            
            # ì§‘ê³„ ê²°ê³¼ ì €ì¥
            if aggregated_results:
                self._store_aggregated_results(aggregated_results)
            
            # S3ì— ë°°ì¹˜ ê²°ê³¼ ì €ì¥
            if processed_count > 0:
                batch_summary = self._create_batch_summary(aggregated_results, processed_count, errors)
                self._store_batch_to_s3(batch_summary)
            
            result_summary = {
                'collector_id': self.collector_id,
                'processed_count': processed_count,
                'error_count': len(errors),
                'aggregated_metrics': self._calculate_summary_metrics(aggregated_results),
                'processing_timestamp': datetime.now().isoformat(),
                'errors': errors[:5]  # ìµœëŒ€ 5ê°œ ì˜¤ë¥˜ë§Œ ë°˜í™˜
            }
            
            logger.info(f"âœ… ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì„±ê³µ, {len(errors)}ê°œ ì˜¤ë¥˜")
            return result_summary
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            return {
                'collector_id': self.collector_id,
                'error': str(e),
                'error_traceback': traceback.format_exc(),
                'failed_at': datetime.now().isoformat()
            }
    
    def _store_individual_result(self, result_data: Dict[str, Any]):
        """ê°œë³„ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ DynamoDBì— ì €ì¥"""
        try:
            if not self.results_table:
                logger.warning("âš ï¸ DynamoDB í…Œì´ë¸” ì—†ìŒ, ì €ì¥ ê±´ë„ˆëœ€")
                return
            
            job_id = result_data.get('job_id')
            worker_id = result_data.get('worker_id', 'unknown')
            
            # DynamoDB ì•„ì´í…œ ì¤€ë¹„
            item = {
                'job_id': job_id,
                'worker_id': worker_id,
                'status': result_data.get('status', 'UNKNOWN'),
                'completed_at': result_data.get('completed_at', datetime.now().isoformat()),
                'execution_time_seconds': Decimal(str(result_data.get('execution_time_seconds', 0))),
                'result_data': json.dumps(result_data.get('result_data', {}), default=str),
                'performance_metrics': json.dumps(result_data.get('performance_metrics', {}), default=str),
                'created_at': datetime.now().isoformat(),
                'ttl': int((datetime.now() + timedelta(days=90)).timestamp())  # 90ì¼ í›„ ìë™ ì‚­ì œ
            }
            
            # ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if result_data.get('error_message'):
                item['error_message'] = result_data['error_message']
                item['error_traceback'] = result_data.get('error_traceback', '')
            
            # DynamoDBì— ì €ì¥
            self.results_table.put_item(Item=item)
            
            logger.info(f"ğŸ“€ ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {job_id}")
            
        except Exception as e:
            logger.error(f"âŒ ê°œë³„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _aggregate_result(self, result_data: Dict[str, Any], aggregated_results: Dict[str, Any]):
        """ê²°ê³¼ ë°ì´í„°ë¥¼ ì§‘ê³„ì— ëˆ„ì """
        try:
            status = result_data.get('status', 'UNKNOWN')
            result_info = result_data.get('result_data', {})
            performance = result_data.get('performance_metrics', {})
            
            # ìƒíƒœë³„ ì¹´ìš´íŠ¸
            if 'status_counts' not in aggregated_results:
                aggregated_results['status_counts'] = {}
            
            aggregated_results['status_counts'][status] = aggregated_results['status_counts'].get(status, 0) + 1
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì§‘ê³„
            if status == 'COMPLETED' and result_info:
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                metrics_to_aggregate = [
                    'win_rate', 'avg_return', 'total_trades', 'mdd', 
                    'sharpe_ratio', 'kelly_fraction', 'data_points'
                ]
                
                if 'aggregated_metrics' not in aggregated_results:
                    aggregated_results['aggregated_metrics'] = {}
                
                for metric in metrics_to_aggregate:
                    if metric in result_info and isinstance(result_info[metric], (int, float)):
                        if metric not in aggregated_results['aggregated_metrics']:
                            aggregated_results['aggregated_metrics'][metric] = []
                        aggregated_results['aggregated_metrics'][metric].append(result_info[metric])
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ëˆ„ì 
                if 'performance_totals' not in aggregated_results:
                    aggregated_results['performance_totals'] = {
                        'total_execution_time': 0,
                        'total_memory_used': 0,
                        'total_data_points': 0,
                        'successful_jobs': 0
                    }
                
                aggregated_results['performance_totals']['total_execution_time'] += performance.get('processing_time', 0)
                aggregated_results['performance_totals']['total_memory_used'] += performance.get('memory_used_mb', 0)
                aggregated_results['performance_totals']['total_data_points'] += performance.get('data_points_processed', 0)
                aggregated_results['performance_totals']['successful_jobs'] += 1
                
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì§‘ê³„ ì‹¤íŒ¨: {e}")
    
    def _store_aggregated_results(self, aggregated_results: Dict[str, Any]):
        """ì§‘ê³„ëœ ê²°ê³¼ë¥¼ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥"""
        try:
            if not self.results_table:
                logger.warning("âš ï¸ DynamoDB í…Œì´ë¸” ì—†ìŒ, ì§‘ê³„ ì €ì¥ ê±´ë„ˆëœ€")
                return
            
            batch_id = f"batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{str(uuid.uuid4())[:8]}"
            
            # ì§‘ê³„ ë©”íŠ¸ë¦­ ê³„ì‚°
            summary_metrics = self._calculate_summary_metrics(aggregated_results)
            
            # DynamoDB ì•„ì´í…œ ìƒì„±
            aggregation_item = {
                'job_id': f"AGGREGATION-{batch_id}",
                'worker_id': 'RESULT_COLLECTOR',
                'status': 'AGGREGATED',
                'completed_at': datetime.now().isoformat(),
                'aggregation_summary': json.dumps(summary_metrics, default=str),
                'raw_aggregation': json.dumps(aggregated_results, default=str),
                'created_at': datetime.now().isoformat(),
                'ttl': int((datetime.now() + timedelta(days=365)).timestamp())  # 1ë…„ ë³´ê´€
            }
            
            # ì €ì¥
            self.results_table.put_item(Item=aggregation_item)
            
            logger.info(f"ğŸ“Š ì§‘ê³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {batch_id}")
            
        except Exception as e:
            logger.error(f"âŒ ì§‘ê³„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _calculate_summary_metrics(self, aggregated_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì§‘ê³„ ê²°ê³¼ì—ì„œ ìš”ì•½ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        try:
            metrics = aggregated_results.get('aggregated_metrics', {})
            performance = aggregated_results.get('performance_totals', {})
            status_counts = aggregated_results.get('status_counts', {})
            
            summary = {
                'total_jobs': sum(status_counts.values()),
                'successful_jobs': status_counts.get('COMPLETED', 0),
                'failed_jobs': status_counts.get('FAILED', 0),
                'success_rate': 0.0,
                'total_execution_time': performance.get('total_execution_time', 0),
                'average_execution_time': 0.0,
                'total_data_points_processed': performance.get('total_data_points', 0)
            }
            
            # ì„±ê³µë¥  ê³„ì‚°
            if summary['total_jobs'] > 0:
                summary['success_rate'] = summary['successful_jobs'] / summary['total_jobs']
            
            # í‰ê·  ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            if summary['successful_jobs'] > 0:
                summary['average_execution_time'] = summary['total_execution_time'] / summary['successful_jobs']
            
            # ë°±í…ŒìŠ¤íŒ… ë©”íŠ¸ë¦­ í‰ê· ê°’ ê³„ì‚°
            backtest_metrics = {}
            for metric_name, values in metrics.items():
                if values and len(values) > 0:
                    backtest_metrics[f"avg_{metric_name}"] = sum(values) / len(values)
                    backtest_metrics[f"min_{metric_name}"] = min(values)
                    backtest_metrics[f"max_{metric_name}"] = max(values)
                    backtest_metrics[f"count_{metric_name}"] = len(values)
            
            summary['backtest_metrics'] = backtest_metrics
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _update_job_status(self, job_id: str, result_data: Dict[str, Any]):
        """ì‘ì—… ìƒíƒœë¥¼ jobs í…Œì´ë¸”ì—ì„œ ì—…ë°ì´íŠ¸"""
        try:
            if not self.jobs_table:
                return
            
            update_expression = "SET job_status = :status, completed_at = :completed_at, result_summary = :result"
            expression_values = {
                ':status': result_data.get('status', 'UNKNOWN'),
                ':completed_at': result_data.get('completed_at', datetime.now().isoformat()),
                ':result': json.dumps(result_data.get('result_data', {}), default=str)
            }
            
            # ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if result_data.get('error_message'):
                update_expression += ", error_message = :error_msg"
                expression_values[':error_msg'] = result_data['error_message']
            
            self.jobs_table.update_item(
                Key={'job_id': job_id},
                UpdateExpression=update_expression,
                ExpressionAttributeValues=expression_values
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _create_batch_summary(self, aggregated_results: Dict[str, Any], processed_count: int, errors: List[str]) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ìƒì„±"""
        return {
            'batch_id': f"batch-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{str(uuid.uuid4())[:8]}",
            'collector_id': self.collector_id,
            'processed_timestamp': datetime.now().isoformat(),
            'processed_count': processed_count,
            'error_count': len(errors),
            'summary_metrics': self._calculate_summary_metrics(aggregated_results),
            'errors': errors,
            'raw_aggregation': aggregated_results
        }
    
    def _store_batch_to_s3(self, batch_summary: Dict[str, Any]):
        """ë°°ì¹˜ ìš”ì•½ì„ S3ì— ì €ì¥"""
        try:
            batch_id = batch_summary['batch_id']
            key = f"backtest-results/batch-summaries/{datetime.now().strftime('%Y/%m/%d')}/{batch_id}.json"
            
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=key,
                Body=json.dumps(batch_summary, indent=2, default=str),
                ContentType='application/json'
            )
            
            logger.info(f"â˜ï¸ S3 ë°°ì¹˜ ìš”ì•½ ì €ì¥ ì™„ë£Œ: s3://{self.s3_bucket}/{key}")
            
        except Exception as e:
            logger.error(f"âŒ S3 ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")

def lambda_handler(event, context):
    """Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ê¸° Lambda ì‹œì‘")
        logger.info(f"ğŸ“¥ ì´ë²¤íŠ¸: {json.dumps(event, default=str)}")
        
        collector = BacktestResultCollector()
        
        # SQS ë©”ì‹œì§€ ì²˜ë¦¬
        if 'Records' in event:
            sqs_messages = []
            for record in event['Records']:
                if record.get('eventSource') == 'aws:sqs':
                    sqs_messages.append(record)
            
            if sqs_messages:
                result = collector.process_result_messages(sqs_messages)
            else:
                result = {'message': 'No SQS messages found', 'processed_count': 0}
        
        # ì§ì ‘ í˜¸ì¶œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
        elif 'test_results' in event:
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì§ì ‘ ê²°ê³¼ ì²˜ë¦¬")
            test_messages = event['test_results']
            result = collector.process_result_messages(test_messages)
        
        else:
            result = {'message': 'No supported event type found', 'processed_count': 0}
        
        logger.info(f"âœ… ê²°ê³¼ ìˆ˜ì§‘ê¸° Lambda ì™„ë£Œ: {result}")
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Result collection completed successfully',
                'result': result
            }, default=str)
        }
        
    except Exception as e:
        logger.error(f"âŒ Lambda í•¸ë“¤ëŸ¬ ì‹¤íŒ¨: {e}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': 'Lambda execution failed',
                'message': str(e),
                'traceback': traceback.format_exc()
            })
        }

# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸
    test_event = {
        'test_results': [
            {
                'body': json.dumps({
                    'job_id': 'test-job-001',
                    'status': 'COMPLETED',
                    'worker_id': 'test-worker-001',
                    'execution_time_seconds': 2.5,
                    'completed_at': datetime.now().isoformat(),
                    'result_data': {
                        'strategy_name': 'Test_Strategy',
                        'win_rate': 0.65,
                        'avg_return': 0.08,
                        'total_trades': 150,
                        'mdd': -0.12,
                        'sharpe_ratio': 1.35,
                        'kelly_fraction': 0.15,
                        'data_points': 720
                    },
                    'performance_metrics': {
                        'processing_time': 2.5,
                        'memory_used_mb': 128,
                        'data_points_processed': 720
                    }
                })
            },
            {
                'body': json.dumps({
                    'job_id': 'test-job-002',
                    'status': 'FAILED',
                    'worker_id': 'test-worker-002',
                    'error_message': 'Test error',
                    'failed_at': datetime.now().isoformat()
                })
            }
        ]
    }
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
    os.environ['RESULT_QUEUE_URL'] = 'https://sqs.ap-northeast-2.amazonaws.com/123456789/test-result-queue'
    os.environ['S3_BUCKET'] = 'makenaide-backtest-data'
    
    print("ğŸ§ª ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    result = lambda_handler(test_event, None)
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {json.dumps(result, indent=2, default=str)}")