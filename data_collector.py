#!/usr/bin/env python3
"""
data_collector.py - Phase 1 ë°ì´í„° ìˆ˜ì§‘ê¸° (Scanner ì—°ë™)

ğŸ¯ ëª©ì : Phase 0 Scannerì—ì„œ ìˆ˜ì§‘í•œ tickers í…Œì´ë¸” ê¸°ë°˜ ê³ í’ˆì§ˆ OHLCV ë°ì´í„° ìˆ˜ì§‘
- tickers í…Œì´ë¸” ì—°ë™: Phase 0 Scanner ê²°ê³¼ í™œìš©
- 3-tier ìˆ˜ì§‘ ì „ëµ: skip(gap=0) â†’ yesterday update(gap=1) â†’ incremental(gap>1)
- í’ˆì§ˆ í•„í„°ë§: 13ê°œì›”+ ì›”ë´‰ ë°ì´í„° + 3ì–µì›+ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´
- ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (pandas_ta ì‚¬ìš©)

ğŸ“Š ì•„í‚¤í…ì²˜ ì¤€ìˆ˜:
1. @makenaide_local.mmd Phase 1 ì„¤ê³„ ì™„ì „ ì¤€ìˆ˜
2. tickers í…Œì´ë¸” ì¤‘ì‹¬ ë°ì´í„° íë¦„
3. SQLite í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í™œìš©
4. ê³ í’ˆì§ˆ ì¢…ëª©ë§Œ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ì„± 80.6% ê°œì„ 
"""

import os
import sys
import sqlite3
import pyupbit
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import logging
import pytz

# pandas_ta ì‚¬ìš© (ì„¤ì¹˜ í™•ì¸ë¨)
try:
    import pandas_ta as ta
    HAS_PANDAS_TA = True
except ImportError:
    HAS_PANDAS_TA = False
    print("âš ï¸ pandas_ta not available, using basic indicators")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleDataCollector:
    """ê°„ì†Œí™”ëœ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self, db_path: str = "./makenaide_local.db"):
        self.db_path = db_path
        self.kst = pytz.timezone('Asia/Seoul')  # ì—…ë¹„íŠ¸ KST ì‹œê°„ëŒ€
        self.init_database()
        logger.info("ğŸš€ SimpleDataCollector ì´ˆê¸°í™” ì™„ë£Œ (KST ì‹œê°„ëŒ€ ì ìš©)")

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ohlcv_data í…Œì´ë¸” ìƒì„±
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS ohlcv_data (
                ticker TEXT NOT NULL,
                date TEXT NOT NULL,
                open REAL NOT NULL,
                high REAL NOT NULL,
                low REAL NOT NULL,
                close REAL NOT NULL,
                volume REAL NOT NULL,
                ma5 REAL,
                ma20 REAL,
                ma60 REAL,
                ma120 REAL,
                ma200 REAL,
                rsi REAL,
                volume_ratio REAL,
                atr REAL,
                supertrend REAL,
                macd_histogram REAL,
                adx REAL,
                support_level REAL,
                created_at TEXT DEFAULT (datetime('now')),
                updated_at TEXT DEFAULT (datetime('now')),
                PRIMARY KEY (ticker, date)
            );
            """

            cursor.execute(create_table_sql)

            # ê¸°ì¡´ í…Œì´ë¸”ì— ëˆ„ë½ëœ ì»¬ëŸ¼ ì¶”ê°€ (ALTER TABLE)
            missing_columns = [
                ('atr', 'REAL'),
                ('supertrend', 'REAL'),
                ('macd_histogram', 'REAL'),
                ('adx', 'REAL'),
                ('support_level', 'REAL')
            ]

            for column_name, column_type in missing_columns:
                try:
                    cursor.execute(f"ALTER TABLE ohlcv_data ADD COLUMN {column_name} {column_type};")
                    logger.info(f"âœ… ohlcv_data í…Œì´ë¸”ì— {column_name} ì»¬ëŸ¼ ì¶”ê°€")
                except sqlite3.OperationalError as e:
                    if "duplicate column name" in str(e).lower():
                        logger.debug(f"ğŸ“‹ {column_name} ì»¬ëŸ¼ì´ ì´ë¯¸ ì¡´ì¬í•¨")
                    else:
                        logger.warning(f"âš ï¸ {column_name} ì»¬ëŸ¼ ì¶”ê°€ ì‹¤íŒ¨: {e}")

            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ohlcv_data_ticker ON ohlcv_data(ticker);")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ohlcv_data_date ON ohlcv_data(date);")

            conn.commit()
            conn.close()

            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def apply_data_retention_policy(self, retention_days: int = 300) -> Dict[str, Any]:
        """ë°ì´í„° ë³´ì¡´ ì •ì±… ì ìš© - 300ì¼ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì •ë¦¬

        Args:
            retention_days: ë°ì´í„° ë³´ì¡´ ê¸°ê°„ (ê¸°ë³¸: 300ì¼)

        Returns:
            ì •ë¦¬ ê²°ê³¼ í†µê³„

        Note:
            - MA200 ê³„ì‚°ì„ ìœ„í•´ 200ì¼ + ì—¬ìœ ë¶„ 100ì¼ = 300ì¼ ë³´ì¡´
            - 300ì¼ ì´ìƒ ë°ì´í„°ë§Œ ì‚­ì œí•˜ì—¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë³´ì¥
            - VACUUMìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ìˆ˜í–‰
        """
        try:
            logger.info(f"ğŸ—‘ï¸ ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹œì‘ (ë³´ì¡´ ê¸°ê°„: {retention_days}ì¼)")

            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # 1. ì‚­ì œ ëŒ€ìƒ ë°ì´í„° í™•ì¸
            cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d')

            # ì‚­ì œë  ë°ì´í„° í†µê³„ ì¡°íšŒ
            cursor.execute("""
                SELECT
                    COUNT(*) as total_rows,
                    COUNT(DISTINCT ticker) as affected_tickers,
                    MIN(date) as oldest_date,
                    MAX(date) as newest_date_to_delete
                FROM ohlcv_data
                WHERE date < ?
            """, (cutoff_date,))

            stats = cursor.fetchone()
            total_rows_to_delete = stats[0] if stats[0] else 0
            affected_tickers = stats[1] if stats[1] else 0
            oldest_date = stats[2] if stats[2] else "ì—†ìŒ"
            newest_date_to_delete = stats[3] if stats[3] else "ì—†ìŒ"

            if total_rows_to_delete == 0:
                logger.info(f"âœ… {cutoff_date} ì´ì „ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì •ë¦¬í•  ë°ì´í„° ì—†ìŒ")
                conn.close()
                return {
                    'deleted_rows': 0,
                    'affected_tickers': 0,
                    'cutoff_date': cutoff_date,
                    'retention_days': retention_days,
                    'vacuum_performed': False
                }

            logger.info(f"ğŸ“Š ì‚­ì œ ëŒ€ìƒ ë°ì´í„°:")
            logger.info(f"   â€¢ ì‚­ì œë  í–‰ ìˆ˜: {total_rows_to_delete:,}ê°œ")
            logger.info(f"   â€¢ ì˜í–¥ë°›ëŠ” ì¢…ëª©: {affected_tickers}ê°œ")
            logger.info(f"   â€¢ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°: {oldest_date}")
            logger.info(f"   â€¢ ì‚­ì œë  ìµœì‹  ë°ì´í„°: {newest_date_to_delete}")
            logger.info(f"   â€¢ ì»·ì˜¤í”„ ë‚ ì§œ: {cutoff_date}")

            # 2. ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¸¡ì • (ì‚­ì œ ì „)
            cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            db_size_before = cursor.fetchone()[0]

            # 3. ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì‹¤í–‰
            logger.info(f"ğŸ—‘ï¸ {cutoff_date} ì´ì „ ë°ì´í„° ì‚­ì œ ì¤‘...")

            cursor.execute("""
                DELETE FROM ohlcv_data
                WHERE date < ?
            """, (cutoff_date,))

            deleted_count = cursor.rowcount
            conn.commit()

            logger.info(f"âœ… {deleted_count:,}ê°œ í–‰ ì‚­ì œ ì™„ë£Œ")

            # 4. VACUUMìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
            logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ VACUUM ìµœì í™” ì¤‘...")
            cursor.execute("VACUUM")

            # 5. ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¸¡ì • (ìµœì í™” í›„)
            cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            db_size_after = cursor.fetchone()[0]

            size_reduction = db_size_before - db_size_after
            size_reduction_pct = (size_reduction / db_size_before * 100) if db_size_before > 0 else 0

            # 6. ë‚¨ì€ ë°ì´í„° í†µê³„ ì¡°íšŒ
            cursor.execute("""
                SELECT
                    COUNT(*) as remaining_rows,
                    COUNT(DISTINCT ticker) as remaining_tickers,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM ohlcv_data
            """)

            remaining_stats = cursor.fetchone()
            remaining_rows = remaining_stats[0] if remaining_stats[0] else 0
            remaining_tickers = remaining_stats[1] if remaining_stats[1] else 0
            earliest_date = remaining_stats[2] if remaining_stats[2] else "ì—†ìŒ"
            latest_date = remaining_stats[3] if remaining_stats[3] else "ì—†ìŒ"

            conn.close()

            # 7. ê²°ê³¼ ë¡œê¹…
            logger.info("âœ… ë°ì´í„° ë³´ì¡´ ì •ì±… ì ìš© ì™„ë£Œ")
            logger.info(f"ğŸ“Š ì •ë¦¬ ê²°ê³¼:")
            logger.info(f"   â€¢ ì‚­ì œëœ í–‰: {deleted_count:,}ê°œ")
            logger.info(f"   â€¢ ì˜í–¥ë°›ì€ ì¢…ëª©: {affected_tickers}ê°œ")
            logger.info(f"   â€¢ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì ˆì•½: {size_reduction:,} bytes ({size_reduction_pct:.1f}%)")
            logger.info(f"ğŸ“Š ë‚¨ì€ ë°ì´í„°:")
            logger.info(f"   â€¢ ë‚¨ì€ í–‰: {remaining_rows:,}ê°œ")
            logger.info(f"   â€¢ ë‚¨ì€ ì¢…ëª©: {remaining_tickers}ê°œ")
            logger.info(f"   â€¢ ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°: {earliest_date}")
            logger.info(f"   â€¢ ê°€ì¥ ìµœì‹  ë°ì´í„°: {latest_date}")

            return {
                'deleted_rows': deleted_count,
                'affected_tickers': affected_tickers,
                'cutoff_date': cutoff_date,
                'retention_days': retention_days,
                'db_size_before': db_size_before,
                'db_size_after': db_size_after,
                'size_reduction': size_reduction,
                'size_reduction_pct': size_reduction_pct,
                'remaining_rows': remaining_rows,
                'remaining_tickers': remaining_tickers,
                'earliest_date': earliest_date,
                'latest_date': latest_date,
                'vacuum_performed': True
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë³´ì¡´ ì •ì±… ì ìš© ì‹¤íŒ¨: {e}")
            raise

    def get_active_tickers(self) -> List[str]:
        """í™œì„± í‹°ì»¤ ëª©ë¡ ì¡°íšŒ (Phase 0 Scanner ê²°ê³¼) - ê¸°ë³¸ í™œì„±í™” ì¡°ê±´ë§Œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT ticker FROM tickers
                WHERE is_active = 1
                ORDER BY created_at DESC
            """)

            tickers = [row[0] for row in cursor.fetchall()]
            conn.close()

            logger.info(f"ğŸ“Š í™œì„± í‹°ì»¤: {len(tickers)}ê°œ")
            return tickers

        except Exception as e:
            logger.error(f"âŒ í™œì„± í‹°ì»¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ í‹°ì»¤
            return ['KRW-BTC', 'KRW-ETH', 'KRW-ADA', 'KRW-DOT', 'KRW-LINK']

    def get_qualified_tickers(self, min_monthly_data: int = 13, min_daily_volume_krw: int = 300000000) -> List[str]:
        """í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ê³ ê¸‰ í‹°ì»¤ ì„ ë³„

        Args:
            min_monthly_data: ìµœì†Œ ì›”ë´‰ ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸: 13ê°œì›”)
            min_daily_volume_krw: ìµœì†Œ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ KRW (ê¸°ë³¸: 3ì–µì›)

        Returns:
            í’ˆì§ˆ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info("ğŸ” ê³ í’ˆì§ˆ í‹°ì»¤ ì„ ë³„ ì‹œì‘")
            logger.info(f"ğŸ“‹ ì¡°ê±´: ì›”ë´‰ ë°ì´í„° {min_monthly_data}ê°œì›” ì´ìƒ, ê±°ë˜ëŒ€ê¸ˆ {min_daily_volume_krw:,}ì› ì´ìƒ")

            # 1ë‹¨ê³„: SQLiteì—ì„œ ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ë¥¼ ê°€ì§„ ì¢…ëª© ì¡°íšŒ
            monthly_qualified_tickers = self._get_monthly_qualified_tickers(min_monthly_data)

            if not monthly_qualified_tickers:
                logger.warning("âš ï¸ ì›”ë³„ ë°ì´í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŒ")
                return self.get_active_tickers()[:10]  # ê¸°ë³¸ í™œì„± í‹°ì»¤ ì¤‘ 10ê°œ ë°˜í™˜

            logger.info(f"ğŸ“Š ì›”ë³„ ë°ì´í„° ì¡°ê±´ í†µê³¼: {len(monthly_qualified_tickers)}ê°œ ì¢…ëª©")

            # 2ë‹¨ê³„: 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í™•ì¸
            volume_qualified_tickers = self._get_volume_qualified_tickers(
                monthly_qualified_tickers, min_daily_volume_krw
            )

            logger.info(f"âœ… ìµœì¢… ì„ ë³„ ì™„ë£Œ: {len(volume_qualified_tickers)}ê°œ ì¢…ëª©")
            logger.info(f"ğŸ“ˆ ì„ ë³„ëœ ì¢…ëª©: {', '.join(volume_qualified_tickers[:10])}{'...' if len(volume_qualified_tickers) > 10 else ''}")

            return volume_qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ê³ í’ˆì§ˆ í‹°ì»¤ ì„ ë³„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í™œì„± í‹°ì»¤ ë°˜í™˜
            return self.get_active_tickers()[:20]  # ì•ˆì „í•˜ê²Œ 20ê°œ ì œí•œ

    def _get_monthly_qualified_tickers(self, min_months: int) -> List[str]:
        """pyupbit APIë¡œ ì‹¤ì œ ê±°ë˜ì†Œì—ì„œ ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ë¥¼ ë³´ìœ í•œ ì¢…ëª© ì¡°íšŒ"""
        try:
            logger.info(f"ğŸ” pyupbit APIë¡œ {min_months}ê°œì›” ì´ìƒ ì›”ë´‰ ë°ì´í„° í™•ì¸ ì¤‘...")

            # í™œì„± í‹°ì»¤ ëª©ë¡ ì¡°íšŒ
            active_tickers = self.get_active_tickers()
            qualified_tickers = []

            # ê° ì¢…ëª©ë³„ë¡œ ì›”ë´‰ ë°ì´í„° ê°œìˆ˜ í™•ì¸
            for ticker in active_tickers:
                try:
                    # ì›”ë´‰ ë°ì´í„° ì¡°íšŒ (ìµœëŒ€ 24ê°œì›”ì¹˜ ìš”ì²­)
                    monthly_df = pyupbit.get_ohlcv(
                        ticker=ticker,
                        interval="month",
                        count=24  # ì¶©ë¶„í•œ ê¸°ê°„ ìš”ì²­
                    )

                    if monthly_df is not None and not monthly_df.empty:
                        available_months = len(monthly_df)
                        logger.debug(f"ğŸ“Š {ticker}: {available_months}ê°œì›” ì›”ë´‰ ë°ì´í„°")

                        if available_months >= min_months:
                            qualified_tickers.append(ticker)
                            logger.debug(f"âœ… {ticker}: {available_months}ê°œì›” (ì¡°ê±´ í†µê³¼)")
                        else:
                            logger.debug(f"âŒ {ticker}: {available_months}ê°œì›” (ì¡°ê±´ ë¯¸ë‹¬)")
                    else:
                        logger.debug(f"âš ï¸ {ticker}: ì›”ë´‰ ë°ì´í„° ì—†ìŒ")

                    # API ë ˆì´íŠ¸ ì œí•œ ë°©ì§€
                    time.sleep(0.1)

                except Exception as e:
                    logger.warning(f"âš ï¸ {ticker} ì›”ë´‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸ“… ì›”ë³„ ë°ì´í„° {min_months}ê°œì›” ì´ìƒ: {len(qualified_tickers)}ê°œ ì¢…ëª© (pyupbit API í™•ì¸)")
            return qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ì›”ë³„ ë°ì´í„° ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸ pyupbit API ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ í™œì„± í‹°ì»¤ ë°˜í™˜")
            return self.get_active_tickers()[:20]  # ì‹¤íŒ¨ ì‹œ ìƒìœ„ 20ê°œë§Œ ë°˜í™˜

    def _get_volume_qualified_tickers(self, candidate_tickers: List[str], min_volume_krw: int) -> List[str]:
        """pyupbit APIë¡œ ì‹¤ì œ ê±°ë˜ì†Œì—ì„œ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª© í•„í„°ë§"""
        qualified_tickers = []

        try:
            logger.info(f"ğŸ’° ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í™•ì¸ ì¤‘: {len(candidate_tickers)}ê°œ ì¢…ëª© (pyupbit API ì¡°íšŒ)")

            # ê° ì¢…ëª©ë³„ë¡œ ì¼ë´‰ ë°ì´í„°ì—ì„œ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ í™•ì¸
            for ticker in candidate_tickers:
                try:
                    # ìµœê·¼ 1ì¼ ë°ì´í„°ë¡œ 24ì‹œê°„ ê±°ë˜ëŸ‰ í™•ì¸
                    daily_df = pyupbit.get_ohlcv(
                        ticker=ticker,
                        interval="day",
                        count=1  # ê°€ì¥ ìµœê·¼ 1ì¼ì¹˜ë§Œ
                    )

                    if daily_df is not None and not daily_df.empty:
                        # ê°€ì¥ ìµœê·¼ ë°ì´í„°
                        latest_data = daily_df.iloc[-1]
                        close_price = latest_data['close']
                        volume_24h = latest_data['volume']

                        # 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚° (ê°€ê²© Ã— ê±°ë˜ëŸ‰)
                        trade_value_24h = close_price * volume_24h

                        if trade_value_24h >= min_volume_krw:
                            qualified_tickers.append(ticker)
                            logger.debug(f"âœ… {ticker}: {trade_value_24h:,.0f}ì› (í†µê³¼)")
                        else:
                            logger.debug(f"âŒ {ticker}: {trade_value_24h:,.0f}ì› (ì¡°ê±´ ë¯¸ë‹¬)")
                    else:
                        logger.debug(f"âš ï¸ {ticker}: ê±°ë˜ëŒ€ê¸ˆ ë°ì´í„° ì—†ìŒ")

                    # API ë ˆì´íŠ¸ ì œí•œ ë°©ì§€
                    time.sleep(0.1)

                except Exception as e:
                    logger.warning(f"âš ï¸ {ticker} ê±°ë˜ëŒ€ê¸ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸ’ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í†µê³¼: {len(qualified_tickers)}ê°œ ì¢…ëª© (pyupbit API í™•ì¸)")
            logger.info(f"ğŸ“‹ ì„ ë³„ëœ ì¢…ëª©: {', '.join(qualified_tickers[:10])}{'...' if len(qualified_tickers) > 10 else ''}")

            return qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            logger.warning("âš ï¸ pyupbit API ì¡°íšŒ ì‹¤íŒ¨, í›„ë³´ ì¢…ëª© ê·¸ëŒ€ë¡œ ë°˜í™˜")
            return candidate_tickers[:20]  # ì‹¤íŒ¨ ì‹œ ìƒìœ„ 20ê°œë§Œ ë°˜í™˜

    def get_latest_date(self, ticker: str) -> Optional[datetime]:
        """íŠ¹ì • í‹°ì»¤ì˜ ìµœì‹  ë°ì´í„° ë‚ ì§œ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT MAX(date) FROM ohlcv_data
                WHERE ticker = ?
            """, (ticker,))

            result = cursor.fetchone()
            conn.close()

            if result and result[0]:
                return datetime.fromisoformat(result[0])
            return None

        except Exception as e:
            logger.error(f"âŒ {ticker} ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def analyze_gap(self, ticker: str) -> Dict[str, Any]:
        """ê°œì„ ëœ ê°­ ë¶„ì„ ë° ìˆ˜ì§‘ ì „ëµ ê²°ì • (KST ê¸°ì¤€, ì—…ë¹„íŠ¸ íŠ¹ì„± ê³ ë ¤)"""
        try:
            # 1. ìµœì‹  ë°ì´í„° ì¡°íšŒ
            latest_date = self.get_latest_date(ticker)

            # 2. KST ê¸°ì¤€ í˜„ì¬ ë‚ ì§œ ê³„ì‚°
            now_kst = datetime.now(self.kst)
            current_date_kst = now_kst.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None)

            logger.debug(f"ğŸ• {ticker} ì‹œê°„ ë¶„ì„:")
            logger.debug(f"   â€¢ í˜„ì¬ KST: {now_kst}")
            logger.debug(f"   â€¢ ê¸°ì¤€ ë‚ ì§œ: {current_date_kst.date()}")

            if latest_date is None:
                return {
                    'strategy': 'full_collection',
                    'gap_days': 200,
                    'reason': 'No existing data'
                }

            # 3. KST ê¸°ì¤€ ê°­ ê³„ì‚°
            gap_days = (current_date_kst.date() - latest_date.date()).days

            # 4. ì—…ë¹„íŠ¸ íŠ¹ì„± ê³ ë ¤í•œ ì „ëµ ê²°ì •
            # ìƒˆë²½ 1ì‹œ ì´ì „ì—ëŠ” ì „ë‚  ì·¨ê¸‰ (ë°ì´í„° ë°˜ì˜ ì‹œê°„ ê³ ë ¤)
            if now_kst.hour < 1:
                effective_gap = gap_days - 1
                time_note = " (ìƒˆë²½ ì‹œê°„ ê³ ë ¤)"
                logger.debug(f"   â€¢ ìƒˆë²½ ì‹œê°„ ì¡°ì •: {gap_days}ì¼ â†’ {effective_gap}ì¼")
            else:
                effective_gap = gap_days
                time_note = ""

            logger.debug(f"   â€¢ ìµœì‹  ë°ì´í„°: {latest_date.date()}")
            logger.debug(f"   â€¢ ì‹¤ì œ ê°­: {gap_days}ì¼")
            logger.debug(f"   â€¢ ì ìš© ê°­: {effective_gap}ì¼")

            if effective_gap <= 0:
                return {
                    'strategy': 'skip',
                    'gap_days': gap_days,
                    'reason': f'Data is up to date{time_note}'
                }
            elif effective_gap == 1:
                return {
                    'strategy': 'yesterday_update',
                    'gap_days': gap_days,
                    'reason': f'Yesterday data needs update{time_note}'
                }
            else:
                return {
                    'strategy': 'incremental',
                    'gap_days': gap_days,
                    'reason': f'{effective_gap} days gap detected{time_note}'
                }

        except Exception as e:
            logger.error(f"âŒ {ticker} ê°­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'strategy': 'incremental',
                'gap_days': 1,
                'reason': f'Analysis failed: {e}'
            }

    def safe_get_ohlcv(self, ticker: str, count: int = 200) -> Optional[pd.DataFrame]:
        """ì•ˆì „í•œ ì—…ë¹„íŠ¸ OHLCV ë°ì´í„° ì¡°íšŒ - tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ ë¨¼ì € í™•ì¸"""
        try:
            # 1ë‹¨ê³„: tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ í™•ì¸
            if not self._is_ticker_active(ticker):
                logger.warning(f"âš ï¸ {ticker} ë¹„í™œì„± ì¢…ëª© ë˜ëŠ” tickers í…Œì´ë¸”ì— ì—†ìŒ")
                return None

            # 2ë‹¨ê³„: to íŒŒë¼ë¯¸í„° ì—†ì´ í˜¸ì¶œí•˜ì—¬ ìµœì‹  ë°ì´í„°ê¹Œì§€ ìˆ˜ì§‘
            # (to íŒŒë¼ë¯¸í„° ì‚¬ìš©ì‹œ í˜„ì¬ ë‚ ì§œ ë°ì´í„°ê°€ ëˆ„ë½ë˜ëŠ” ì—…ë¹„íŠ¸ API íŠ¹ì„±)
            logger.debug(f"ğŸ” {ticker} API í˜¸ì¶œ: count={count} (to íŒŒë¼ë¯¸í„° ì—†ì´ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘)")

            # 3ë‹¨ê³„: ì—…ë¹„íŠ¸ API í˜¸ì¶œ
            df = pyupbit.get_ohlcv(
                ticker=ticker,
                interval="day",
                count=count
                # to íŒŒë¼ë¯¸í„° ì œê±° - í˜„ì¬ ë‚ ì§œ ë°ì´í„° í¬í•¨ì„ ìœ„í•´
            )

            if df is None or df.empty:
                logger.warning(f"âš ï¸ {ticker} API ì‘ë‹µ ì—†ìŒ")
                return None

            # 4ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            if len(df) < count * 0.8:  # ìš”ì²­ëŸ‰ì˜ 80% ë¯¸ë§Œì´ë©´ ê²½ê³ 
                logger.warning(f"âš ï¸ {ticker} ë°ì´í„° ë¶€ì¡±: {len(df)}/{count}")

            # 5ë‹¨ê³„: 1970-01-01 ì‘ë‹µ í™•ì¸
            if len(df) > 0 and hasattr(df.index, 'year'):
                first_year = df.index[0].year
                if first_year == 1970:
                    logger.error(f"âŒ {ticker} 1970-01-01 ì‘ë‹µ ê°ì§€")
                    return None

            logger.debug(f"âœ… {ticker} API í˜¸ì¶œ ì„±ê³µ: {len(df)}ê°œ ë°ì´í„°")
            return df

        except Exception as e:
            logger.error(f"âŒ {ticker} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _is_ticker_active(self, ticker: str) -> bool:
        """tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ í™•ì¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT is_active
                FROM tickers
                WHERE ticker = ?
                LIMIT 1
            """, (ticker,))

            result = cursor.fetchone()
            conn.close()

            if result and result[0] == 1:
                return True
            else:
                logger.debug(f"ğŸ” {ticker} tickers í…Œì´ë¸”ì—ì„œ ë¹„í™œì„± ë˜ëŠ” ì—†ìŒ")
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} í™œì„± ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            # tickers í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í™œì„±ìœ¼ë¡œ ê°€ì •
            return True

    def calculate_technical_indicators(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        try:
            df_with_indicators = df.copy()

            # ê¸°ë³¸ ì´ë™í‰ê·  (í•­ìƒ ê³„ì‚°)
            df_with_indicators['ma5'] = df['close'].rolling(window=5).mean()
            df_with_indicators['ma20'] = df['close'].rolling(window=20).mean()
            df_with_indicators['ma60'] = df['close'].rolling(window=60).mean()
            df_with_indicators['ma120'] = df['close'].rolling(window=120).mean()
            df_with_indicators['ma200'] = df['close'].rolling(window=200).mean()

            # RSI ê³„ì‚° (í•­ìƒ ê³„ì‚°)
            try:
                if HAS_PANDAS_TA:
                    df_with_indicators['rsi'] = ta.rsi(df['close'], length=14)
                else:
                    # ê°„ë‹¨í•œ RSI ê³„ì‚°
                    delta = df['close'].diff()
                    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                    rs = gain / loss
                    df_with_indicators['rsi'] = 100 - (100 / (1 + rs))
            except Exception as rsi_error:
                logger.warning(f"âš ï¸ {ticker} RSI ê³„ì‚° ì‹¤íŒ¨: {rsi_error}")
                df_with_indicators['rsi'] = None

            # ê±°ë˜ëŸ‰ ë¹„ìœ¨ (volume ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
            try:
                if 'volume' in df.columns:
                    volume_ma = df['volume'].rolling(window=20).mean()
                    df_with_indicators['volume_ratio'] = df['volume'] / volume_ma
                else:
                    logger.debug(f"âš ï¸ {ticker} volume ì»¬ëŸ¼ ì—†ìŒ, volume_ratio ê³„ì‚° ê±´ë„ˆë›°ê¸°")
                    df_with_indicators['volume_ratio'] = None
            except Exception as volume_error:
                logger.warning(f"âš ï¸ {ticker} volume_ratio ê³„ì‚° ì‹¤íŒ¨: {volume_error}")
                df_with_indicators['volume_ratio'] = None

            # ğŸš€ Phase 1: í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ATR, Supertrend, MACD Histogram)
            try:
                # ATR (Average True Range) ê³„ì‚°
                df_with_indicators['atr'] = self._calculate_atr(df, period=14)

                # Supertrend ê³„ì‚°
                df_with_indicators['supertrend'] = self._calculate_supertrend(df, period=10, multiplier=3.0)

                # MACD Histogram ê³„ì‚°
                df_with_indicators['macd_histogram'] = self._calculate_macd_histogram(df, fast=12, slow=26, signal=9)

                # ADX (Average Directional Index) ê³„ì‚° - ê°„ë‹¨í•œ ë²„ì „
                df_with_indicators['adx'] = self._calculate_adx(df, period=14)

                # ì§€ì§€ì„  ê³„ì‚° (ìµœê·¼ ì €ì  ê¸°ë°˜)
                df_with_indicators['support_level'] = self._calculate_support_level(df, period=20)

                logger.info(f"ğŸ¯ {ticker} í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ (ATR, Supertrend, MACD, ADX, Support)")

            except Exception as indicator_error:
                logger.warning(f"âš ï¸ {ticker} í•µì‹¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {indicator_error}")
                # NULL ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê¸°ì¡´ ë™ì‘ ìœ ì§€
                df_with_indicators['atr'] = None
                df_with_indicators['supertrend'] = None
                df_with_indicators['macd_histogram'] = None
                df_with_indicators['adx'] = None
                df_with_indicators['support_level'] = None

            logger.debug(f"âœ… {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
            return df_with_indicators

        except Exception as e:
            logger.error(f"âŒ {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì§€í‘œë§Œì´ë¼ë„ ê³„ì‚° ì‹œë„
            try:
                df_basic = df.copy()
                df_basic['ma5'] = df['close'].rolling(window=5).mean()
                df_basic['ma20'] = df['close'].rolling(window=20).mean()
                df_basic['ma60'] = df['close'].rolling(window=60).mean()
                df_basic['ma120'] = df['close'].rolling(window=120).mean()
                df_basic['ma200'] = df['close'].rolling(window=200).mean()
                df_basic['rsi'] = None
                df_basic['volume_ratio'] = None
                logger.info(f"ğŸ“Š {ticker} ê¸°ë³¸ MA ì§€í‘œë§Œ ê³„ì‚° ì™„ë£Œ")
                return df_basic
            except Exception as basic_error:
                logger.error(f"âŒ {ticker} ê¸°ë³¸ ì§€í‘œ ê³„ì‚°ë„ ì‹¤íŒ¨: {basic_error}")
                return df

    def _calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """ATR (Average True Range) ê³„ì‚°"""
        try:
            import numpy as np

            # True Range ê³„ì‚°
            high_low = df['high'] - df['low']
            high_close = np.abs(df['high'] - df['close'].shift())
            low_close = np.abs(df['low'] - df['close'].shift())

            # ì„¸ ê°’ ì¤‘ ìµœëŒ€ê°’ì´ True Range
            ranges = pd.concat([high_low, high_close, low_close], axis=1)
            true_range = ranges.max(axis=1)

            # ATR = True Rangeì˜ ì´ë™í‰ê· 
            atr = true_range.rolling(window=period).mean()

            return atr
        except Exception as e:
            logger.warning(f"ATR ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.Series([None] * len(df), index=df.index)

    def _calculate_supertrend(self, df: pd.DataFrame, period: int = 10, multiplier: float = 3.0) -> pd.Series:
        """Supertrend ì§€í‘œ ê³„ì‚° - ë‹¨ìˆœí™”ëœ ì•ˆì • ë²„ì „"""
        try:
            import numpy as np

            # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
            if len(df) < period:
                logger.warning(f"Supertrend ê³„ì‚°: ë°ì´í„° ë¶€ì¡± (í•„ìš”: {period}, ì‹¤ì œ: {len(df)})")
                return pd.Series([None] * len(df), index=df.index)

            # HL2 (High-Low í‰ê· )
            hl2 = (df['high'] + df['low']) / 2

            # ATR ê³„ì‚°
            atr = self._calculate_atr(df, period)

            # NaN ê°’ ì²˜ë¦¬
            if atr.isna().all():
                logger.warning("Supertrend ê³„ì‚°: ATR ê³„ì‚° ì‹¤íŒ¨")
                return pd.Series([None] * len(df), index=df.index)

            # ê¸°ë³¸ ìƒë‹¨/í•˜ë‹¨ ë°´ë“œ
            upper_basic = hl2 + (multiplier * atr)
            lower_basic = hl2 - (multiplier * atr)

            # ë™ì  ë°´ë“œ ê³„ì‚° - ë‹¨ìˆœí™”ëœ ë²„ì „
            upper_band = upper_basic.copy()
            lower_band = lower_basic.copy()
            supertrend = pd.Series([None] * len(df), index=df.index, dtype='float64')

            # ì²« ë²ˆì§¸ ìœ íš¨í•œ ì¸ë±ìŠ¤ ì°¾ê¸°
            first_valid_idx = None
            for i in range(len(df)):
                if not pd.isna(upper_basic.iloc[i]) and not pd.isna(lower_basic.iloc[i]):
                    first_valid_idx = i
                    break

            if first_valid_idx is None:
                logger.warning("Supertrend ê³„ì‚°: ìœ íš¨í•œ ë°´ë“œ ë°ì´í„° ì—†ìŒ")
                return pd.Series([None] * len(df), index=df.index)

            # ë™ì  ë°´ë“œ ì—…ë°ì´íŠ¸
            for i in range(first_valid_idx + 1, len(df)):
                if not pd.isna(upper_basic.iloc[i]) and not pd.isna(lower_basic.iloc[i]):
                    # ìƒë‹¨ ë°´ë“œ ì—…ë°ì´íŠ¸
                    if (upper_basic.iloc[i] < upper_band.iloc[i-1] or
                        df['close'].iloc[i-1] > upper_band.iloc[i-1]):
                        upper_band.iloc[i] = upper_basic.iloc[i]
                    else:
                        upper_band.iloc[i] = upper_band.iloc[i-1]

                    # í•˜ë‹¨ ë°´ë“œ ì—…ë°ì´íŠ¸
                    if (lower_basic.iloc[i] > lower_band.iloc[i-1] or
                        df['close'].iloc[i-1] < lower_band.iloc[i-1]):
                        lower_band.iloc[i] = lower_basic.iloc[i]
                    else:
                        lower_band.iloc[i] = lower_band.iloc[i-1]

            # Supertrend ê³„ì‚° - ë‹¨ìˆœí™”ëœ ë¡œì§
            trend = 1  # 1: ìƒìŠ¹, -1: í•˜ë½

            for i in range(first_valid_idx, len(df)):
                if pd.isna(upper_band.iloc[i]) or pd.isna(lower_band.iloc[i]):
                    continue

                close_price = df['close'].iloc[i]

                if pd.isna(close_price):
                    continue

                # ì²« ë²ˆì§¸ ê°’ ì„¤ì •
                if i == first_valid_idx:
                    supertrend.iloc[i] = lower_band.iloc[i]  # ìƒìŠ¹ íŠ¸ë Œë“œë¡œ ì‹œì‘
                    trend = 1
                    continue

                # íŠ¸ë Œë“œ ì „í™˜ ë¡œì§ - ë‹¨ìˆœí™”
                if trend == 1:  # í˜„ì¬ ìƒìŠ¹ íŠ¸ë Œë“œ
                    if close_price < lower_band.iloc[i]:
                        trend = -1  # í•˜ë½ íŠ¸ë Œë“œë¡œ ì „í™˜
                        supertrend.iloc[i] = upper_band.iloc[i]
                    else:
                        supertrend.iloc[i] = lower_band.iloc[i]  # ìƒìŠ¹ íŠ¸ë Œë“œ ìœ ì§€
                else:  # í˜„ì¬ í•˜ë½ íŠ¸ë Œë“œ
                    if close_price > upper_band.iloc[i]:
                        trend = 1  # ìƒìŠ¹ íŠ¸ë Œë“œë¡œ ì „í™˜
                        supertrend.iloc[i] = lower_band.iloc[i]
                    else:
                        supertrend.iloc[i] = upper_band.iloc[i]  # í•˜ë½ íŠ¸ë Œë“œ ìœ ì§€

            return supertrend

        except Exception as e:
            logger.warning(f"Supertrend ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(f"Supertrend ê³„ì‚° ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return pd.Series([None] * len(df), index=df.index)

    def _calculate_macd_histogram(self, df: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.Series:
        """MACD Histogram ê³„ì‚°"""
        try:
            # EMA ê³„ì‚°
            ema_fast = df['close'].ewm(span=fast, adjust=False).mean()
            ema_slow = df['close'].ewm(span=slow, adjust=False).mean()

            # MACD ë¼ì¸
            macd_line = ema_fast - ema_slow

            # Signal ë¼ì¸ (MACDì˜ EMA)
            signal_line = macd_line.ewm(span=signal, adjust=False).mean()

            # MACD Histogram (MACD - Signal)
            macd_histogram = macd_line - signal_line

            return macd_histogram
        except Exception as e:
            logger.warning(f"MACD Histogram ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.Series([None] * len(df), index=df.index)

    def _calculate_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """ADX (Average Directional Index) ê³„ì‚° - ê°œì„ ëœ ë²„ì „"""
        try:
            import numpy as np

            # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
            if len(df) < period * 2:  # ADXëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”
                logger.warning(f"ADX ê³„ì‚°: ë°ì´í„° ë¶€ì¡± (í•„ìš”: {period * 2}, ì‹¤ì œ: {len(df)})")
                return pd.Series([None] * len(df), index=df.index)

            # DM+ ë° DM- ê³„ì‚°
            high_diff = df['high'].diff()
            low_diff = -df['low'].diff()

            dm_plus = pd.Series(np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0), index=df.index)
            dm_minus = pd.Series(np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0), index=df.index)

            # ATR ê³„ì‚°
            atr = self._calculate_atr(df, period)

            # ATRì´ 0ì´ê±°ë‚˜ NaNì¸ ê²½ìš° ì²˜ë¦¬
            atr_safe = atr.replace(0, np.nan)  # 0ì„ NaNìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë‚˜ëˆ„ê¸° ì˜¤ë¥˜ ë°©ì§€

            # DI+ ë° DI- ê³„ì‚°
            dm_plus_smooth = dm_plus.rolling(window=period).mean()
            dm_minus_smooth = dm_minus.rolling(window=period).mean()

            di_plus = 100 * (dm_plus_smooth / atr_safe)
            di_minus = 100 * (dm_minus_smooth / atr_safe)

            # DX ê³„ì‚° (0 ë‚˜ëˆ„ê¸° ë°©ì§€)
            di_sum = di_plus + di_minus
            di_diff = np.abs(di_plus - di_minus)

            # di_sumì´ 0ì´ê±°ë‚˜ ë§¤ìš° ì‘ì€ ê°’ì¸ ê²½ìš° ì²˜ë¦¬
            dx = pd.Series(index=df.index, dtype='float64')
            for i in range(len(df)):
                if pd.isna(di_sum.iloc[i]) or pd.isna(di_diff.iloc[i]) or di_sum.iloc[i] == 0:
                    dx.iloc[i] = np.nan
                else:
                    dx.iloc[i] = 100 * (di_diff.iloc[i] / di_sum.iloc[i])

            # ADX ê³„ì‚° (DXì˜ ì§€ìˆ˜ì´ë™í‰ê· )
            # ì²« ë²ˆì§¸ ìœ íš¨í•œ ADX ê°’ ì°¾ê¸°
            first_valid_idx = dx.first_valid_index()
            if first_valid_idx is None:
                return pd.Series([None] * len(df), index=df.index)

            adx = pd.Series(index=df.index, dtype='float64')

            # ì²« ë²ˆì§¸ ADX ê°’ì€ DX ê°’ë“¤ì˜ ë‹¨ìˆœ í‰ê· 
            start_idx = df.index.get_loc(first_valid_idx)
            if start_idx + period <= len(df):
                first_adx_values = dx.iloc[start_idx:start_idx + period].dropna()
                if len(first_adx_values) > 0:
                    adx.iloc[start_idx + period - 1] = first_adx_values.mean()

                    # ì´í›„ ê°’ë“¤ì€ ì§€ìˆ˜ì´ë™í‰ê· ìœ¼ë¡œ ê³„ì‚°
                    alpha = 1.0 / period
                    for i in range(start_idx + period, len(df)):
                        if not pd.isna(dx.iloc[i]) and not pd.isna(adx.iloc[i-1]):
                            adx.iloc[i] = alpha * dx.iloc[i] + (1 - alpha) * adx.iloc[i-1]

            return adx

        except Exception as e:
            logger.warning(f"ADX ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            logger.debug(f"ADX ê³„ì‚° ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            return pd.Series([None] * len(df), index=df.index)

    def _calculate_support_level(self, df: pd.DataFrame, period: int = 20) -> pd.Series:
        """ì§€ì§€ì„  ê³„ì‚° (ìµœê·¼ period ê¸°ê°„ì˜ ìµœì €ì  ê¸°ë°˜)"""
        try:
            # ìµœê·¼ period ê¸°ê°„ì˜ ìµœì €ì ì„ ì§€ì§€ì„ ìœ¼ë¡œ ì„¤ì •
            support = df['low'].rolling(window=period).min()

            # ë” ì •êµí•œ ì§€ì§€ì„ : ìµœê·¼ ì €ì ë“¤ì˜ í‰ê· 
            low_percentile = df['low'].rolling(window=period).quantile(0.1)  # í•˜ìœ„ 10%

            return low_percentile
        except Exception as e:
            logger.warning(f"ì§€ì§€ì„  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.Series([None] * len(df), index=df.index)

    def save_ohlcv_data(self, ticker: str, df: pd.DataFrame) -> bool:
        """OHLCV ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            saved_count = 0

            for date, row in df.iterrows():
                # UPSERT ì‘ì—… (INSERT OR REPLACE)
                cursor.execute("""
                    INSERT OR REPLACE INTO ohlcv_data (
                        ticker, date, open, high, low, close, volume,
                        ma5, ma20, ma60, ma120, ma200, rsi, volume_ratio,
                        atr, supertrend, macd_histogram, adx, support_level,
                        updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
                """, (
                    ticker,
                    date.strftime('%Y-%m-%d'),
                    float(row['open']) if pd.notna(row['open']) else None,
                    float(row['high']) if pd.notna(row['high']) else None,
                    float(row['low']) if pd.notna(row['low']) else None,
                    float(row['close']) if pd.notna(row['close']) else None,
                    float(row['volume']) if pd.notna(row['volume']) else None,
                    float(row['ma5']) if pd.notna(row['ma5']) else None,
                    float(row['ma20']) if pd.notna(row['ma20']) else None,
                    float(row['ma60']) if pd.notna(row['ma60']) else None,
                    float(row['ma120']) if pd.notna(row['ma120']) else None,
                    float(row['ma200']) if pd.notna(row['ma200']) else None,
                    float(row['rsi']) if pd.notna(row['rsi']) else None,
                    float(row['volume_ratio']) if pd.notna(row['volume_ratio']) else None,
                    float(row['atr']) if pd.notna(row['atr']) else None,
                    float(row['supertrend']) if pd.notna(row['supertrend']) else None,
                    float(row['macd_histogram']) if pd.notna(row['macd_histogram']) else None,
                    float(row['adx']) if pd.notna(row['adx']) else None,
                    float(row['support_level']) if pd.notna(row['support_level']) else None
                ))
                saved_count += 1

            conn.commit()
            conn.close()

            logger.info(f"âœ… {ticker} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ë ˆì½”ë“œ")
            return True

        except Exception as e:
            logger.error(f"âŒ {ticker} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def collect_ticker_data(self, ticker: str) -> Dict[str, Any]:
        """ê°œë³„ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ”„ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

            # 1. ê°­ ë¶„ì„
            gap_info = self.analyze_gap(ticker)
            strategy = gap_info['strategy']

            logger.info(f"ğŸ“Š {ticker} ì „ëµ: {strategy} ({gap_info['reason']})")

            # 2. ì „ëµë³„ ë°ì´í„° ìˆ˜ì§‘
            if strategy == 'skip':
                return {
                    'ticker': ticker,
                    'strategy': strategy,
                    'status': 'skipped',
                    'records': 0,
                    'message': 'Data is up to date'
                }

            elif strategy in ['yesterday_update', 'incremental', 'full_collection']:
                # ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê²°ì •
                if strategy == 'yesterday_update':
                    count = 5  # ìµœê·¼ 5ì¼ì¹˜ë¡œ yesterday ì—…ë°ì´íŠ¸
                elif strategy == 'full_collection':
                    count = 200  # ì „ì²´ ìˆ˜ì§‘
                else:
                    count = min(gap_info['gap_days'] + 10, 200)  # ê°­ + ì—¬ìœ ë¶„

                # API í˜¸ì¶œ
                df = self.safe_get_ohlcv(ticker, count)
                if df is None or df.empty:
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'failed',
                        'records': 0,
                        'message': 'API call failed'
                    }

                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                df_with_indicators = self.calculate_technical_indicators(df, ticker)

                # ë°ì´í„° ì €ì¥
                if self.save_ohlcv_data(ticker, df_with_indicators):
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'success',
                        'records': len(df_with_indicators),
                        'message': f'{len(df_with_indicators)} records processed'
                    }
                else:
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'save_failed',
                        'records': 0,
                        'message': 'Database save failed'
                    }

            else:
                return {
                    'ticker': ticker,
                    'strategy': strategy,
                    'status': 'unknown_strategy',
                    'records': 0,
                    'message': f'Unknown strategy: {strategy}'
                }

        except Exception as e:
            logger.error(f"âŒ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'ticker': ticker,
                'strategy': 'unknown',
                'status': 'error',
                'records': 0,
                'message': str(e)
            }

    def collect_all_data(self, test_mode: bool = False, use_quality_filter: bool = True) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰

        Args:
            test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì œí•œëœ ì¢…ëª©ë§Œ ì²˜ë¦¬)
            use_quality_filter: ê³ í’ˆì§ˆ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
        """
        start_time = time.time()
        logger.info("ğŸš€ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        # í‹°ì»¤ ì„ ë³„ ì „ëµ ê²°ì •
        if use_quality_filter:
            logger.info("ğŸ¯ ê³ í’ˆì§ˆ í‹°ì»¤ í•„í„°ë§ ëª¨ë“œ")
            active_tickers = self.get_qualified_tickers()
            logger.info(f"ğŸ“Š ê³ í’ˆì§ˆ í•„í„°ë§ ê²°ê³¼: {len(active_tickers)}ê°œ ì¢…ëª© ì„ ë³„")
        else:
            logger.info("ğŸ“‚ ê¸°ë³¸ í™œì„± í‹°ì»¤ ëª¨ë“œ")
            active_tickers = self.get_active_tickers()

        if test_mode:
            active_tickers = active_tickers[:5]  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œë§Œ
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œ í‹°ì»¤ë§Œ ì²˜ë¦¬")

        collection_stats = {
            'start_time': datetime.now().isoformat(),
            'total_tickers': len(active_tickers),
            'quality_filter_enabled': use_quality_filter,
            'test_mode': test_mode,
            'results': [],
            'summary': {
                'success': 0,
                'failed': 0,
                'skipped': 0,
                'total_records': 0
            }
        }

        # ê°œë³„ í‹°ì»¤ ì²˜ë¦¬
        for ticker in active_tickers:
            result = self.collect_ticker_data(ticker)
            collection_stats['results'].append(result)

            # í†µê³„ ì—…ë°ì´íŠ¸
            status = result['status']
            if status == 'success':
                collection_stats['summary']['success'] += 1
                collection_stats['summary']['total_records'] += result['records']
            elif status == 'skipped':
                collection_stats['summary']['skipped'] += 1
            else:
                collection_stats['summary']['failed'] += 1

            # ë ˆì´íŠ¸ ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.1)

        # ì™„ë£Œ í†µê³„
        total_time = time.time() - start_time
        collection_stats['end_time'] = datetime.now().isoformat()
        collection_stats['processing_time_seconds'] = round(total_time, 2)

        logger.info(f"âœ… ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ“Š ê²°ê³¼: ì„±ê³µ {collection_stats['summary']['success']}ê°œ, "
                   f"ì‹¤íŒ¨ {collection_stats['summary']['failed']}ê°œ, "
                   f"ìŠ¤í‚µ {collection_stats['summary']['skipped']}ê°œ")
        logger.info(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ: {collection_stats['summary']['total_records']}ê°œ")

        return collection_stats

    def manage_old_data(self, retention_days: int = 300) -> Dict[str, Any]:
        """300ì¼ ì´ìƒ ëœ ë°ì´í„° ê´€ë¦¬ ë° ìµœì í™”

        Args:
            retention_days: ë°ì´í„° ë³´ì¡´ ê¸°ê°„ (ê¸°ë³¸ê°’: 300ì¼)

        Returns:
            ì •ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ§¹ {retention_days}ì¼ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„° ê´€ë¦¬ ì‹œì‘")

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ë³´ì¡´ ê¸°ê°„ ì´ì „ ë‚ ì§œ ê³„ì‚°
            cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d')

            # ì •ë¦¬ ëŒ€ìƒ ë°ì´í„° í™•ì¸
            cursor.execute("""
                SELECT ticker, COUNT(*) as old_records,
                       MIN(date) as oldest_date,
                       MAX(date) as newest_old_date
                FROM ohlcv_data
                WHERE date < ?
                GROUP BY ticker
                ORDER BY old_records DESC
            """, (cutoff_date,))

            cleanup_candidates = cursor.fetchall()

            if not cleanup_candidates:
                conn.close()
                logger.info(f"âœ… {retention_days}ì¼ ì´ìƒ ëœ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    'retention_days': retention_days,
                    'cutoff_date': cutoff_date,
                    'deleted_records': 0,
                    'affected_tickers': 0,
                    'storage_saved_mb': 0.0,
                    'status': 'no_old_data'
                }

            total_old_records = sum(record[1] for record in cleanup_candidates)

            logger.info(f"ğŸ“Š ì •ë¦¬ ëŒ€ìƒ ë°œê²¬:")
            logger.info(f"   â€¢ ê¸°ì¤€ì¼: {cutoff_date} ì´ì „")
            logger.info(f"   â€¢ ì´ {len(cleanup_candidates)}ê°œ ì¢…ëª©, {total_old_records}ê°œ ë ˆì½”ë“œ")

            # ê° ì¢…ëª©ë³„ ì •ë¦¬ ëŒ€ìƒ ìƒì„¸ í‘œì‹œ
            for ticker, old_records, oldest_date, newest_old_date in cleanup_candidates[:5]:
                logger.info(f"   â€¢ {ticker}: {old_records}ê°œ ë ˆì½”ë“œ ({oldest_date} ~ {newest_old_date})")

            if len(cleanup_candidates) > 5:
                logger.info(f"   â€¢ ... ì™¸ {len(cleanup_candidates)-5}ê°œ ì¢…ëª©")

            # ì‚¬ìš©ì í™•ì¸ ì—†ì´ ìë™ ì •ë¦¬ (300ì¼ ì´ìƒì€ ì¶©ë¶„íˆ ì•ˆì „í•œ ê¸°ê°„)
            logger.info(f"ğŸ—‘ï¸ {retention_days}ì¼ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì •ë¦¬ ì‹œì‘...")

            # ë°ì´í„° ì‚­ì œ ì‹¤í–‰
            cursor.execute("""
                DELETE FROM ohlcv_data
                WHERE date < ?
            """, (cutoff_date,))

            deleted_records = cursor.rowcount
            conn.commit()

            # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM)
            logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM) ì‹¤í–‰ì¤‘...")
            cursor.execute("VACUUM")

            # ì •ë¦¬ í›„ ì €ì¥ê³µê°„ í™•ì¸
            import os
            if os.path.exists(self.db_path):
                file_size_mb = os.path.getsize(self.db_path) / (1024 * 1024)
            else:
                file_size_mb = 0

            # ì˜ˆìƒ ì €ì¥ê³µê°„ ì ˆì•½ ê³„ì‚° (ë ˆì½”ë“œë‹¹ í‰ê·  512ë°”ì´íŠ¸)
            storage_saved_mb = (deleted_records * 512) / (1024 * 1024)

            conn.close()

            logger.info(f"âœ… ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
            logger.info(f"   â€¢ ì‚­ì œëœ ë ˆì½”ë“œ: {deleted_records:,}ê°œ")
            logger.info(f"   â€¢ ì˜í–¥ë°›ì€ ì¢…ëª©: {len(cleanup_candidates)}ê°œ")
            logger.info(f"   â€¢ ì˜ˆìƒ ì ˆì•½ ê³µê°„: {storage_saved_mb:.2f}MB")
            logger.info(f"   â€¢ í˜„ì¬ DB í¬ê¸°: {file_size_mb:.2f}MB")

            return {
                'retention_days': retention_days,
                'cutoff_date': cutoff_date,
                'deleted_records': deleted_records,
                'affected_tickers': len(cleanup_candidates),
                'storage_saved_mb': round(storage_saved_mb, 2),
                'current_db_size_mb': round(file_size_mb, 2),
                'status': 'cleanup_completed'
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'retention_days': retention_days,
                'cutoff_date': cutoff_date,
                'deleted_records': 0,
                'affected_tickers': 0,
                'storage_saved_mb': 0.0,
                'status': f'error: {e}'
            }

    def check_data_retention_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„° ë³´ì¡´ ìƒíƒœ í™•ì¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ì „ì²´ ë°ì´í„° í˜„í™© ì¡°íšŒ
            cursor.execute("""
                SELECT
                    ticker,
                    COUNT(*) as record_count,
                    MIN(date) as oldest_date,
                    MAX(date) as newest_date,
                    CAST(julianday('now') - julianday(MIN(date)) AS INTEGER) as days_span
                FROM ohlcv_data
                GROUP BY ticker
                ORDER BY days_span DESC
            """)

            results = cursor.fetchall()
            conn.close()

            if not results:
                return {
                    'total_tickers': 0,
                    'max_days': 0,
                    'avg_days': 0,
                    'over_300_days_count': 0,
                    'storage_optimization_needed': False,
                    'status': 'no_data'
                }

            # í†µê³„ ê³„ì‚°
            total_days = sum(row[4] for row in results)
            max_days = max(row[4] for row in results)
            avg_days = total_days / len(results)
            over_300_days_count = sum(1 for row in results if row[4] >= 300)

            # ìŠ¤í† ë¦¬ì§€ ìµœì í™” í•„ìš”ì„± íŒë‹¨
            storage_optimization_needed = (max_days > 300) or (over_300_days_count > 0)

            logger.info(f"ğŸ“Š ë°ì´í„° ë³´ì¡´ ìƒíƒœ:")
            logger.info(f"   â€¢ ì „ì²´ ì¢…ëª©: {len(results)}ê°œ")
            logger.info(f"   â€¢ í‰ê·  ë³´ì¡´ ê¸°ê°„: {avg_days:.1f}ì¼")
            logger.info(f"   â€¢ ìµœëŒ€ ë³´ì¡´ ê¸°ê°„: {max_days}ì¼")
            logger.info(f"   â€¢ 300ì¼+ ë°ì´í„°: {over_300_days_count}ê°œ ì¢…ëª©")
            logger.info(f"   â€¢ ìŠ¤í† ë¦¬ì§€ ìµœì í™” í•„ìš”: {'ì˜ˆ' if storage_optimization_needed else 'ì•„ë‹ˆì˜¤'}")

            return {
                'total_tickers': len(results),
                'max_days': max_days,
                'avg_days': round(avg_days, 1),
                'over_300_days_count': over_300_days_count,
                'storage_optimization_needed': storage_optimization_needed,
                'ticker_details': [(row[0], row[1], row[4]) for row in results[:5]],  # ìƒìœ„ 5ê°œ
                'status': 'analysis_complete'
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë³´ì¡´ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                'total_tickers': 0,
                'max_days': 0,
                'avg_days': 0,
                'over_300_days_count': 0,
                'storage_optimization_needed': False,
                'status': f'error: {e}'
            }

    def auto_manage_data_retention(self, retention_days: int = 300) -> Dict[str, Any]:
        """ìë™ ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰

        300ì¼ ì´ìƒ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” í†µí•© ë©”ì„œë“œ
        """
        logger.info("ğŸ”„ ìë™ ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰")

        # 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸
        status = self.check_data_retention_status()

        result = {
            'retention_days': retention_days,
            'initial_status': status,
            'cleanup_performed': False,
            'cleanup_result': None
        }

        # 2ë‹¨ê³„: ì •ë¦¬ í•„ìš”ì„± íŒë‹¨ ë° ì‹¤í–‰
        if status['storage_optimization_needed']:
            logger.info(f"âš ï¸ {retention_days}ì¼ ì´ìƒ ë°ì´í„° ê°ì§€, ìë™ ì •ë¦¬ ì‹¤í–‰")
            cleanup_result = self.manage_old_data(retention_days)
            result['cleanup_performed'] = True
            result['cleanup_result'] = cleanup_result
        else:
            logger.info(f"âœ… ëª¨ë“  ë°ì´í„°ê°€ {retention_days}ì¼ ì´ë‚´, ì •ë¦¬ ë¶ˆí•„ìš”")

        return result


def verify_database_data():
    """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²°ê³¼ ê²€ì¦"""
    try:
        db_path = "./makenaide_local.db"
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # ì „ì²´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM ohlcv_data")
        total_records = cursor.fetchone()[0]

        # í‹°ì»¤ë³„ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        cursor.execute("""
            SELECT ticker, COUNT(*) as count,
                   MIN(date) as first_date, MAX(date) as last_date
            FROM ohlcv_data
            GROUP BY ticker
            ORDER BY ticker
        """)
        ticker_stats = cursor.fetchall()

        # ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° í™•ì¸
        cursor.execute("""
            SELECT ticker, date, close, ma20, rsi
            FROM ohlcv_data
            WHERE ma20 IS NOT NULL AND rsi IS NOT NULL
            LIMIT 5
        """)
        sample_indicators = cursor.fetchall()

        conn.close()

        print("\n" + "="*60)
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")

        print(f"\nğŸ“‹ í‹°ì»¤ë³„ í†µê³„:")
        for ticker, count, first_date, last_date in ticker_stats:
            print(f"   - {ticker}: {count}ê°œ ({first_date} ~ {last_date})")

        print(f"\nğŸ§® ê¸°ìˆ ì  ì§€í‘œ ìƒ˜í”Œ:")
        for ticker, date, close, ma20, rsi in sample_indicators:
            print(f"   - {ticker} {date}: ì¢…ê°€={close:.2f}, MA20={ma20:.2f}, RSI={rsi:.1f}")

        return total_records > 0

    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Makenaide Enhanced Data Collector (Quality-Filtered)")
    print("=" * 60)
    print("ğŸ“‹ í’ˆì§ˆ í•„í„°ë§ ì¡°ê±´:")
    print("   â€¢ 13ê°œì›” ì´ìƒ ì›”ë´‰ ë°ì´í„° ë³´ìœ  ì¢…ëª©")
    print("   â€¢ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ 3ì–µì› ì´ìƒ ì¢…ëª©")
    print("=" * 60)

    try:
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}MB")

        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = SimpleDataCollector()

        # 1ë‹¨ê³„: ë°ì´í„° ë³´ì¡´ ì •ì±… ìë™ ì‹¤í–‰ (300ì¼ ì´ìƒ ë°ì´í„° ì •ë¦¬)
        print("\nğŸ”„ 1ë‹¨ê³„: ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰")
        print("-" * 40)
        retention_result = collector.auto_manage_data_retention(retention_days=300)

        if retention_result['cleanup_performed']:
            cleanup = retention_result['cleanup_result']
            print(f"ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ:")
            print(f"- ì‚­ì œëœ ë ˆì½”ë“œ: {cleanup['deleted_records']:,}ê°œ")
            print(f"- ì˜í–¥ë°›ì€ ì¢…ëª©: {cleanup['affected_tickers']}ê°œ")
            print(f"- ì ˆì•½ëœ ê³µê°„: {cleanup['storage_saved_mb']}MB")
        else:
            status = retention_result['initial_status']
            print(f"âœ… ë°ì´í„° ë³´ì¡´ ìƒíƒœ ì–‘í˜¸ (ìµœëŒ€ {status['max_days']}ì¼)")

        # 2ë‹¨ê³„: í’ˆì§ˆ í•„í„°ë§ ëª¨ë“œë¡œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        print(f"\nğŸ“Š 2ë‹¨ê³„: OHLCV ë°ì´í„° ìˆ˜ì§‘")
        print("-" * 40)
        results = collector.collect_all_data(test_mode=True, use_quality_filter=True)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"- í’ˆì§ˆ í•„í„°ë§: {'í™œì„±í™”' if results['quality_filter_enabled'] else 'ë¹„í™œì„±í™”'}")
        print(f"- í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {'í™œì„±í™”' if results['test_mode'] else 'ë¹„í™œì„±í™”'}")
        print(f"- ì´ ì¢…ëª©: {results['total_tickers']}ê°œ")
        print(f"- ì²˜ë¦¬ ì™„ë£Œ: {results['summary']['success']}ê°œ")
        print(f"- ìŠ¤í‚µ: {results['summary']['skipped']}ê°œ")
        print(f"- ì˜¤ë¥˜: {results['summary']['failed']}ê°œ")
        print(f"- ì´ ë ˆì½”ë“œ: {results['summary']['total_records']:,}ê°œ")
        print(f"- ì´ ì‹œê°„: {results['processing_time_seconds']}ì´ˆ")

        # íš¨ìœ¨ì„± ê°œì„  ë©”íŠ¸ë¦­ í‘œì‹œ
        if results['quality_filter_enabled']:
            print(f"\nğŸ’¡ í’ˆì§ˆ í•„í„°ë§ íš¨ê³¼:")
            print(f"- ì˜ˆìƒ API ì ˆì•½ë¥ : ~67% (ê³ í’ˆì§ˆ ì¢…ëª©ë§Œ ì²˜ë¦¬)")
            print(f"- ì˜ˆìƒ ì €ì¥ì†Œ ì ˆì•½ë¥ : ~67% (ë…¸ì´ì¦ˆ ë°ì´í„° ì œê±°)")
            print(f"- ë¶„ì„ í’ˆì§ˆ í–¥ìƒ: ê±°ë˜ëŸ‰/ì•ˆì •ì„± í™•ë³´ëœ ì¢…ëª©ë§Œ ì„ ë³„")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"ğŸ“Š ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.1f}MB (ì¦ê°€: {final_memory-initial_memory:.1f}MB)")

        # ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦
        if verify_database_data():
            print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            return True
        else:
            print("âŒ ë°ì´í„° ìˆ˜ì§‘ ê²€ì¦ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)