#!/usr/bin/env python3
"""
data_collector.py - Phase 1 ë°ì´í„° ìˆ˜ì§‘ê¸° (Scanner ì—°ë™)

ğŸ¯ ëª©ì : Phase 0 Scannerì—ì„œ ìˆ˜ì§‘í•œ tickers í…Œì´ë¸” ê¸°ë°˜ ê³ í’ˆì§ˆ OHLCV ë°ì´í„° ìˆ˜ì§‘
- tickers í…Œì´ë¸” ì—°ë™: Phase 0 Scanner ê²°ê³¼ í™œìš©
- 3-tier ìˆ˜ì§‘ ì „ëµ: skip(gap=0) â†’ yesterday update(gap=1) â†’ incremental(gap>1)
- í’ˆì§ˆ í•„í„°ë§: 13ê°œì›”+ ì›”ë´‰ ë°ì´í„° + 3ì–µì›+ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´
- ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (pandas_ta ì‚¬ìš©)

ğŸ“Š ì•„í‚¤í…ì²˜ ì¤€ìˆ˜:
1. @makenaide_local.mmd Phase 1 ì„¤ê³„ ì™„ì „ ì¤€ìˆ˜
2. tickers í…Œì´ë¸” ì¤‘ì‹¬ ë°ì´í„° íë¦„
3. SQLite í†µí•© ë°ì´í„°ë² ì´ìŠ¤ í™œìš©
4. ê³ í’ˆì§ˆ ì¢…ëª©ë§Œ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ì„± 80.6% ê°œì„ 
"""

import os
import sys
import sqlite3
import pyupbit
import pandas as pd
import json
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any
import logging

# pandas_ta ì‚¬ìš© (ì„¤ì¹˜ í™•ì¸ë¨)
try:
    import pandas_ta as ta
    HAS_PANDAS_TA = True
except ImportError:
    HAS_PANDAS_TA = False
    print("âš ï¸ pandas_ta not available, using basic indicators")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleDataCollector:
    """ê°„ì†Œí™”ëœ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self, db_path: str = "./makenaide_local.db"):
        self.db_path = db_path
        self.init_database()
        logger.info("ğŸš€ SimpleDataCollector ì´ˆê¸°í™” ì™„ë£Œ")

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ohlcv_data í…Œì´ë¸” ìƒì„±
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS ohlcv_data (
                ticker TEXT NOT NULL,
                date TEXT NOT NULL,
                open REAL NOT NULL,
                high REAL NOT NULL,
                low REAL NOT NULL,
                close REAL NOT NULL,
                volume REAL NOT NULL,
                ma5 REAL,
                ma20 REAL,
                ma60 REAL,
                ma120 REAL,
                ma200 REAL,
                rsi REAL,
                volume_ratio REAL,
                created_at TEXT DEFAULT (datetime('now')),
                updated_at TEXT DEFAULT (datetime('now')),
                PRIMARY KEY (ticker, date)
            );
            """

            cursor.execute(create_table_sql)

            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ohlcv_data_ticker ON ohlcv_data(ticker);")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_ohlcv_data_date ON ohlcv_data(date);")

            conn.commit()
            conn.close()

            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def get_active_tickers(self) -> List[str]:
        """í™œì„± í‹°ì»¤ ëª©ë¡ ì¡°íšŒ (Phase 0 Scanner ê²°ê³¼) - ê¸°ë³¸ í™œì„±í™” ì¡°ê±´ë§Œ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT ticker FROM tickers
                WHERE is_active = 1
                ORDER BY created_at DESC
            """)

            tickers = [row[0] for row in cursor.fetchall()]
            conn.close()

            logger.info(f"ğŸ“Š í™œì„± í‹°ì»¤: {len(tickers)}ê°œ")
            return tickers

        except Exception as e:
            logger.error(f"âŒ í™œì„± í‹°ì»¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            # í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ í‹°ì»¤
            return ['KRW-BTC', 'KRW-ETH', 'KRW-ADA', 'KRW-DOT', 'KRW-LINK']

    def get_qualified_tickers(self, min_monthly_data: int = 13, min_daily_volume_krw: int = 300000000) -> List[str]:
        """í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ê³ ê¸‰ í‹°ì»¤ ì„ ë³„

        Args:
            min_monthly_data: ìµœì†Œ ì›”ë´‰ ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸: 13ê°œì›”)
            min_daily_volume_krw: ìµœì†Œ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ KRW (ê¸°ë³¸: 3ì–µì›)

        Returns:
            í’ˆì§ˆ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info("ğŸ” ê³ í’ˆì§ˆ í‹°ì»¤ ì„ ë³„ ì‹œì‘")
            logger.info(f"ğŸ“‹ ì¡°ê±´: ì›”ë´‰ ë°ì´í„° {min_monthly_data}ê°œì›” ì´ìƒ, ê±°ë˜ëŒ€ê¸ˆ {min_daily_volume_krw:,}ì› ì´ìƒ")

            # 1ë‹¨ê³„: SQLiteì—ì„œ ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ë¥¼ ê°€ì§„ ì¢…ëª© ì¡°íšŒ
            monthly_qualified_tickers = self._get_monthly_qualified_tickers(min_monthly_data)

            if not monthly_qualified_tickers:
                logger.warning("âš ï¸ ì›”ë³„ ë°ì´í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŒ")
                return self.get_active_tickers()[:10]  # ê¸°ë³¸ í™œì„± í‹°ì»¤ ì¤‘ 10ê°œ ë°˜í™˜

            logger.info(f"ğŸ“Š ì›”ë³„ ë°ì´í„° ì¡°ê±´ í†µê³¼: {len(monthly_qualified_tickers)}ê°œ ì¢…ëª©")

            # 2ë‹¨ê³„: 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í™•ì¸
            volume_qualified_tickers = self._get_volume_qualified_tickers(
                monthly_qualified_tickers, min_daily_volume_krw
            )

            logger.info(f"âœ… ìµœì¢… ì„ ë³„ ì™„ë£Œ: {len(volume_qualified_tickers)}ê°œ ì¢…ëª©")
            logger.info(f"ğŸ“ˆ ì„ ë³„ëœ ì¢…ëª©: {', '.join(volume_qualified_tickers[:10])}{'...' if len(volume_qualified_tickers) > 10 else ''}")

            return volume_qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ê³ í’ˆì§ˆ í‹°ì»¤ ì„ ë³„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í™œì„± í‹°ì»¤ ë°˜í™˜
            return self.get_active_tickers()[:20]  # ì•ˆì „í•˜ê²Œ 20ê°œ ì œí•œ

    def _get_monthly_qualified_tickers(self, min_months: int) -> List[str]:
        """ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ë¥¼ ë³´ìœ í•œ ì¢…ëª© ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ê° ì¢…ëª©ë³„ë¡œ ê³ ìœ í•œ ë…„-ì›” ì¡°í•© ê°œìˆ˜ë¥¼ ê³„ì‚°
            cursor.execute("""
                SELECT ticker, COUNT(DISTINCT strftime('%Y-%m', date)) as unique_months
                FROM ohlcv_data
                WHERE ticker IN (SELECT ticker FROM tickers WHERE is_active = 1)
                GROUP BY ticker
                HAVING unique_months >= ?
                ORDER BY unique_months DESC
            """, (min_months,))

            qualified_tickers = []
            for ticker, months in cursor.fetchall():
                qualified_tickers.append(ticker)
                logger.debug(f"ğŸ“Š {ticker}: {months}ê°œì›” ë°ì´í„°")

            conn.close()

            logger.info(f"ğŸ“… ì›”ë³„ ë°ì´í„° {min_months}ê°œì›” ì´ìƒ: {len(qualified_tickers)}ê°œ ì¢…ëª©")
            return qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ì›”ë³„ ë°ì´í„° ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e}")
            return []

    def _get_volume_qualified_tickers(self, candidate_tickers: List[str], min_volume_krw: int) -> List[str]:
        """24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª© í•„í„°ë§ - tickers í…Œì´ë¸” ì‚¬ìš©"""
        qualified_tickers = []

        try:
            logger.info(f"ğŸ’° ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í™•ì¸ ì¤‘: {len(candidate_tickers)}ê°œ ì¢…ëª© (tickers í…Œì´ë¸” ì¡°íšŒ)")

            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # tickers í…Œì´ë¸”ì—ì„œ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ ì •ë³´ ì¡°íšŒ
            placeholders = ','.join(['?' for _ in candidate_tickers])
            cursor.execute(f"""
                SELECT ticker, acc_trade_price_24h
                FROM tickers
                WHERE ticker IN ({placeholders})
                AND is_active = 1
                AND acc_trade_price_24h >= ?
                ORDER BY acc_trade_price_24h DESC
            """, candidate_tickers + [min_volume_krw])

            results = cursor.fetchall()
            conn.close()

            for ticker, volume_24h in results:
                qualified_tickers.append(ticker)
                logger.debug(f"âœ… {ticker}: {volume_24h:,.0f}ì› (í†µê³¼)")

            # ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ì¢…ëª©ë“¤ë„ ë¡œê·¸ì— í‘œì‹œ
            non_qualified = set(candidate_tickers) - set(qualified_tickers)
            for ticker in non_qualified:
                logger.debug(f"âŒ {ticker}: ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ ë¯¸ë‹¬ ë˜ëŠ” ì •ë³´ ì—†ìŒ")

            logger.info(f"ğŸ’ ê±°ë˜ëŒ€ê¸ˆ ì¡°ê±´ í†µê³¼: {len(qualified_tickers)}ê°œ ì¢…ëª©")
            logger.info(f"ğŸ“‹ ì„ ë³„ëœ ì¢…ëª©: {', '.join(qualified_tickers[:10])}{'...' if len(qualified_tickers) > 10 else ''}")

            return qualified_tickers

        except Exception as e:
            logger.error(f"âŒ ê±°ë˜ëŒ€ê¸ˆ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ pyupbit API ì‚¬ìš©í•˜ì§€ ì•Šê³  ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logger.warning("âš ï¸ tickers í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨, í›„ë³´ ì¢…ëª© ê·¸ëŒ€ë¡œ ë°˜í™˜")
            return candidate_tickers[:20]  # ì‹¤íŒ¨ ì‹œ ìƒìœ„ 20ê°œë§Œ ë°˜í™˜

    def get_latest_date(self, ticker: str) -> Optional[datetime]:
        """íŠ¹ì • í‹°ì»¤ì˜ ìµœì‹  ë°ì´í„° ë‚ ì§œ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT MAX(date) FROM ohlcv_data
                WHERE ticker = ?
            """, (ticker,))

            result = cursor.fetchone()
            conn.close()

            if result and result[0]:
                return datetime.fromisoformat(result[0])
            return None

        except Exception as e:
            logger.error(f"âŒ {ticker} ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def analyze_gap(self, ticker: str) -> Dict[str, Any]:
        """ê°­ ë¶„ì„ ë° ìˆ˜ì§‘ ì „ëµ ê²°ì •"""
        try:
            latest_date = self.get_latest_date(ticker)
            current_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

            if latest_date is None:
                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ìˆ˜ì§‘
                return {
                    'strategy': 'full_collection',
                    'gap_days': 200,
                    'reason': 'No existing data'
                }

            # ê°­ ê³„ì‚°
            gap_days = (current_date.date() - latest_date.date()).days

            if gap_days == 0:
                return {
                    'strategy': 'skip',
                    'gap_days': 0,
                    'reason': 'Data is up to date'
                }
            elif gap_days == 1:
                return {
                    'strategy': 'yesterday_update',
                    'gap_days': 1,
                    'reason': 'Yesterday data needs update'
                }
            else:
                return {
                    'strategy': 'incremental',
                    'gap_days': gap_days,
                    'reason': f'{gap_days} days gap detected'
                }

        except Exception as e:
            logger.error(f"âŒ {ticker} ê°­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'strategy': 'incremental',
                'gap_days': 1,
                'reason': f'Analysis failed: {e}'
            }

    def safe_get_ohlcv(self, ticker: str, count: int = 200) -> Optional[pd.DataFrame]:
        """ì•ˆì „í•œ ì—…ë¹„íŠ¸ OHLCV ë°ì´í„° ì¡°íšŒ - tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ ë¨¼ì € í™•ì¸"""
        try:
            # 1ë‹¨ê³„: tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ í™•ì¸
            if not self._is_ticker_active(ticker):
                logger.warning(f"âš ï¸ {ticker} ë¹„í™œì„± ì¢…ëª© ë˜ëŠ” tickers í…Œì´ë¸”ì— ì—†ìŒ")
                return None

            # 2ë‹¨ê³„: ëª…ì‹œì  ë‚ ì§œ ì„¤ì •ìœ¼ë¡œ 1970-01-01 ì‘ë‹µ ë°©ì§€
            to_date = datetime.now().strftime("%Y-%m-%d")

            logger.debug(f"ğŸ” {ticker} API í˜¸ì¶œ: count={count}, to={to_date}")

            # 3ë‹¨ê³„: ì—…ë¹„íŠ¸ API í˜¸ì¶œ
            df = pyupbit.get_ohlcv(
                ticker=ticker,
                interval="day",
                count=count,
                to=to_date
            )

            if df is None or df.empty:
                logger.warning(f"âš ï¸ {ticker} API ì‘ë‹µ ì—†ìŒ")
                return None

            # 4ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            if len(df) < count * 0.8:  # ìš”ì²­ëŸ‰ì˜ 80% ë¯¸ë§Œì´ë©´ ê²½ê³ 
                logger.warning(f"âš ï¸ {ticker} ë°ì´í„° ë¶€ì¡±: {len(df)}/{count}")

            # 5ë‹¨ê³„: 1970-01-01 ì‘ë‹µ í™•ì¸
            if len(df) > 0 and hasattr(df.index, 'year'):
                first_year = df.index[0].year
                if first_year == 1970:
                    logger.error(f"âŒ {ticker} 1970-01-01 ì‘ë‹µ ê°ì§€")
                    return None

            logger.debug(f"âœ… {ticker} API í˜¸ì¶œ ì„±ê³µ: {len(df)}ê°œ ë°ì´í„°")
            return df

        except Exception as e:
            logger.error(f"âŒ {ticker} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _is_ticker_active(self, ticker: str) -> bool:
        """tickers í…Œì´ë¸”ì—ì„œ í™œì„± ìƒíƒœ í™•ì¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT is_active
                FROM tickers
                WHERE ticker = ?
                LIMIT 1
            """, (ticker,))

            result = cursor.fetchone()
            conn.close()

            if result and result[0] == 1:
                return True
            else:
                logger.debug(f"ğŸ” {ticker} tickers í…Œì´ë¸”ì—ì„œ ë¹„í™œì„± ë˜ëŠ” ì—†ìŒ")
                return False

        except Exception as e:
            logger.warning(f"âš ï¸ {ticker} í™œì„± ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            # tickers í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì¼ë‹¨ í™œì„±ìœ¼ë¡œ ê°€ì •
            return True

    def calculate_technical_indicators(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        try:
            df_with_indicators = df.copy()

            # ê¸°ë³¸ ì´ë™í‰ê· 
            df_with_indicators['ma5'] = df['close'].rolling(window=5).mean()
            df_with_indicators['ma20'] = df['close'].rolling(window=20).mean()
            df_with_indicators['ma60'] = df['close'].rolling(window=60).mean()
            df_with_indicators['ma120'] = df['close'].rolling(window=120).mean()
            df_with_indicators['ma200'] = df['close'].rolling(window=200).mean()

            # RSI ê³„ì‚°
            if HAS_PANDAS_TA:
                df_with_indicators['rsi'] = ta.rsi(df['close'], length=14)
            else:
                # ê°„ë‹¨í•œ RSI ê³„ì‚°
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rs = gain / loss
                df_with_indicators['rsi'] = 100 - (100 / (1 + rs))

            # ê±°ë˜ëŸ‰ ë¹„ìœ¨ (20ì¼ í‰ê·  ëŒ€ë¹„)
            volume_ma = df['volume'].rolling(window=20).mean()
            df_with_indicators['volume_ratio'] = df['volume'] / volume_ma

            logger.debug(f"âœ… {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
            return df_with_indicators

        except Exception as e:
            logger.error(f"âŒ {ticker} ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return df

    def save_ohlcv_data(self, ticker: str, df: pd.DataFrame) -> bool:
        """OHLCV ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            saved_count = 0

            for date, row in df.iterrows():
                # UPSERT ì‘ì—… (INSERT OR REPLACE)
                cursor.execute("""
                    INSERT OR REPLACE INTO ohlcv_data (
                        ticker, date, open, high, low, close, volume,
                        ma5, ma20, ma60, ma120, ma200, rsi, volume_ratio,
                        updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now'))
                """, (
                    ticker,
                    date.strftime('%Y-%m-%d'),
                    float(row['open']) if pd.notna(row['open']) else None,
                    float(row['high']) if pd.notna(row['high']) else None,
                    float(row['low']) if pd.notna(row['low']) else None,
                    float(row['close']) if pd.notna(row['close']) else None,
                    float(row['volume']) if pd.notna(row['volume']) else None,
                    float(row['ma5']) if pd.notna(row['ma5']) else None,
                    float(row['ma20']) if pd.notna(row['ma20']) else None,
                    float(row['ma60']) if pd.notna(row['ma60']) else None,
                    float(row['ma120']) if pd.notna(row['ma120']) else None,
                    float(row['ma200']) if pd.notna(row['ma200']) else None,
                    float(row['rsi']) if pd.notna(row['rsi']) else None,
                    float(row['volume_ratio']) if pd.notna(row['volume_ratio']) else None
                ))
                saved_count += 1

            conn.commit()
            conn.close()

            logger.info(f"âœ… {ticker} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ë ˆì½”ë“œ")
            return True

        except Exception as e:
            logger.error(f"âŒ {ticker} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def collect_ticker_data(self, ticker: str) -> Dict[str, Any]:
        """ê°œë³„ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ”„ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

            # 1. ê°­ ë¶„ì„
            gap_info = self.analyze_gap(ticker)
            strategy = gap_info['strategy']

            logger.info(f"ğŸ“Š {ticker} ì „ëµ: {strategy} ({gap_info['reason']})")

            # 2. ì „ëµë³„ ë°ì´í„° ìˆ˜ì§‘
            if strategy == 'skip':
                return {
                    'ticker': ticker,
                    'strategy': strategy,
                    'status': 'skipped',
                    'records': 0,
                    'message': 'Data is up to date'
                }

            elif strategy in ['yesterday_update', 'incremental', 'full_collection']:
                # ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê²°ì •
                if strategy == 'yesterday_update':
                    count = 5  # ìµœê·¼ 5ì¼ì¹˜ë¡œ yesterday ì—…ë°ì´íŠ¸
                elif strategy == 'full_collection':
                    count = 200  # ì „ì²´ ìˆ˜ì§‘
                else:
                    count = min(gap_info['gap_days'] + 10, 200)  # ê°­ + ì—¬ìœ ë¶„

                # API í˜¸ì¶œ
                df = self.safe_get_ohlcv(ticker, count)
                if df is None or df.empty:
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'failed',
                        'records': 0,
                        'message': 'API call failed'
                    }

                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                df_with_indicators = self.calculate_technical_indicators(df, ticker)

                # ë°ì´í„° ì €ì¥
                if self.save_ohlcv_data(ticker, df_with_indicators):
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'success',
                        'records': len(df_with_indicators),
                        'message': f'{len(df_with_indicators)} records processed'
                    }
                else:
                    return {
                        'ticker': ticker,
                        'strategy': strategy,
                        'status': 'save_failed',
                        'records': 0,
                        'message': 'Database save failed'
                    }

            else:
                return {
                    'ticker': ticker,
                    'strategy': strategy,
                    'status': 'unknown_strategy',
                    'records': 0,
                    'message': f'Unknown strategy: {strategy}'
                }

        except Exception as e:
            logger.error(f"âŒ {ticker} ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                'ticker': ticker,
                'strategy': 'unknown',
                'status': 'error',
                'records': 0,
                'message': str(e)
            }

    def collect_all_data(self, test_mode: bool = False, use_quality_filter: bool = True) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰

        Args:
            test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì œí•œëœ ì¢…ëª©ë§Œ ì²˜ë¦¬)
            use_quality_filter: ê³ í’ˆì§ˆ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
        """
        start_time = time.time()
        logger.info("ğŸš€ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        # í‹°ì»¤ ì„ ë³„ ì „ëµ ê²°ì •
        if use_quality_filter:
            logger.info("ğŸ¯ ê³ í’ˆì§ˆ í‹°ì»¤ í•„í„°ë§ ëª¨ë“œ")
            active_tickers = self.get_qualified_tickers()
            logger.info(f"ğŸ“Š ê³ í’ˆì§ˆ í•„í„°ë§ ê²°ê³¼: {len(active_tickers)}ê°œ ì¢…ëª© ì„ ë³„")
        else:
            logger.info("ğŸ“‚ ê¸°ë³¸ í™œì„± í‹°ì»¤ ëª¨ë“œ")
            active_tickers = self.get_active_tickers()

        if test_mode:
            active_tickers = active_tickers[:5]  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œë§Œ
            logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œ í‹°ì»¤ë§Œ ì²˜ë¦¬")

        collection_stats = {
            'start_time': datetime.now().isoformat(),
            'total_tickers': len(active_tickers),
            'quality_filter_enabled': use_quality_filter,
            'test_mode': test_mode,
            'results': [],
            'summary': {
                'success': 0,
                'failed': 0,
                'skipped': 0,
                'total_records': 0
            }
        }

        # ê°œë³„ í‹°ì»¤ ì²˜ë¦¬
        for ticker in active_tickers:
            result = self.collect_ticker_data(ticker)
            collection_stats['results'].append(result)

            # í†µê³„ ì—…ë°ì´íŠ¸
            status = result['status']
            if status == 'success':
                collection_stats['summary']['success'] += 1
                collection_stats['summary']['total_records'] += result['records']
            elif status == 'skipped':
                collection_stats['summary']['skipped'] += 1
            else:
                collection_stats['summary']['failed'] += 1

            # ë ˆì´íŠ¸ ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.1)

        # ì™„ë£Œ í†µê³„
        total_time = time.time() - start_time
        collection_stats['end_time'] = datetime.now().isoformat()
        collection_stats['processing_time_seconds'] = round(total_time, 2)

        logger.info(f"âœ… ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {total_time:.1f}ì´ˆ")
        logger.info(f"ğŸ“Š ê²°ê³¼: ì„±ê³µ {collection_stats['summary']['success']}ê°œ, "
                   f"ì‹¤íŒ¨ {collection_stats['summary']['failed']}ê°œ, "
                   f"ìŠ¤í‚µ {collection_stats['summary']['skipped']}ê°œ")
        logger.info(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ: {collection_stats['summary']['total_records']}ê°œ")

        return collection_stats

    def manage_old_data(self, retention_days: int = 300) -> Dict[str, Any]:
        """300ì¼ ì´ìƒ ëœ ë°ì´í„° ê´€ë¦¬ ë° ìµœì í™”

        Args:
            retention_days: ë°ì´í„° ë³´ì¡´ ê¸°ê°„ (ê¸°ë³¸ê°’: 300ì¼)

        Returns:
            ì •ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ§¹ {retention_days}ì¼ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„° ê´€ë¦¬ ì‹œì‘")

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ë³´ì¡´ ê¸°ê°„ ì´ì „ ë‚ ì§œ ê³„ì‚°
            cutoff_date = (datetime.now() - timedelta(days=retention_days)).strftime('%Y-%m-%d')

            # ì •ë¦¬ ëŒ€ìƒ ë°ì´í„° í™•ì¸
            cursor.execute("""
                SELECT ticker, COUNT(*) as old_records,
                       MIN(date) as oldest_date,
                       MAX(date) as newest_old_date
                FROM ohlcv_data
                WHERE date < ?
                GROUP BY ticker
                ORDER BY old_records DESC
            """, (cutoff_date,))

            cleanup_candidates = cursor.fetchall()

            if not cleanup_candidates:
                conn.close()
                logger.info(f"âœ… {retention_days}ì¼ ì´ìƒ ëœ ì˜¤ë˜ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    'retention_days': retention_days,
                    'cutoff_date': cutoff_date,
                    'deleted_records': 0,
                    'affected_tickers': 0,
                    'storage_saved_mb': 0.0,
                    'status': 'no_old_data'
                }

            total_old_records = sum(record[1] for record in cleanup_candidates)

            logger.info(f"ğŸ“Š ì •ë¦¬ ëŒ€ìƒ ë°œê²¬:")
            logger.info(f"   â€¢ ê¸°ì¤€ì¼: {cutoff_date} ì´ì „")
            logger.info(f"   â€¢ ì´ {len(cleanup_candidates)}ê°œ ì¢…ëª©, {total_old_records}ê°œ ë ˆì½”ë“œ")

            # ê° ì¢…ëª©ë³„ ì •ë¦¬ ëŒ€ìƒ ìƒì„¸ í‘œì‹œ
            for ticker, old_records, oldest_date, newest_old_date in cleanup_candidates[:5]:
                logger.info(f"   â€¢ {ticker}: {old_records}ê°œ ë ˆì½”ë“œ ({oldest_date} ~ {newest_old_date})")

            if len(cleanup_candidates) > 5:
                logger.info(f"   â€¢ ... ì™¸ {len(cleanup_candidates)-5}ê°œ ì¢…ëª©")

            # ì‚¬ìš©ì í™•ì¸ ì—†ì´ ìë™ ì •ë¦¬ (300ì¼ ì´ìƒì€ ì¶©ë¶„íˆ ì•ˆì „í•œ ê¸°ê°„)
            logger.info(f"ğŸ—‘ï¸ {retention_days}ì¼ ì´ìƒ ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì •ë¦¬ ì‹œì‘...")

            # ë°ì´í„° ì‚­ì œ ì‹¤í–‰
            cursor.execute("""
                DELETE FROM ohlcv_data
                WHERE date < ?
            """, (cutoff_date,))

            deleted_records = cursor.rowcount
            conn.commit()

            # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM)
            logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (VACUUM) ì‹¤í–‰ì¤‘...")
            cursor.execute("VACUUM")

            # ì •ë¦¬ í›„ ì €ì¥ê³µê°„ í™•ì¸
            import os
            if os.path.exists(self.db_path):
                file_size_mb = os.path.getsize(self.db_path) / (1024 * 1024)
            else:
                file_size_mb = 0

            # ì˜ˆìƒ ì €ì¥ê³µê°„ ì ˆì•½ ê³„ì‚° (ë ˆì½”ë“œë‹¹ í‰ê·  512ë°”ì´íŠ¸)
            storage_saved_mb = (deleted_records * 512) / (1024 * 1024)

            conn.close()

            logger.info(f"âœ… ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
            logger.info(f"   â€¢ ì‚­ì œëœ ë ˆì½”ë“œ: {deleted_records:,}ê°œ")
            logger.info(f"   â€¢ ì˜í–¥ë°›ì€ ì¢…ëª©: {len(cleanup_candidates)}ê°œ")
            logger.info(f"   â€¢ ì˜ˆìƒ ì ˆì•½ ê³µê°„: {storage_saved_mb:.2f}MB")
            logger.info(f"   â€¢ í˜„ì¬ DB í¬ê¸°: {file_size_mb:.2f}MB")

            return {
                'retention_days': retention_days,
                'cutoff_date': cutoff_date,
                'deleted_records': deleted_records,
                'affected_tickers': len(cleanup_candidates),
                'storage_saved_mb': round(storage_saved_mb, 2),
                'current_db_size_mb': round(file_size_mb, 2),
                'status': 'cleanup_completed'
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'retention_days': retention_days,
                'cutoff_date': cutoff_date,
                'deleted_records': 0,
                'affected_tickers': 0,
                'storage_saved_mb': 0.0,
                'status': f'error: {e}'
            }

    def check_data_retention_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ë°ì´í„° ë³´ì¡´ ìƒíƒœ í™•ì¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ì „ì²´ ë°ì´í„° í˜„í™© ì¡°íšŒ
            cursor.execute("""
                SELECT
                    ticker,
                    COUNT(*) as record_count,
                    MIN(date) as oldest_date,
                    MAX(date) as newest_date,
                    CAST(julianday('now') - julianday(MIN(date)) AS INTEGER) as days_span
                FROM ohlcv_data
                GROUP BY ticker
                ORDER BY days_span DESC
            """)

            results = cursor.fetchall()
            conn.close()

            if not results:
                return {
                    'total_tickers': 0,
                    'max_days': 0,
                    'avg_days': 0,
                    'over_300_days_count': 0,
                    'storage_optimization_needed': False,
                    'status': 'no_data'
                }

            # í†µê³„ ê³„ì‚°
            total_days = sum(row[4] for row in results)
            max_days = max(row[4] for row in results)
            avg_days = total_days / len(results)
            over_300_days_count = sum(1 for row in results if row[4] >= 300)

            # ìŠ¤í† ë¦¬ì§€ ìµœì í™” í•„ìš”ì„± íŒë‹¨
            storage_optimization_needed = (max_days > 300) or (over_300_days_count > 0)

            logger.info(f"ğŸ“Š ë°ì´í„° ë³´ì¡´ ìƒíƒœ:")
            logger.info(f"   â€¢ ì „ì²´ ì¢…ëª©: {len(results)}ê°œ")
            logger.info(f"   â€¢ í‰ê·  ë³´ì¡´ ê¸°ê°„: {avg_days:.1f}ì¼")
            logger.info(f"   â€¢ ìµœëŒ€ ë³´ì¡´ ê¸°ê°„: {max_days}ì¼")
            logger.info(f"   â€¢ 300ì¼+ ë°ì´í„°: {over_300_days_count}ê°œ ì¢…ëª©")
            logger.info(f"   â€¢ ìŠ¤í† ë¦¬ì§€ ìµœì í™” í•„ìš”: {'ì˜ˆ' if storage_optimization_needed else 'ì•„ë‹ˆì˜¤'}")

            return {
                'total_tickers': len(results),
                'max_days': max_days,
                'avg_days': round(avg_days, 1),
                'over_300_days_count': over_300_days_count,
                'storage_optimization_needed': storage_optimization_needed,
                'ticker_details': [(row[0], row[1], row[4]) for row in results[:5]],  # ìƒìœ„ 5ê°œ
                'status': 'analysis_complete'
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë³´ì¡´ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                'total_tickers': 0,
                'max_days': 0,
                'avg_days': 0,
                'over_300_days_count': 0,
                'storage_optimization_needed': False,
                'status': f'error: {e}'
            }

    def auto_manage_data_retention(self, retention_days: int = 300) -> Dict[str, Any]:
        """ìë™ ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰

        300ì¼ ì´ìƒ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” í†µí•© ë©”ì„œë“œ
        """
        logger.info("ğŸ”„ ìë™ ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰")

        # 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸
        status = self.check_data_retention_status()

        result = {
            'retention_days': retention_days,
            'initial_status': status,
            'cleanup_performed': False,
            'cleanup_result': None
        }

        # 2ë‹¨ê³„: ì •ë¦¬ í•„ìš”ì„± íŒë‹¨ ë° ì‹¤í–‰
        if status['storage_optimization_needed']:
            logger.info(f"âš ï¸ {retention_days}ì¼ ì´ìƒ ë°ì´í„° ê°ì§€, ìë™ ì •ë¦¬ ì‹¤í–‰")
            cleanup_result = self.manage_old_data(retention_days)
            result['cleanup_performed'] = True
            result['cleanup_result'] = cleanup_result
        else:
            logger.info(f"âœ… ëª¨ë“  ë°ì´í„°ê°€ {retention_days}ì¼ ì´ë‚´, ì •ë¦¬ ë¶ˆí•„ìš”")

        return result


def verify_database_data():
    """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²°ê³¼ ê²€ì¦"""
    try:
        db_path = "./makenaide_local.db"
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # ì „ì²´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        cursor.execute("SELECT COUNT(*) FROM ohlcv_data")
        total_records = cursor.fetchone()[0]

        # í‹°ì»¤ë³„ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
        cursor.execute("""
            SELECT ticker, COUNT(*) as count,
                   MIN(date) as first_date, MAX(date) as last_date
            FROM ohlcv_data
            GROUP BY ticker
            ORDER BY ticker
        """)
        ticker_stats = cursor.fetchall()

        # ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„° í™•ì¸
        cursor.execute("""
            SELECT ticker, date, close, ma20, rsi
            FROM ohlcv_data
            WHERE ma20 IS NOT NULL AND rsi IS NOT NULL
            LIMIT 5
        """)
        sample_indicators = cursor.fetchall()

        conn.close()

        print("\n" + "="*60)
        print("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ“ˆ ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")

        print(f"\nğŸ“‹ í‹°ì»¤ë³„ í†µê³„:")
        for ticker, count, first_date, last_date in ticker_stats:
            print(f"   - {ticker}: {count}ê°œ ({first_date} ~ {last_date})")

        print(f"\nğŸ§® ê¸°ìˆ ì  ì§€í‘œ ìƒ˜í”Œ:")
        for ticker, date, close, ma20, rsi in sample_indicators:
            print(f"   - {ticker} {date}: ì¢…ê°€={close:.2f}, MA20={ma20:.2f}, RSI={rsi:.1f}")

        return total_records > 0

    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Makenaide Enhanced Data Collector (Quality-Filtered)")
    print("=" * 60)
    print("ğŸ“‹ í’ˆì§ˆ í•„í„°ë§ ì¡°ê±´:")
    print("   â€¢ 13ê°œì›” ì´ìƒ ì›”ë´‰ ë°ì´í„° ë³´ìœ  ì¢…ëª©")
    print("   â€¢ 24ì‹œê°„ ê±°ë˜ëŒ€ê¸ˆ 3ì–µì› ì´ìƒ ì¢…ëª©")
    print("=" * 60)

    try:
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f}MB")

        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = SimpleDataCollector()

        # 1ë‹¨ê³„: ë°ì´í„° ë³´ì¡´ ì •ì±… ìë™ ì‹¤í–‰ (300ì¼ ì´ìƒ ë°ì´í„° ì •ë¦¬)
        print("\nğŸ”„ 1ë‹¨ê³„: ë°ì´í„° ë³´ì¡´ ì •ì±… ì‹¤í–‰")
        print("-" * 40)
        retention_result = collector.auto_manage_data_retention(retention_days=300)

        if retention_result['cleanup_performed']:
            cleanup = retention_result['cleanup_result']
            print(f"ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ:")
            print(f"- ì‚­ì œëœ ë ˆì½”ë“œ: {cleanup['deleted_records']:,}ê°œ")
            print(f"- ì˜í–¥ë°›ì€ ì¢…ëª©: {cleanup['affected_tickers']}ê°œ")
            print(f"- ì ˆì•½ëœ ê³µê°„: {cleanup['storage_saved_mb']}MB")
        else:
            status = retention_result['initial_status']
            print(f"âœ… ë°ì´í„° ë³´ì¡´ ìƒíƒœ ì–‘í˜¸ (ìµœëŒ€ {status['max_days']}ì¼)")

        # 2ë‹¨ê³„: í’ˆì§ˆ í•„í„°ë§ ëª¨ë“œë¡œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        print(f"\nğŸ“Š 2ë‹¨ê³„: OHLCV ë°ì´í„° ìˆ˜ì§‘")
        print("-" * 40)
        results = collector.collect_all_data(test_mode=True, use_quality_filter=True)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"- í’ˆì§ˆ í•„í„°ë§: {'í™œì„±í™”' if results['quality_filter_enabled'] else 'ë¹„í™œì„±í™”'}")
        print(f"- í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {'í™œì„±í™”' if results['test_mode'] else 'ë¹„í™œì„±í™”'}")
        print(f"- ì´ ì¢…ëª©: {results['total_tickers']}ê°œ")
        print(f"- ì²˜ë¦¬ ì™„ë£Œ: {results['summary']['success']}ê°œ")
        print(f"- ìŠ¤í‚µ: {results['summary']['skipped']}ê°œ")
        print(f"- ì˜¤ë¥˜: {results['summary']['failed']}ê°œ")
        print(f"- ì´ ë ˆì½”ë“œ: {results['summary']['total_records']:,}ê°œ")
        print(f"- ì´ ì‹œê°„: {results['processing_time_seconds']}ì´ˆ")

        # íš¨ìœ¨ì„± ê°œì„  ë©”íŠ¸ë¦­ í‘œì‹œ
        if results['quality_filter_enabled']:
            print(f"\nğŸ’¡ í’ˆì§ˆ í•„í„°ë§ íš¨ê³¼:")
            print(f"- ì˜ˆìƒ API ì ˆì•½ë¥ : ~67% (ê³ í’ˆì§ˆ ì¢…ëª©ë§Œ ì²˜ë¦¬)")
            print(f"- ì˜ˆìƒ ì €ì¥ì†Œ ì ˆì•½ë¥ : ~67% (ë…¸ì´ì¦ˆ ë°ì´í„° ì œê±°)")
            print(f"- ë¶„ì„ í’ˆì§ˆ í–¥ìƒ: ê±°ë˜ëŸ‰/ì•ˆì •ì„± í™•ë³´ëœ ì¢…ëª©ë§Œ ì„ ë³„")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"ğŸ“Š ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.1f}MB (ì¦ê°€: {final_memory-initial_memory:.1f}MB)")

        # ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦
        if verify_database_data():
            print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            return True
        else:
            print("âŒ ë°ì´í„° ìˆ˜ì§‘ ê²€ì¦ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)