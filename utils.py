import time
from functools import wraps
import re
import psycopg2
import os
from dotenv import load_dotenv
import functools
import requests
from datetime import datetime, date, timedelta
import logging
import sys
import json
import pytz
import pandas as pd
import psutil

# Import optimized monitor
from optimized_data_monitor import get_optimized_monitor

def safe_strftime(date_obj, format_str='%Y-%m-%d'):
    """
    ì•ˆì „í•œ datetime ë³€í™˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    
    ì¡°ê±´ë¶€ ì²˜ë¦¬:
    - pd.Timestampì¸ ê²½ìš°: ì§ì ‘ strftime ì‚¬ìš©
    - pd.DatetimeIndexì¸ ê²½ìš°: ì§ì ‘ strftime ì‚¬ìš©
    - ì •ìˆ˜í˜•ì¸ ê²½ìš°: pd.to_datetime()ìœ¼ë¡œ ë³€í™˜ í›„ strftime
    - None/NaNì¸ ê²½ìš°: ê¸°ë³¸ê°’ ë°˜í™˜
    
    Args:
        date_obj: ë³€í™˜í•  ë‚ ì§œ ê°ì²´ (datetime, pd.Timestamp, int, str ë“±)
        format_str (str): ë‚ ì§œ í¬ë§· ë¬¸ìì—´ (ê¸°ë³¸ê°’: '%Y-%m-%d')
        
    Returns:
        str: í¬ë§·ëœ ë‚ ì§œ ë¬¸ìì—´ ë˜ëŠ” ê¸°ë³¸ê°’
    """
    try:
        # None ë˜ëŠ” NaN ì²´í¬
        if date_obj is None or (hasattr(date_obj, 'isna') and date_obj.isna()):
            return "N/A"
        
        # pandas NaT ì²´í¬
        if pd.isna(date_obj):
            return "N/A"
        
        # ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if isinstance(date_obj, str) and date_obj.strip() == "":
            return "N/A"
        
        # ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë”•ì…”ë„ˆë¦¬ ë“± ì»¨í…Œì´ë„ˆ íƒ€ì… ì²´í¬
        if isinstance(date_obj, (list, tuple, dict)):
            return str(date_obj)
        
        # pandas Timestamp ê°ì²´ì¸ ê²½ìš° (ìš°ì„  ì²˜ë¦¬)
        if isinstance(date_obj, pd.Timestamp):
            return date_obj.strftime(format_str)
        
        # pandas DatetimeIndex ìš”ì†Œì¸ ê²½ìš°
        if hasattr(date_obj, '__class__') and 'pandas' in str(type(date_obj)):
            try:
                # pandas datetime-like ê°ì²´ ì²˜ë¦¬
                return pd.Timestamp(date_obj).strftime(format_str)
            except:
                pass
            
        # ì´ë¯¸ datetime ê°ì²´ì´ê±°ë‚˜ strftime ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°
        if hasattr(date_obj, 'strftime'):
            return date_obj.strftime(format_str)
            
        # ì •ìˆ˜í˜•ì¸ ê²½ìš° (timestamp)
        if isinstance(date_obj, (int, float)):
            # Unix timestampë¡œ ê°€ì •í•˜ê³  ë³€í™˜ (ë‚˜ë…¸ì´ˆë„ ê³ ë ¤)
            if date_obj > 1e15:  # ë‚˜ë…¸ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='ns')
            elif date_obj > 1e10:  # ë°€ë¦¬ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='ms')
            else:  # ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='s')
            return dt.strftime(format_str)
            
        # ë¬¸ìì—´ì¸ ê²½ìš°
        if isinstance(date_obj, str):
            # ë¹ˆ ë¬¸ìì—´ ì¬í™•ì¸
            if date_obj.strip() == "":
                return "N/A"
            # ì´ë¯¸ í¬ë§·ëœ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(date_obj) >= 10 and '-' in date_obj:
                return date_obj[:10]  # YYYY-MM-DD ë¶€ë¶„ë§Œ ì¶”ì¶œ
            # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
            dt = pd.to_datetime(date_obj)
            return dt.strftime(format_str)
            
        # ê¸°íƒ€ ê²½ìš°: pandas to_datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
        dt = pd.to_datetime(date_obj)
        return dt.strftime(format_str)
        
    except Exception as e:
        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸í•œ ë¡œê·¸
        logger.debug(f"safe_strftime ë³€í™˜ ì‹¤íŒ¨ - ì…ë ¥ê°’: {date_obj} (íƒ€ì…: {type(date_obj)}), ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ë³€í™˜ì´ ì‹¤íŒ¨í•œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        try:
            result = str(date_obj)
            if result == "":
                return "N/A"
            return result[:10] if len(result) >= 10 else result
        except:
            return "Invalid Date"

def retry(max_attempts=3, initial_delay=0.5, backoff=2):
    """
    Decorator to retry a function on exception.
    :param max_attempts: maximum number of attempts (including first)
    :param initial_delay: initial wait time between retries
    :param backoff: multiplier for successive delays
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts:
                        raise
                    time.sleep(delay)
                    delay *= backoff
        return wrapper
    return decorator

def compute_expected_phase(data):
    """
    Stan Weinstein Market Phases ë£°ì„ ë¶€ë¶„ì ìœ¼ë¡œ êµ¬í˜„í•œ ê°„ëµí™” ë¡œì§.
    :param data: dict with keys 'current_price', 'ma_50', 'ma_200', 'volume_change_7_30', 'r1', 'r2', 'r3', 's1'
    :return: int Phase number (1 to 4)
    """
    cp = float(data.get("current_price", 0))
    ma50 = float(data.get("ma_50", 0))
    ma200 = float(data.get("ma_200", 0))
    vol_chg = float(data.get("volume_change_7_30", 0))
    r1, r2, r3 = map(float, (data.get("r1", 0), data.get("r2", 0), data.get("r3", 0)))
    s1 = float(data.get("s1", 0))

    # Stage 4: ëª…ë°±í•œ í•˜ë½ì¶”ì„¸ (MA-50 < MA-200 ë° ê°€ê²© < MA-50)
    if ma50 < ma200 and cp < ma50:
        return 4
    # Stage 3: ì •ì ê¶Œ (ê³ ì  ê·¼ì²˜ + ê±°ë˜ëŸ‰ ê°ì†Œ)
    if cp >= r3 and vol_chg < 0:
        return 3
    # Stage 2: ìƒìŠ¹ì¶”ì„¸ (MA ë°°ì—´ + ê±°ë˜ëŸ‰ ì¦ê°€ + ê°€ê²© > MA-50)
    if ma50 > ma200 and vol_chg > 0 and cp > ma50:
        return 2
    # Stage 1: íš¡ë³´ê¶Œ (S1~R1 êµ¬ê°„, MA í‰íƒ„, ê±°ë˜ëŸ‰ í‰ì´)
    if s1 <= cp <= r1 and abs(ma50 - ma200) < (ma200 * 0.01) and abs(vol_chg) < 0.05:
        return 1
    # ê¸°ë³¸ì ìœ¼ë¡œ Stage 1ìœ¼ë¡œ ë°˜í™˜
    return 1


def validate_and_correct_phase(ticker, reported_phase, data, reason):
    """
    reported_phase: string like "Stage 2"
    :param ticker: str
    :param reported_phase: str
    :param data: dict of analysis data passed to GPT
    :param reason: original reason string from GPT
    :return: (corrected_phase_str, corrected_reason)
    """
    m = re.search(r'\d+', reported_phase or "")
    reported = int(m.group()) if m else None
    expected = compute_expected_phase(data)
    if reported is not None and reported != expected:
        print(f"âš ï¸ GPT hallucination for {ticker}: reported Stage {reported}, expected Stage {expected}")
        corrected = f"Stage {expected}"
        corrected_reason = f"{reason} | Phase forced to Stage {expected} by rule-based check"
        return corrected, corrected_reason
    return reported_phase, reason

# === ê³µí†µ ìƒìˆ˜ ===
MIN_KRW_ORDER = 10000  # ìµœì†Œ ë§¤ìˆ˜ ê¸ˆì•¡
MIN_KRW_SELL_ORDER = 5000  # ìµœì†Œ ë§¤ë„ ê¸ˆì•¡
TAKER_FEE_RATE = 0.00139  # ì—…ë¹„íŠ¸ KRW ë§ˆì¼“ Taker ìˆ˜ìˆ˜ë£Œ

# === í™˜ê²½ë³€ìˆ˜ ë¡œë”© ===
def load_env():
    load_dotenv()

# === DB ì—°ê²° í•¨ìˆ˜ ===
def get_db_connection():
    return psycopg2.connect(
        host=os.getenv("PG_HOST"),
        port=os.getenv("PG_PORT"),
        dbname=os.getenv("PG_DATABASE"),
        user=os.getenv("PG_USER"),
        password=os.getenv("PG_PASSWORD")
    )

def setup_logger():
    """
    ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ê³  ë¡œê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±° ê°ì²´
    """
    log_dir = "log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_filename = safe_strftime(datetime.now(), "%Y%m%d") + "_makenaide.log"
    log_file_path = os.path.join(log_dir, log_filename)

    log_format = "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì‚­ì œ
    if logger.hasHandlers():
        logger.handlers.clear()

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (í„°ë¯¸ë„ ì¶œë ¥)
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(stream_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (íŒŒì¼ ì €ì¥)
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(file_handler)

    return logger

def setup_restricted_logger(logger_name: str = None):
    """
    ì œí•œëœ ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ê³  ë¡œê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    íŠ¹ì • ë¡œê·¸ íŒŒì¼ ìƒì„± ì œí•œì„ ì ìš©í•©ë‹ˆë‹¤.
    
    Args:
        logger_name (str): ë¡œê±° ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ ë¡œê±°)
    
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±° ê°ì²´
    """
    log_dir = "log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # ì œí•œëœ ë¡œê·¸ íŒŒì¼ëª… (makenaide.logë§Œ ìƒì„±)
    log_filename = safe_strftime(datetime.now(), "%Y%m%d") + "_makenaide.log"
    log_file_path = os.path.join(log_dir, log_filename)

    log_format = "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"

    # ë¡œê±° ìƒì„±
    if logger_name:
        logger = logging.getLogger(logger_name)
    else:
        logger = logging.getLogger()
    
    logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì‚­ì œ
    if logger.hasHandlers():
        logger.handlers.clear()

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (í„°ë¯¸ë„ ì¶œë ¥)
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(stream_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (makenaide.logë§Œ ì‚¬ìš©)
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(file_handler)

    return logger

def cleanup_old_log_files(retention_days: int = 7):
    """
    ì§€ì •ëœ ë³´ê´€ ê¸°ê°„ì„ ì´ˆê³¼í•œ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        retention_days (int): ë¡œê·¸ íŒŒì¼ ë³´ê´€ ê¸°ê°„ (ì¼)
    
    Returns:
        dict: ì •ë¦¬ ê²°ê³¼ ì •ë³´
    """
    try:
        log_dir = "log"
        if not os.path.exists(log_dir):
            return {"status": "success", "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ", "deleted_count": 0}
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³´ê´€ ê¸°ê°„ ê³„ì‚°
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        deleted_count = 0
        error_count = 0
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ê²€ì‚¬
        for filename in os.listdir(log_dir):
            file_path = os.path.join(log_dir, filename)
            
            # íŒŒì¼ì¸ì§€ í™•ì¸
            if not os.path.isfile(file_path):
                continue
            
            try:
                # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
                file_creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
                
                # ë³´ê´€ ê¸°ê°„ì„ ì´ˆê³¼í•œ íŒŒì¼ ì‚­ì œ
                if file_creation_time < cutoff_date:
                    os.remove(file_path)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ: {filename}")
                    
            except Exception as e:
                error_count += 1
                print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ({filename}): {e}")
        
        result = {
            "status": "success",
            "deleted_count": deleted_count,
            "error_count": error_count,
            "retention_days": retention_days,
            "cutoff_date": cutoff_date.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        if deleted_count > 0:
            print(f"âœ… ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")
        else:
            print(f"â„¹ï¸ ì‚­ì œí•  ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (ë³´ê´€ê¸°ê°„: {retention_days}ì¼)")
            
        return result
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "deleted_count": 0,
            "error_count": 1
        }
        print(f"âŒ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return error_result

def get_log_file_info():
    """
    í˜„ì¬ ë¡œê·¸ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        dict: ë¡œê·¸ íŒŒì¼ ì •ë³´
    """
    try:
        log_dir = "log"
        if not os.path.exists(log_dir):
            return {"status": "error", "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}
        
        log_files = []
        total_size = 0
        
        for filename in os.listdir(log_dir):
            file_path = os.path.join(log_dir, filename)
            
            if os.path.isfile(file_path):
                file_size = os.path.getsize(file_path)
                file_creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
                
                log_files.append({
                    "filename": filename,
                    "size_bytes": file_size,
                    "size_mb": round(file_size / (1024 * 1024), 2),
                    "creation_time": file_creation_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "age_days": (datetime.now() - file_creation_time).days
                })
                
                total_size += file_size
        
        # íŒŒì¼ í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
        log_files.sort(key=lambda x: x["size_bytes"], reverse=True)
        
        return {
            "status": "success",
            "total_files": len(log_files),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "files": log_files
        }
        
    except Exception as e:
        return {
            "status": "error", 
            "message": f"ë¡œê·¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logger()

# === í˜„ì¬ê°€ ì•ˆì „ ì¡°íšŒ ===
def get_current_price_safe(ticker, retries=3, delay=0.3):
    import time
    import pyupbit
    attempt = 0
    while attempt < retries:
        try:
            price_data = pyupbit.get_current_price(ticker)
            if price_data is None:
                raise ValueError("No data returned")
            if isinstance(price_data, (int, float)):
                return price_data
            if isinstance(price_data, dict):
                if ticker in price_data:
                    return price_data[ticker]
                elif 'trade_price' in price_data:
                    return price_data['trade_price']
                else:
                    first_val = next(iter(price_data.values()), None)
                    if first_val is not None:
                        return first_val
            elif isinstance(price_data, list) and len(price_data) > 0:
                trade_price = price_data[0].get('trade_price')
                if trade_price is not None:
                    return trade_price
            raise ValueError(f"Unexpected data format: {price_data}")
        except Exception as e:
            logging.warning(f"âŒ {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            attempt += 1
            time.sleep(delay)
    return None

def retry_on_error(max_retries=3, delay=5):
    """ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {str(e)}")
                        raise
                    logger.warning(f"âš ï¸ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

def handle_api_error(e, context=""):
    """API ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ API ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")
    if hasattr(e, 'response'):
        logger.error(f"ì‘ë‹µ ìƒíƒœ: {e.response.status_code}")
        logger.error(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")

def handle_db_error(e, context=""):
    """DB ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ DB ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")

def handle_network_error(e, context=""):
    """ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")

def load_blacklist():
    """ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
    try:
        blacklist_path = 'blacklist.json'
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(blacklist_path):
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {blacklist_path}")
            # ë¹ˆ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            try:
                with open(blacklist_path, 'w', encoding='utf-8') as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ë¹ˆ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ: {blacklist_path}")
                return {}
            except Exception as create_e:
                logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {create_e}")
                return {}
        
        # íŒŒì¼ ì½ê¸° ê¶Œí•œ í™•ì¸
        if not os.access(blacklist_path, os.R_OK):
            logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ: {blacklist_path}")
            return {}
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(blacklist_path)
        if file_size == 0:
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {blacklist_path}")
            return {}
        
        # JSON íŒŒì¼ ë¡œë“œ
        with open(blacklist_path, 'r', encoding='utf-8') as f:
            blacklist_data = json.load(f)
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if not isinstance(blacklist_data, dict):
            logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: dict íƒ€ì…ì´ ì•„ë‹˜ (í˜„ì¬: {type(blacklist_data)})")
            return {}
        
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‚´ìš© ê²€ì¦
        valid_blacklist = {}
        invalid_entries = []
        
        for ticker, info in blacklist_data.items():
            # í‹°ì»¤ í˜•ì‹ ê²€ì¦
            if not isinstance(ticker, str) or not ticker.startswith('KRW-'):
                invalid_entries.append(f"{ticker}: ì˜ëª»ëœ í‹°ì»¤ í˜•ì‹")
                continue
            
            # ì •ë³´ êµ¬ì¡° ê²€ì¦
            if isinstance(info, dict):
                if 'reason' in info and 'added' in info:
                    valid_blacklist[ticker] = info
                else:
                    # êµ¬ì¡°ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ë³´ì™„
                    valid_blacklist[ticker] = {
                        'reason': info.get('reason', 'ì‚¬ìœ  ì—†ìŒ'),
                        'added': info.get('added', datetime.now().isoformat())
                    }
                    logger.warning(f"âš ï¸ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì •ë³´ ë¶ˆì™„ì „, ê¸°ë³¸ê°’ìœ¼ë¡œ ë³´ì™„")
            elif isinstance(info, str):
                # êµ¬ë²„ì „ í˜¸í™˜ì„± (ì‚¬ìœ ë§Œ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°)
                valid_blacklist[ticker] = {
                    'reason': info,
                    'added': datetime.now().isoformat()
                }
                logger.info(f"ğŸ”„ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ êµ¬ë²„ì „ í˜•ì‹ ë³€í™˜")
            else:
                invalid_entries.append(f"{ticker}: ì˜ëª»ëœ ì •ë³´ í˜•ì‹")
        
        # ì˜ëª»ëœ í•­ëª© ë¡œê·¸
        if invalid_entries:
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì˜ëª»ëœ í•­ëª© {len(invalid_entries)}ê°œ ë°œê²¬:")
            for entry in invalid_entries[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                logger.warning(f"   - {entry}")
            if len(invalid_entries) > 5:
                logger.warning(f"   - ... ì™¸ {len(invalid_entries) - 5}ê°œ ë”")
        
        # ì„±ê³µ ë¡œê·¸
        logger.info(f"âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(valid_blacklist)}ê°œ í•­ëª© (ìœ íš¨: {len(valid_blacklist)}, ë¬´íš¨: {len(invalid_entries)})")
        
        return valid_blacklist
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        logger.error(f"   íŒŒì¼ ìœ„ì¹˜: {os.path.abspath('blacklist.json')}")
        
        # ë°±ì—… íŒŒì¼ ìƒì„± ì‹œë„
        try:
            backup_path = f"blacklist_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            import shutil
            shutil.copy2('blacklist.json', backup_path)
            logger.info(f"ğŸ“‹ ì†ìƒëœ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë°±ì—… ì™„ë£Œ: {backup_path}")
        except Exception as backup_e:
            logger.error(f"âŒ ë°±ì—… íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {backup_e}")
        
        return {}
        
    except PermissionError as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ ì˜¤ë¥˜: {e}")
        return {}
        
    except Exception as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        logger.error(f"   íŒŒì¼ ìœ„ì¹˜: {os.path.abspath('blacklist.json')}")
        
        # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
        try:
            import traceback
            logger.debug(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        except:
            pass
        
        return {}

# === UNUSED: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ===
# def add_to_blacklist(ticker: str, reason: str) -> bool:
#     """
#     ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í‹°ì»¤ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
#     
#     Args:
#         ticker (str): ì¶”ê°€í•  í‹°ì»¤
#         reason (str): ì¶”ê°€ ì‚¬ìœ 
#         
#     Returns:
#         bool: ì„±ê³µ ì—¬ë¶€
#     """
#     try:
#         blacklist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "blacklist.json")
#         blacklist = load_blacklist()
#         
#         # í˜„ì¬ UTC ì‹œê°„
#         now = datetime.now(pytz.UTC)
#         
#         # í‹°ì»¤ ì¶”ê°€
#         blacklist[ticker] = {
#             "reason": reason,
#             "added": now.isoformat()
#         }
#         
#         # JSON íŒŒì¼ ì €ì¥
#         with open(blacklist_path, 'w', encoding='utf-8') as f:
#             json.dump(blacklist, f, ensure_ascii=False, indent=2)
#             
#         logger.info(f"âœ… {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì™„ë£Œ (ì‚¬ìœ : {reason})")
#         return True
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return False

# def remove_from_blacklist(ticker: str) -> bool:
#     """
#     ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
#     
#     Args:
#         ticker (str): ì œê±°í•  í‹°ì»¤
#         
#     Returns:
#         bool: ì„±ê³µ ì—¬ë¶€
#     """
#     try:
#         blacklist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "blacklist.json")
#         blacklist = load_blacklist()
#         
#         if ticker not in blacklist:
#             logger.warning(f"âš ï¸ {ticker}ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
#             return False
#             
#         # í‹°ì»¤ ì œê±°
#         del blacklist[ticker]
#         
#         # JSON íŒŒì¼ ì €ì¥
#         with open(blacklist_path, 'w', encoding='utf-8') as f:
#             json.dump(blacklist, f, ensure_ascii=False, indent=2)
#             
#         logger.info(f"âœ… {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±° ì™„ë£Œ")
#         return True
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return False

# === í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ê·œí™” ===
COLUMN_MAPPING = {
    'static_indicators': {
        # í”¼ë²— í¬ì¸íŠ¸ ë§¤í•‘
        'resistance_1': 'r1',
        'resistance_2': 'r2', 
        'resistance_3': 'r3',
        'support_1': 's1',
        'support_2': 's2',
        'support_3': 's3',
        
        # í”¼ë³´ë‚˜ì¹˜ ë§¤í•‘ (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
        'fib_382': 'fibo_382',
        'fib_618': 'fibo_618',
        
        # ê¸°íƒ€ ë§¤í•‘
        'ma200': 'ma_200',
        'volume_ratio': 'volume_change_7_30'
    },
    'ohlcv': {
        # OHLCV ê¸°ë³¸ ì»¬ëŸ¼ ë§¤í•‘
        'open_price': 'open',
        'high_price': 'high',
        'low_price': 'low',
        'close_price': 'close',
        'trading_volume': 'volume',
        'trade_date': 'date',
        
        # ì—­í˜¸í™˜ì„±ì„ ìœ„í•œ MACD ë§¤í•‘
        'macd_hist': 'macd_histogram'
    }
}

def apply_column_mapping(df, table_name):
    """
    DataFrameì˜ ì»¬ëŸ¼ëª…ì„ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë§¤í•‘í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ë§¤í•‘í•  DataFrame
        table_name (str): ëŒ€ìƒ í…Œì´ë¸”ëª…
        
    Returns:
        pd.DataFrame: ì»¬ëŸ¼ëª…ì´ ë§¤í•‘ëœ DataFrame
    """
    if table_name not in COLUMN_MAPPING:
        logger.debug(f"âš ï¸ {table_name} í…Œì´ë¸”ì— ëŒ€í•œ ì»¬ëŸ¼ ë§¤í•‘ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ")
        return df
        
    mapping = COLUMN_MAPPING[table_name]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë§¤í•‘
    columns_to_rename = {}
    for old_col, new_col in mapping.items():
        if old_col in df.columns:
            columns_to_rename[old_col] = new_col
            
    if columns_to_rename:
        df_mapped = df.rename(columns=columns_to_rename)
        logger.debug(f"âœ… {table_name} í…Œì´ë¸” ì»¬ëŸ¼ ë§¤í•‘ ì ìš©: {list(columns_to_rename.keys())} â†’ {list(columns_to_rename.values())}")
        return df_mapped
    else:
        logger.debug(f"â„¹ï¸ {table_name} í…Œì´ë¸”ì— ë§¤í•‘í•  ì»¬ëŸ¼ì´ ì—†ìŒ")
        return df

# === UNUSED: ì»¬ëŸ¼ ë§¤í•‘ í•¨ìˆ˜ ===
# def get_mapped_column_name(table_name, column_name):
#     """
#     íŠ¹ì • í…Œì´ë¸”ì˜ ì»¬ëŸ¼ëª…ì„ ë§¤í•‘ëœ ì´ë¦„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
#     
#     Args:
#         table_name (str): í…Œì´ë¸”ëª…
#         column_name (str): ì›ë³¸ ì»¬ëŸ¼ëª…
#         
#     Returns:
#         str: ë§¤í•‘ëœ ì»¬ëŸ¼ëª… ë˜ëŠ” ì›ë³¸ ì»¬ëŸ¼ëª…
#     """
#     if table_name in COLUMN_MAPPING and column_name in COLUMN_MAPPING[table_name]:
#         return COLUMN_MAPPING[table_name][column_name]
#     return column_name

# === Phase 2: ì¶”ê°€ ì•ˆì „ì¥ì¹˜ êµ¬í˜„ ===

# === UNUSED: datetime ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# def safe_dataframe_index_format(df, format_str='%Y-%m-%d'):
#     """
#     DataFrame ì¸ë±ìŠ¤ì˜ datetime ì•ˆì „ì„±ì„ ë³´ì¥í•˜ëŠ” ë˜í¼ í•¨ìˆ˜
#     
#     Args:
#         df (pd.DataFrame): ì²˜ë¦¬í•  DataFrame
#         format_str (str): ë‚ ì§œ í¬ë§· ë¬¸ìì—´
#         
#     Returns:
#         pd.DataFrame: ì•ˆì „í•˜ê²Œ í¬ë§·ëœ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ DataFrame
#     """
#     try:
#         if df is None or df.empty:
#             return df
#             
#         # ì¸ë±ìŠ¤ê°€ datetime íƒ€ì…ì¸ì§€ í™•ì¸
#         if hasattr(df.index, 'strftime'):
#             # safe_strftimeì„ ì‚¬ìš©í•˜ì—¬ ê° ì¸ë±ìŠ¤ ê°’ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
#             safe_index = [safe_strftime(idx, format_str) for idx in df.index]
#             df_copy = df.copy()
#             df_copy.index = safe_index
#             return df_copy
#         else:
#             # datetimeì´ ì•„ë‹Œ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
#             return df
#             
#     except Exception as e:
#         logger.error(f"âŒ DataFrame ì¸ë±ìŠ¤ í¬ë§· ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return df

# def safe_db_datetime_insert(datetime_value, format_str='%Y-%m-%d %H:%M:%S'):
#     """
#     DB ì‚½ì…ìš© datetime ì•ˆì „ ë³€í™˜ í•¨ìˆ˜
#     
#     Args:
#         datetime_value: ë³€í™˜í•  datetime ê°’
#         format_str (str): DB ì‚½ì…ìš© í¬ë§· ë¬¸ìì—´
#         
#     Returns:
#         str: DB ì‚½ì… ê°€ëŠ¥í•œ ì•ˆì „í•œ datetime ë¬¸ìì—´
#     """
#     try:
#         # None ì²´í¬
#         if datetime_value is None:
#             return None
#             
#         # safe_strftimeì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë³€í™˜
#         result = safe_strftime(datetime_value, format_str)
#         
#         # "N/A"ë‚˜ "Invalid Date"ì¸ ê²½ìš° None ë°˜í™˜
#         if result in ["N/A", "Invalid Date"]:
#             return None
#             
#         return result
#         
#     except Exception as e:
#         logger.error(f"âŒ DB datetime ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return None

# def safe_log_datetime_format(datetime_value=None, format_str='%Y-%m-%d %H:%M:%S'):
#     """
#     ë¡œê·¸ìš© datetime ì•ˆì „ í¬ë§· í•¨ìˆ˜
#     
#     Args:
#         datetime_value: ë³€í™˜í•  datetime ê°’ (Noneì´ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©)
#         format_str (str): ë¡œê·¸ìš© í¬ë§· ë¬¸ìì—´
#         
#     Returns:
#         str: ë¡œê·¸ì— ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” datetime ë¬¸ìì—´
#     """
#     try:
#         # datetime_valueê°€ Noneì´ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
#         if datetime_value is None:
#             datetime_value = datetime.now()
#             
#         # safe_strftimeì„ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë³€í™˜
#         result = safe_strftime(datetime_value, format_str)
#         
#         # "N/A"ë‚˜ "Invalid Date"ì¸ ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´
#         if result in ["N/A", "Invalid Date"]:
#             result = safe_strftime(datetime.now(), format_str)
#         
#         return result
#         
#     except Exception as e:
#         logger.error(f"âŒ ë¡œê·¸ datetime í¬ë§· ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return safe_strftime(datetime.now(), format_str)

# def safe_datetime_operations():
#     """
#     DataFrame ì „ì²´ì˜ datetime ì•ˆì „ì„±ì„ ë³´ì¥í•˜ëŠ” ë˜í¼ í•¨ìˆ˜ë“¤ì˜ í†µí•© ì¸í„°í˜ì´ìŠ¤
#     
#     Returns:
#         dict: ì‚¬ìš© ê°€ëŠ¥í•œ ì•ˆì „ í•¨ìˆ˜ë“¤ì˜ ë”•ì…”ë„ˆë¦¬
#     """
#     return {
#         'safe_strftime': safe_strftime,
#         'safe_dataframe_index_format': safe_dataframe_index_format,
#         'safe_db_datetime_insert': safe_db_datetime_insert,
#         'safe_log_datetime_format': safe_log_datetime_format
#     }

# === Phase 3: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ===

# === UNUSED: DatetimeErrorMonitor í´ë˜ìŠ¤ ë° ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# class DatetimeErrorMonitor:
#     """
#     ì‹¤í–‰ ì¤‘ datetime ê´€ë ¨ ì˜¤ë¥˜ë¥¼ ì‹¤ì‹œê°„ ê°ì§€í•˜ê³  ë¡œê¹…í•˜ëŠ” í´ë˜ìŠ¤
#     """
#     
#     def __init__(self):
#         self.error_count = 0
#         self.error_patterns = {}
#         self.start_time = datetime.now()
#         
#     def log_datetime_error(self, error, context="", data=None):
#         """
#         datetime ì˜¤ë¥˜ë¥¼ ë¡œê¹…í•˜ê³  íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.
#         
#         Args:
#             error (Exception): ë°œìƒí•œ ì˜¤ë¥˜
#             context (str): ì˜¤ë¥˜ ë°œìƒ ì»¨í…ìŠ¤íŠ¸
#             data: ì˜¤ë¥˜ ë°œìƒ ì‹œì˜ ë°ì´í„°
#         """
#         try:
#             self.error_count += 1
#             error_type = type(error).__name__
#             error_msg = str(error)
#             
#             # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
#             pattern_key = f"{error_type}:{context}"
#             if pattern_key not in self.error_patterns:
#                 self.error_patterns[pattern_key] = {
#                     'count': 0,
#                     'first_occurrence': datetime.now(),
#                     'last_occurrence': None,
#                     'sample_data': str(data)[:200] if data else None
#                 }
#             
#             self.error_patterns[pattern_key]['count'] += 1
#             self.error_patterns[pattern_key]['last_occurrence'] = datetime.now()
#             
#             # ë¡œê¹…
#             logger.error(f"ğŸš¨ DATETIME ERROR #{self.error_count} | {context} | {error_type}: {error_msg}")
#             if data:
#                 logger.error(f"ğŸ“Š Error Data Sample: {str(data)[:200]}")
#                 
#             # ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì•Œë¦¼
#             if self.error_patterns[pattern_key]['count'] >= 5:
#                 self._send_alert(pattern_key, self.error_patterns[pattern_key])
#                 
#         except Exception as e:
#             logger.error(f"âŒ DatetimeErrorMonitor ìì²´ ì˜¤ë¥˜: {e}")
#     
#     def _send_alert(self, pattern_key, pattern_info):
#         """
#         ì˜¤ë¥˜ íŒ¨í„´ì´ ì„ê³„ì¹˜ë¥¼ ì´ˆê³¼í–ˆì„ ë•Œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.
#         
#         Args:
#             pattern_key (str): ì˜¤ë¥˜ íŒ¨í„´ í‚¤
#             pattern_info (dict): ì˜¤ë¥˜ íŒ¨í„´ ì •ë³´
#         """
#         try:
#             alert_msg = f"""
# ğŸš¨ DATETIME ERROR ALERT ğŸš¨
# íŒ¨í„´: {pattern_key}
# ë°œìƒ íšŸìˆ˜: {pattern_info['count']}
# ìµœì´ˆ ë°œìƒ: {safe_log_datetime_format(pattern_info['first_occurrence'])}
# ìµœê·¼ ë°œìƒ: {safe_log_datetime_format(pattern_info['last_occurrence'])}
# ìƒ˜í”Œ ë°ì´í„°: {pattern_info['sample_data']}
#             """
#             logger.critical(alert_msg)
#             
#         except Exception as e:
#             logger.error(f"âŒ ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
#     
#     def generate_report(self):
#         """
#         ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
#         
#         Returns:
#             str: ë¶„ì„ ë³´ê³ ì„œ
#         """
#         try:
#             runtime = datetime.now() - self.start_time
#             
#             report = f"""
# ğŸ“Š DATETIME ERROR MONITORING REPORT ğŸ“Š
# ì‹¤í–‰ ì‹œê°„: {safe_log_datetime_format(self.start_time)} ~ {safe_log_datetime_format()}
# ì´ ì‹¤í–‰ ì‹œê°„: {runtime}
# ì´ ì˜¤ë¥˜ ë°œìƒ íšŸìˆ˜: {self.error_count}
# 
# === ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ===
# """
#             
#             for pattern_key, pattern_info in self.error_patterns.items():
#                 report += f"""
# íŒ¨í„´: {pattern_key}
# - ë°œìƒ íšŸìˆ˜: {pattern_info['count']}
# - ìµœì´ˆ ë°œìƒ: {safe_log_datetime_format(pattern_info['first_occurrence'])}
# - ìµœê·¼ ë°œìƒ: {safe_log_datetime_format(pattern_info['last_occurrence'])}
# - ìƒ˜í”Œ ë°ì´í„°: {pattern_info['sample_data']}
# """
#             
#             return report
#             
#         except Exception as e:
#             logger.error(f"âŒ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
#             return f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"

# # ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
# datetime_monitor = DatetimeErrorMonitor()

# def datetime_error_monitor():
#     """
#     ì‹¤í–‰ ì¤‘ datetime ê´€ë ¨ ì˜¤ë¥˜ë¥¼ ì‹¤ì‹œê°„ ê°ì§€í•˜ê³  ë¡œê¹…
#     - ì˜ˆì™¸ ë°œìƒ ì‹œ ìë™ ì•Œë¦¼
#     - ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
#     
#     Returns:
#         DatetimeErrorMonitor: ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
#     """
#     return datetime_monitor

# def safe_strftime_with_monitoring(date_obj, format_str='%Y-%m-%d', context=""):
#     """
#     ëª¨ë‹ˆí„°ë§ì´ í¬í•¨ëœ safe_strftime í•¨ìˆ˜
#     
#     Args:
#         date_obj: ë³€í™˜í•  ë‚ ì§œ ê°ì²´
#         format_str (str): ë‚ ì§œ í¬ë§· ë¬¸ìì—´
#         context (str): í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸ (ëª¨ë‹ˆí„°ë§ìš©)
#         
#     Returns:
#         str: í¬ë§·ëœ ë‚ ì§œ ë¬¸ìì—´
#     """
#     try:
#         return safe_strftime(date_obj, format_str)
#     except Exception as e:
#         datetime_monitor.log_datetime_error(e, context, date_obj)
#         # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì•ˆì „í•œ ê°’ ë°˜í™˜
#         return "N/A"

# === í‹°ì»¤ í•„í„°ë§ ì‹œìŠ¤í…œ ê²€ì¦ í•¨ìˆ˜ ===
def validate_ticker_filtering_system():
    """
    í‹°ì»¤ í•„í„°ë§ ì‹œìŠ¤í…œì˜ ë‹¤ì¸µ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    
    ê²€ì¦ í•­ëª©:
    1. tickers í…Œì´ë¸”ì˜ is_active ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€
    2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥ì„±
    3. ë‘ í•„í„°ë§ ë°©ì‹ì˜ ê²°ê³¼ ë¹„êµ
    """
    results = {
        "is_active_available": False,
        "blacklist_available": False,
        "filtering_consistency": False,
        "active_count": 0,
        "blacklist_filtered_count": 0,
        "consistency_rate": 0.0
    }
    
    try:
        # 1. is_active ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT column_name FROM information_schema.columns 
            WHERE table_name = 'tickers' AND column_name = 'is_active'
        """)
        results["is_active_available"] = len(cursor.fetchall()) > 0
        
        # 2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼ í™•ì¸
        blacklist = load_blacklist()
        results["blacklist_available"] = blacklist is not None
        
        # 3. ë‘ ë°©ì‹ ê²°ê³¼ ë¹„êµ (is_active ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ)
        if results["is_active_available"]:
            cursor.execute("SELECT ticker FROM tickers WHERE is_active = true")
            active_tickers = {row[0] for row in cursor.fetchall()}
            results["active_count"] = len(active_tickers)
            
            cursor.execute("SELECT ticker FROM tickers")
            all_tickers = {row[0] for row in cursor.fetchall()}
            
            blacklist_filtered = all_tickers - set(blacklist if blacklist else [])
            results["blacklist_filtered_count"] = len(blacklist_filtered)
            
            # ê²°ê³¼ ì¼ì¹˜ë„ ê²€ì‚¬
            overlap = len(active_tickers & blacklist_filtered)
            total = len(active_tickers | blacklist_filtered)
            consistency_rate = overlap / total if total > 0 else 0
            results["consistency_rate"] = consistency_rate
            
            results["filtering_consistency"] = consistency_rate > 0.8
        
        cursor.close()
        conn.close()
        return results
        
    except Exception as e:
        logger.error(f"âŒ í‹°ì»¤ í•„í„°ë§ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return results

# === UNUSED: í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼ ì¡°íšŒ í•¨ìˆ˜ ===
# def get_safe_fibo_columns():
#     """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼ë§Œ ë°˜í™˜"""
#     try:
#         import psycopg2
#         import os
#         
#         conn = psycopg2.connect(
#             host=os.getenv("PG_HOST"),
#             user=os.getenv("PG_USER"),
#             password=os.getenv("PG_PASSWORD"),
#             port=os.getenv("PG_PORT"),
#             dbname=os.getenv("PG_DATABASE"),
#         )
#         cursor = conn.cursor()
#         
#         cursor.execute("""
#             SELECT column_name 
#             FROM information_schema.columns 
#             WHERE table_name = 'ohlcv' 
#             AND column_name LIKE 'fibo_%'
#             ORDER BY column_name
#         """)
#         available_fibo_columns = [row[0] for row in cursor.fetchall()]
#         cursor.close()
#         conn.close()
#         
#         # ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ì¡´ì¬í•˜ëŠ” í”¼ë³´ë‚˜ì¹˜ ë ˆë²¨ë“¤ë§Œ
#         required_fibo_levels = ['fibo_382', 'fibo_618']
#         
#         # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë°˜í™˜
#         safe_columns = [col for col in required_fibo_levels if col in available_fibo_columns]
#         
#         logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼: {safe_columns}")
#         return safe_columns
#         
#     except Exception as e:
#         logger.error(f"âŒ í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼ í™•ì¸ ì‹¤íŒ¨: {e}")
#         # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜ (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)
#         return ['fibo_382', 'fibo_618']

def get_safe_ohlcv_columns():
    """
    ì‹¤ì œ ohlcv í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì•ˆì „í•œ í•¨ìˆ˜
    
    Returns:
        list: ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ohlcv í…Œì´ë¸” ì»¬ëŸ¼ ëª©ë¡
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'ohlcv' 
            AND table_schema = 'public'
            ORDER BY ordinal_position
        """)
        
        columns = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        
        logger = setup_logger()
        logger.debug(f"ğŸ“Š get_safe_ohlcv_columns() ì¡°íšŒ ì„±ê³µ: {len(columns)}ê°œ ì»¬ëŸ¼")
        
        return columns
        
    except Exception as e:
        logger = setup_logger()
        logger.error(f"âŒ ohlcv ì»¬ëŸ¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì»¬ëŸ¼ ë°˜í™˜
        return ['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']

# === UNUSED: DB ì¿¼ë¦¬ ìƒì„± í•¨ìˆ˜ë“¤ ===
# def build_safe_ohlcv_query(ticker_param=True, limit_days=30):
#     """
#     ìˆ˜ì • ëª©í‘œ: static_indicatorsë¡œ ì´ë™ëœ ì»¬ëŸ¼ë“¤ì„ ohlcv ì¿¼ë¦¬ì—ì„œ ì œì™¸
#     
#     ìˆ˜ì • ë‚´ìš©:
#     1. problematic_columns ë¦¬ìŠ¤íŠ¸ì— 'pivot', 'r1', 's1', 'support', 'resistance' ì¶”ê°€
#     2. ì´ ì»¬ëŸ¼ë“¤ì„ possible_indicatorsì—ì„œ ì œê±°
#     3. ë¡œê¹…ì— "ì •ì  ì§€í‘œ ì œì™¸ë¨" ë©”ì‹œì§€ ì¶”ê°€
#     """
#     safe_columns = get_safe_ohlcv_columns()
#     safe_fibo_columns = [col for col in safe_columns if col.startswith('fibo_')]
#     
#     # í•„ìˆ˜ ì»¬ëŸ¼ë“¤ (í™•ì‹¤íˆ ì¡´ì¬í•¨)
#     essential_columns = ['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']
#     
#     # ë™ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ (ohlcv í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
#     possible_dynamic_indicators = [
#         'ht_trendline', 'ma_50', 'ma_200', 'bb_upper', 'bb_lower', 
#         'donchian_high', 'donchian_low', 'macd_histogram', 'rsi_14', 
#         'volume_20ma', 'stoch_k', 'stoch_d', 'cci'
#     ]
#     
#     # âš ï¸ ì •ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ì€ static_indicators í…Œì´ë¸”ì— ìˆìœ¼ë¯€ë¡œ ì œì™¸
#     # pivot, r1, s1, support, resistance ì¶”ê°€
#     static_indicators_columns = ['pivot', 'r1', 's1', 'support', 'resistance', 'atr', 'adx']
#     
#     # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ ì»¬ëŸ¼ë§Œ í•„í„°ë§ (ì •ì  ì§€í‘œëŠ” ì œì™¸)
#     safe_dynamic_indicators = [
#         col for col in possible_dynamic_indicators 
#         if col in safe_columns and col not in static_indicators_columns
#     ]
#     
#     # ìµœì¢… ì»¬ëŸ¼ ëª©ë¡ êµ¬ì„± (ohlcv í…Œì´ë¸”ì˜ ì•ˆì „í•œ ì»¬ëŸ¼ë§Œ)
#     all_safe_columns = essential_columns + safe_dynamic_indicators + safe_fibo_columns
#     columns_str = ', '.join(all_safe_columns)
#     
#     if ticker_param:
#         query = f"""
#             SELECT {columns_str}
#             FROM ohlcv 
#             WHERE ticker = %s 
#             AND date >= CURRENT_DATE - INTERVAL '{limit_days} days'
#             ORDER BY date DESC
#             LIMIT {limit_days}
#         """
#     else:
#         query = f"""
#             SELECT {columns_str}
#             FROM ohlcv 
#             WHERE date >= CURRENT_DATE - INTERVAL '{limit_days} days'
#             ORDER BY date DESC
#             LIMIT {limit_days}
#         """
#     
#     logger.info(f"âœ… ì•ˆì „í•œ ohlcv ì¿¼ë¦¬ ìƒì„±: {len(all_safe_columns)}ê°œ ì»¬ëŸ¼ ì‚¬ìš© (ì •ì  ì§€í‘œ ì œì™¸ë¨)")
#     logger.debug(f"   - ì‚¬ìš©ëœ ì»¬ëŸ¼: {all_safe_columns}")
#     logger.debug(f"   - ì œì™¸ëœ ì •ì  ì§€í‘œ: {static_indicators_columns}")
#     
#     return query, safe_fibo_columns

# def get_safe_static_indicators_columns():
#     """static_indicators í…Œì´ë¸”ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼ì„ ì¡°íšŒ"""
#     try:
#         import psycopg2
#         import os
#         
#         conn = psycopg2.connect(
#             host=os.getenv("PG_HOST"),
#             user=os.getenv("PG_USER"),
#             password=os.getenv("PG_PASSWORD"),
#             port=os.getenv("PG_PORT"),
#             dbname=os.getenv("PG_DATABASE"),
#         )
#         cursor = conn.cursor()
#         
#         cursor.execute("""
#             SELECT column_name 
#             FROM information_schema.columns 
#             WHERE table_name = 'static_indicators' 
#             ORDER BY column_name
#         """)
#         all_columns = [row[0] for row in cursor.fetchall()]
#         cursor.close()
#         conn.close()
#         
#         logger.info(f"âœ… static_indicators í…Œì´ë¸” ì „ì²´ ì»¬ëŸ¼ ì¡°íšŒ ì„±ê³µ: {len(all_columns)}ê°œ")
#         return all_columns
#         
#     except Exception as e:
#         logger.error(f"âŒ static_indicators í…Œì´ë¸” ì»¬ëŸ¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
#         # ê¸°ë³¸ì ìœ¼ë¡œ í™•ì‹¤íˆ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ë§Œ ë°˜í™˜
#         return ['ticker', 'pivot', 'r1', 's1', 'support', 'resistance', 'atr', 'adx']

# def build_safe_static_indicators_query(ticker_param=True):
#     """static_indicators í…Œì´ë¸”ìš© ì•ˆì „í•œ SELECT ì¿¼ë¦¬ ìƒì„±"""
#     safe_columns = get_safe_static_indicators_columns()
#     
#     # í•„ìˆ˜ ì»¬ëŸ¼ (tickerëŠ” í•­ìƒ í¬í•¨)
#     essential_columns = ['ticker']
#     
#     # ì •ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ (ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì„ íƒ)
#     possible_static_indicators = [
#         'pivot', 'r1', 's1', 'support', 'resistance', 'atr', 'adx',
#         'ma200_slope', 'nvt_relative', 'volume_change_7_30', 'price',
#         'high_60', 'low_60', 'supertrend_signal'
#     ]
#     
#     # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì •ì  ì§€í‘œ ì»¬ëŸ¼ë§Œ í•„í„°ë§
#     safe_static_indicators = [col for col in possible_static_indicators if col in safe_columns]
#     
#     # ìµœì¢… ì»¬ëŸ¼ ëª©ë¡ êµ¬ì„±
#     all_safe_columns = essential_columns + safe_static_indicators
#     columns_str = ', '.join(all_safe_columns)
#     
#     if ticker_param:
#         query = f"""
#             SELECT {columns_str}
#             FROM static_indicators 
#             WHERE ticker = %s
#         """
#     else:
#         query = f"""
#             SELECT {columns_str}
#             FROM static_indicators
#         """
#     
#     logger.info(f"âœ… ì•ˆì „í•œ static_indicators ì¿¼ë¦¬ ìƒì„±: {len(all_safe_columns)}ê°œ ì»¬ëŸ¼ ì‚¬ìš©")
#     logger.debug(f"   - ì‚¬ìš©ëœ ì»¬ëŸ¼: {all_safe_columns}")
#     
#     return query, safe_static_indicators

def get_combined_ohlcv_and_static_data(ticker, limit_days=30):
    """
    ohlcvì™€ static_indicatorsë¥¼ ì•ˆì „í•˜ê²Œ ì¡°í•©í•˜ì—¬ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    - ohlcvì—ì„œ ë™ì  ì§€í‘œì™€ ê¸°ë³¸ OHLCV ë°ì´í„°
    - static_indicatorsì—ì„œ pivot, r1, s1 ë“± ì •ì  ì§€í‘œ
    """
    try:
        # 1. ohlcv ë°ì´í„° ì¡°íšŒ (ì•ˆì „í•œ ì¿¼ë¦¬ ì‚¬ìš©)
        ohlcv_query, _ = build_safe_ohlcv_query(ticker_param=True, limit_days=limit_days)
        
        # 2. static_indicators ë°ì´í„° ì¡°íšŒ (ì•ˆì „í•œ ì¿¼ë¦¬ ì‚¬ìš©) 
        static_query, static_columns = build_safe_static_indicators_query(ticker_param=True)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # 3. ohlcv ë°ì´í„° ì‹¤í–‰
        cursor.execute(ohlcv_query, (ticker,))
        ohlcv_rows = cursor.fetchall()
        
        # 4. static_indicators ë°ì´í„° ì‹¤í–‰
        cursor.execute(static_query, (ticker,))
        static_rows = cursor.fetchone()  # ë‹¨ì¼ í–‰ (ìµœì‹  ì •ì  ì§€í‘œ)
        
        cursor.close()
        conn.close()
        
        logger.info(f"âœ… {ticker} í†µí•© ë°ì´í„° ì¡°íšŒ ì„±ê³µ: OHLCV {len(ohlcv_rows)}í–‰, ì •ì ì§€í‘œ {len(static_columns)}ê°œ ì»¬ëŸ¼")
        
        return {
            'ohlcv_data': ohlcv_rows,
            'static_data': static_rows,
            'static_columns': static_columns
        }
        
    except Exception as e:
        logger.error(f"âŒ {ticker} í†µí•© ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            'ohlcv_data': [],
            'static_data': None,
            'static_columns': []
        }

# utils.pyì— ì¶”ê°€í•  ë²”ìš© íƒ€ì… ë³€í™˜ í•¨ìˆ˜

# === UNUSED: DB ì¿¼ë¦¬ ê²°ê³¼ ê²€ì¦ ë° íƒ€ì… ë³€í™˜ í†µê³„ í•¨ìˆ˜ë“¤ ===
# def validate_db_query_results(results, expected_types, context=""):
#     """
#     DB ì¿¼ë¦¬ ê²°ê³¼ì˜ íƒ€ì… ì¼ê´€ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.
#     
#     Args:
#         results: DB ì¿¼ë¦¬ ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸)
#         expected_types: ê¸°ëŒ€ë˜ëŠ” íƒ€ì…ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
#         context: ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
#         
#     Returns:
#         dict: ê²€ì¦ ê²°ê³¼ í†µê³„
#     """
#     validation_stats = {
#         "total_rows": 0,
#         "total_columns": 0,
#         "type_mismatches": 0,
#         "none_values": 0,
#         "problematic_columns": set()
#     }
#     
#     if not results:
#         logger.warning(f"âš ï¸ {context} DB ì¿¼ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
#         return validation_stats
#     
#     validation_stats["total_rows"] = len(results)
#     validation_stats["total_columns"] = len(expected_types) if expected_types else 0
#     
#     for row_idx, row in enumerate(results):
#         if len(row) != len(expected_types):
#             logger.warning(f"âš ï¸ {context} Row {row_idx}: ì»¬ëŸ¼ ìˆ˜ ë¶ˆì¼ì¹˜ (ì‹¤ì œ: {len(row)}, ê¸°ëŒ€: {len(expected_types)})")
#             continue
#             
#         for col_idx, (value, expected_type) in enumerate(zip(row, expected_types)):
#             if value is None:
#                 validation_stats["none_values"] += 1
#                 continue
#                 
#             # pandas ê°ì²´ë‚˜ datetime ê³„ì—´ íŠ¹ë³„ ì²˜ë¦¬
#             if expected_type in [float, int] and isinstance(value, (datetime, date)):
#                 validation_stats["type_mismatches"] += 1
#                 validation_stats["problematic_columns"].add(col_idx)
#                 logger.warning(f"âš ï¸ {context} Row {row_idx}, Col {col_idx}: datetime íƒ€ì…ì„ ìˆ«ìë¡œ ì‚¬ìš© ì‹œë„ - ê°’: {value}")
#                 
#             elif not isinstance(value, expected_type) and expected_type is not None:
#                 # íƒ€ì… ë³€í™˜ ê°€ëŠ¥ì„± ì²´í¬
#                 if expected_type == float:
#                     try:
#                         float(value)  # ë³€í™˜ ê°€ëŠ¥í•˜ë©´ ë¬¸ì œì—†ìŒ
#                     except (ValueError, TypeError):
#                         validation_stats["type_mismatches"] += 1
#                         validation_stats["problematic_columns"].add(col_idx)
#                         logger.warning(f"âš ï¸ {context} Row {row_idx}, Col {col_idx}: ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… - ê°’: {value} (íƒ€ì…: {type(value)}, ê¸°ëŒ€: {expected_type})")
#                 elif expected_type == int:
#                     try:
#                         int(float(value))  # floatë¥¼ ê±°ì³ int ë³€í™˜ ì‹œë„
#                     except (ValueError, TypeError):
#                         validation_stats["type_mismatches"] += 1
#                         validation_stats["problematic_columns"].add(col_idx)
#                         logger.warning(f"âš ï¸ {context} Row {row_idx}, Col {col_idx}: ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… - ê°’: {value} (íƒ€ì…: {type(value)}, ê¸°ëŒ€: {expected_type})")
#                 else:
#                     validation_stats["type_mismatches"] += 1
#                     validation_stats["problematic_columns"].add(col_idx)
#                     logger.warning(f"âš ï¸ {context} Row {row_idx}, Col {col_idx}: ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… - ê°’: {value} (íƒ€ì…: {type(value)}, ê¸°ëŒ€: {expected_type})")
#     
#     # ê²€ì¦ ê²°ê³¼ ìš”ì•½ ë¡œê·¸
#     if validation_stats["type_mismatches"] > 0:
#         logger.warning(f"âš ï¸ {context} íƒ€ì… ê²€ì¦ ì™„ë£Œ: {validation_stats['type_mismatches']}ê°œ ë¶ˆì¼ì¹˜ ë°œê²¬")
#     else:
#         logger.info(f"âœ… {context} íƒ€ì… ê²€ì¦ í†µê³¼: ëª¨ë“  ë°ì´í„° íƒ€ì… ì¼ì¹˜")
#     
#     return validation_stats

# íƒ€ì… ë³€í™˜ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ê¸€ë¡œë²Œ ì¹´ìš´í„°
_conversion_stats = {
    "total_calls": 0,
    "successful_conversions": 0,
    "datetime_detections": 0,
    "conversion_failures": 0,
    "start_time": datetime.now()
}

# def get_conversion_stats():
#     """íƒ€ì… ë³€í™˜ í†µê³„ ì¡°íšŒ"""
#     runtime = datetime.now() - _conversion_stats["start_time"]
#     return {
#         **_conversion_stats,
#         "runtime_seconds": runtime.total_seconds(),
#         "success_rate": (_conversion_stats["successful_conversions"] / _conversion_stats["total_calls"] * 100) if _conversion_stats["total_calls"] > 0 else 0,
#         "datetime_detection_rate": (_conversion_stats["datetime_detections"] / _conversion_stats["total_calls"] * 100) if _conversion_stats["total_calls"] > 0 else 0
#     }

# def reset_conversion_stats():
#     """íƒ€ì… ë³€í™˜ í†µê³„ ì´ˆê¸°í™”"""
#     global _conversion_stats
#     _conversion_stats = {
#         "total_calls": 0,
#         "successful_conversions": 0,
#         "datetime_detections": 0,
#         "conversion_failures": 0,
#         "start_time": datetime.now()
#     }

def safe_float_convert(value, default=0.0, context=""):
    """
    ëª¨ë“  íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ì¶”ê°€)
    
    Args:
        value: ë³€í™˜í•  ê°’
        default (float): ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        context (str): ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        float: ë³€í™˜ëœ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
    """
    global _conversion_stats
    _conversion_stats["total_calls"] += 1
    
    if value is None:
        _conversion_stats["successful_conversions"] += 1
        return default
        
    # datetime ê³„ì—´ ê°ì²´ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    if isinstance(value, (datetime, date)):
        _conversion_stats["datetime_detections"] += 1
        logger.warning(f"âš ï¸ {context} datetime ê°ì²´ë¥¼ floatë¡œ ë³€í™˜ ì‹œë„: {value} -> {default}")
        return default
        
    # pandas Timestampë„ ì²´í¬
    if hasattr(value, '__class__') and 'pandas' in str(type(value)):
        _conversion_stats["datetime_detections"] += 1
        logger.warning(f"âš ï¸ {context} pandas ê°ì²´ë¥¼ floatë¡œ ë³€í™˜ ì‹œë„: {value} -> {default}")
        return default
        
    try:
        result = float(value)
        _conversion_stats["successful_conversions"] += 1
        return result
    except (ValueError, TypeError) as e:
        _conversion_stats["conversion_failures"] += 1
        logger.warning(f"âš ï¸ {context} float ë³€í™˜ ì‹¤íŒ¨: {value} (íƒ€ì…: {type(value)}) -> {default}")
        return default

# === UNUSED: íƒ€ì… ë³€í™˜ í†µê³„ ìš”ì•½ í•¨ìˆ˜ ===
# def log_conversion_stats_summary():
#     """íƒ€ì… ë³€í™˜ í†µê³„ ìš”ì•½ ë¡œê·¸ ì¶œë ¥"""
#     stats = get_conversion_stats()
#     logger.info("=" * 50)
#     logger.info("ğŸ“Š íƒ€ì… ë³€í™˜ ëª¨ë‹ˆí„°ë§ ìš”ì•½")
#     logger.info("=" * 50)
#     logger.info(f"ì´ ë³€í™˜ ì‹œë„: {stats['total_calls']}íšŒ")
#     logger.info(f"ì„±ê³µì  ë³€í™˜: {stats['successful_conversions']}íšŒ ({stats['success_rate']:.1f}%)")
#     logger.info(f"datetime ê°ì§€: {stats['datetime_detections']}íšŒ ({stats['datetime_detection_rate']:.1f}%)")
#     logger.info(f"ë³€í™˜ ì‹¤íŒ¨: {stats['conversion_failures']}íšŒ")
#     logger.info(f"ì‹¤í–‰ ì‹œê°„: {stats['runtime_seconds']:.1f}ì´ˆ")
#     
#     if stats['datetime_detections'] > 0:
#         logger.warning(f"âš ï¸ datetime íƒ€ì…ì´ {stats['datetime_detections']}íšŒ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
#     
#     return stats

def safe_int_convert(value, default=0, context=""):
    """
    ëª¨ë“  íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        value: ë³€í™˜í•  ê°’
        default (int): ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        context (str): ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        int: ë³€í™˜ëœ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
    """
    if value is None:
        return default
        
    # datetime ê³„ì—´ ê°ì²´ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    if isinstance(value, (datetime, date)):
        logger.warning(f"âš ï¸ {context} datetime ê°ì²´ë¥¼ intë¡œ ë³€í™˜ ì‹œë„: {value} -> {default}")
        return default
        
    try:
        return int(float(value))  # floatë¥¼ ê±°ì³ì„œ intë¡œ ë³€í™˜
    except (ValueError, TypeError) as e:
        logger.warning(f"âš ï¸ {context} int ë³€í™˜ ì‹¤íŒ¨: {value} (íƒ€ì…: {type(value)}) -> {default}")
        return default

# === UNUSED: TypeConverter í´ë˜ìŠ¤ ë° ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# # ===== ê°„ì†Œí™”ëœ íƒ€ì… ë³€í™˜ê¸° (refactored_type_converter.py í†µí•©) =====

# class TypeConverter:
#     """ê°„ì†Œí™”ëœ íƒ€ì… ë³€í™˜ê¸° í´ë˜ìŠ¤ - 50ë¼ì¸ ì´í•˜ë¡œ ì••ì¶•"""
#     
#     @staticmethod
#     def safe_convert(value, target_type, context=""):
#         """
#         datetime ê°ì§€ + íƒ€ì… ë³€í™˜ + JSON í˜¸í™˜ì„±ì„ í•œ ë²ˆì— ì²˜ë¦¬
#         
#         Args:
#             value: ë³€í™˜í•  ê°’
#             target_type: ëª©í‘œ íƒ€ì… ('float', 'int', 'date', 'str')
#             context: ë³€í™˜ ì»¨í…ìŠ¤íŠ¸ (ë¡œê¹…ìš©)
#             
#         Returns:
#             ë³€í™˜ëœ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
#         """
#         if value is None:
#             return None
#             
#         try:
#             # datetime ê°ì§€ + ì²˜ë¦¬
#             if isinstance(value, (datetime, date, pd.Timestamp)):
#                 if target_type == 'date':
#                     return value.strftime('%Y-%m-%d')
#                 else:
#                     logger.warning(f"datetime in {target_type} column: {context}")
#                     return 0.0
#             
#             # ê°„ë‹¨í•œ íƒ€ì… ë³€í™˜
#             if target_type == 'float':
#                 converted = float(value)
#                 return converted.item() if hasattr(converted, 'item') else converted
#             elif target_type == 'int':
#                 converted = int(float(value))
#                 return converted.item() if hasattr(converted, 'item') else converted
#             elif target_type == 'date':
#                 return safe_strftime(value, '%Y-%m-%d')
#             else:
#                 return str(value)
#                 
#         except Exception as e:
#             logger.debug(f"ë³€í™˜ ì‹¤íŒ¨ {context}: {e}")
#             return {'float': 0.0, 'int': 0, 'date': '1970-01-01', 'str': 'N/A'}.get(target_type, 'N/A')

# def safe_ohlcv_column_convert(value, column_name, ticker, expected_type='float'):
#     """
#     ê°„ì†Œí™”ëœ OHLCV ì»¬ëŸ¼ ë³€í™˜ í•¨ìˆ˜ (ê¸°ì¡´ 200+ ë¼ì¸ì„ 50ë¼ì¸ ì´í•˜ë¡œ ì••ì¶•)
#     """
#     converter = TypeConverter()
#     context = f"{ticker}.{column_name}"
#     
#     # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
#     try:
#         monitor = get_optimized_monitor()
#         monitor.log_conversion_failure(ticker, column_name, value, "conversion_attempt", "attempting")
#     except:
#         pass  # ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨í•´ë„ ë³€í™˜ì€ ê³„ì†
#     
#     return converter.safe_convert(value, expected_type, context)

# JSON ì§ë ¬í™” ì•ˆì „ì„± ê°•í™” í•¨ìˆ˜ë“¤

# === UNUSED: JSON ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# def validate_json_compatibility(data, context=""):
#     """
#     JSON ì§ë ¬í™” í˜¸í™˜ì„±ì„ ê²€ì¦í•˜ê³  ë¬¸ì œê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
#     
#     Args:
#         data: ê²€ì¦í•  ë°ì´í„° (dict, list, ê¸°ë³¸ íƒ€ì…)
#         context: ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
#         
#     Returns:
#         dict: ê²€ì¦ ê²°ê³¼ {'valid': bool, 'issues': list, 'corrected_data': any}
#     """
#     import json
#     import numpy as np
#     
#     issues = []
#     corrected_data = data
#     
#     def check_and_fix_value(value, path=""):
#         """ì¬ê·€ì ìœ¼ë¡œ ê°’ì„ ê²€ì‚¬í•˜ê³  ìˆ˜ì •"""
#         # Noneì€ JSON í˜¸í™˜
#         if value is None:
#             return value
#             
#         # ê¸°ë³¸ JSON í˜¸í™˜ íƒ€ì…ë“¤
#         if isinstance(value, (str, int, float, bool)):
#             # numpy íƒ€ì…ì„ ë„¤ì´í‹°ë¸Œ íŒŒì´ì¬ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
#             if hasattr(value, 'item'):
#                 return value.item()
#             return value
#             
#         # datetime ê´€ë ¨ ê°ì²´ë“¤
#         if isinstance(value, (datetime, date)):
#             if hasattr(value, 'strftime'):
#                 converted = value.strftime('%Y-%m-%d')
#                 converted = value.strftime('%Y-%m-%d')
#                 issues.append(f"{path}: datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ ({type(value).__name__} â†’ str)")
#                 return converted
#             else:
#                 converted = str(value)
#                 issues.append(f"{path}: datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (fallback)")
#                 return converted
#                 
#         # pandas/numpy ê°ì²´ë“¤
#         if hasattr(value, '__class__'):
#             class_str = str(type(value))
#             if any(lib in class_str for lib in ['pandas', 'numpy']):
#                 if hasattr(value, 'item'):  # numpy scalar
#                     converted = value.item()
#                     issues.append(f"{path}: numpy scalarì„ ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜")
#                     return converted
#                 elif hasattr(value, 'strftime'):  # pandas Timestamp
#                     converted = value.strftime('%Y-%m-%d')
#                     issues.append(f"{path}: pandas Timestampë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜")
#                     return converted
#                 else:
#                     converted = str(value)
#                     issues.append(f"{path}: pandas/numpy ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜")
#                     return converted
#                     
#         # ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
#         if isinstance(value, dict):
#             fixed_dict = {}
#             for k, v in value.items():
#                 new_path = f"{path}.{k}" if path else k
#                 fixed_dict[k] = check_and_fix_value(v, new_path)
#             return fixed_dict
#             
#         # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
#         if isinstance(value, (list, tuple)):
#             fixed_list = []
#             for i, item in enumerate(value):
#                 new_path = f"{path}[{i}]" if path else f"[{i}]"
#                 fixed_list.append(check_and_fix_value(item, new_path))
#             return fixed_list
#             
#         # ê¸°íƒ€ íƒ€ì…ë“¤ - ë¬¸ìì—´ë¡œ ë³€í™˜
#         converted = str(value)
#         issues.append(f"{path}: ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… {type(value).__name__}ì„ ë¬¸ìì—´ë¡œ ë³€í™˜")
#         return converted
#     
#     try:
#         corrected_data = check_and_fix_value(data, context)
#         
#         # ìµœì¢… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
#         json.dumps(corrected_data, ensure_ascii=False)
#         
#         if issues:
#             logger.debug(f"ğŸ”§ {context}: JSON í˜¸í™˜ì„± ë¬¸ì œ {len(issues)}ê°œ ìˆ˜ì •ë¨")
#             for issue in issues[:5]:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
#                 logger.debug(f"  - {issue}")
#         else:
#             logger.debug(f"âœ… {context}: JSON í˜¸í™˜ì„± ê²€ì¦ í†µê³¼")
#             
#         return {
#             'valid': True,
#             'issues': issues,
#             'corrected_data': corrected_data
#         }
#         
#     except Exception as e:
#         logger.error(f"âŒ {context}: JSON í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨ - {e}")
#         return {
#             'valid': False,
#             'issues': issues + [f"JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}"],
#             'corrected_data': corrected_data
#         }

# def safe_json_structure_with_fallback(data_dict, context=""):
#     """
#     ì•ˆì „í•œ JSON êµ¬ì¡°í™” - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ìµœëŒ€í•œ ë°ì´í„° ë³´ì¡´
#     
#     Args:
#         data_dict: êµ¬ì¡°í™”í•  ë”•ì…”ë„ˆë¦¬
#         context: ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
#         
#     Returns:
#         dict: ì•ˆì „í•˜ê²Œ êµ¬ì¡°í™”ëœ ë°ì´í„°
#     """
#     safe_result = {}
#     issues = []
#     
#     for key, value in data_dict.items():
#         try:
#         # ê°œë³„ í•„ë“œë³„ JSON í˜¸í™˜ì„± ê²€ì¦
#         validation_result = validate_json_compatibility(value, f"{context}.{key}")
#         
#         if validation_result['valid']:
#         safe_result[key] = validation_result['corrected_data']
#         if validation_result['issues']:
#         issues.extend(validation_result['issues'])
#         else:
#         # ì‹¤íŒ¨í•œ í•„ë“œëŠ” ì•ˆì „í•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
#         if isinstance(value, (dict, list)):
#         safe_result[key] = {} if isinstance(value, dict) else []
#         elif isinstance(value, (int, float)):
#         safe_result[key] = 0
#         else:
#         safe_result[key] = "Error"
#         
#         issues.append(f"{key}: êµ¬ì¡°í™” ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©")
#         logger.warning(f"âš ï¸ {context}.{key}: JSON êµ¬ì¡°í™” ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
#         
#         except Exception as field_error:
#         # ê°œë³„ í•„ë“œ ì²˜ë¦¬ ì‹¤íŒ¨
#         safe_result[key] = "ProcessingError"
#         issues.append(f"{key}: ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ - {field_error}")
#         logger.warning(f"âš ï¸ {context}.{key}: ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ - {field_error}")
#     
#     # ìµœì¢… JSON ì§ë ¬í™” ê²€ì¦
#     try:
#         import json
#         json.dumps(safe_result, ensure_ascii=False)
#         logger.debug(f"âœ… {context}: ì•ˆì „í•œ JSON êµ¬ì¡°í™” ì™„ë£Œ ({len(issues)}ê°œ ì´ìŠˆ í•´ê²°)")
#     except Exception as final_error:
#         logger.error(f"âŒ {context}: ìµœì¢… JSON ê²€ì¦ ì‹¤íŒ¨ - {final_error}")
#         # ê·¹ë‹¨ì ì¸ ê²½ìš°ë¥¼ ìœ„í•œ ìµœì†Œí•œì˜ ì•ˆì „í•œ êµ¬ì¡°
#         safe_result = {
#         "error": "JSON êµ¬ì¡°í™” ì‹¤íŒ¨",
#         "original_keys": list(data_dict.keys()),
#         "timestamp": safe_strftime(datetime.now())
#         }
#     
#     return safe_result

# def enhanced_safe_strftime_for_json(date_obj, format_str='%Y-%m-%d'):
#     """
#     JSON ì§ë ¬í™”ì— ìµœì í™”ëœ safe_strftime - ëª¨ë“  ë‚ ì§œ í•„ë“œì— ì ìš©
#     
#     Args:
#         date_obj: ë³€í™˜í•  ë‚ ì§œ ê°ì²´
#         format_str: ë‚ ì§œ í¬ë§· (ê¸°ë³¸: ISO ë‚ ì§œ)
#         
#     Returns:
#         str: JSON í˜¸í™˜ ë‚ ì§œ ë¬¸ìì—´
#     """
#     try:
#         # ê¸°ì¡´ safe_strftime ì‚¬ìš©
#         result = safe_strftime(date_obj, format_str)
#         
#         # ì¶”ê°€ ê²€ì¦: ê²°ê³¼ê°€ ìœ íš¨í•œ JSON ë¬¸ìì—´ì¸ì§€ í™•ì¸
#         if not isinstance(result, str):
#             logger.warning(f"âš ï¸ safe_strftime ê²°ê³¼ê°€ ë¬¸ìì—´ì´ ì•„ë‹˜: {type(result)} - {result}")
#             return "1970-01-01"
#             
#         # ISO í˜•ì‹ ê²€ì¦ (ê¸°ë³¸ í¬ë§·ì¸ ê²½ìš°)
#         if format_str == '%Y-%m-%d' and len(result) >= 10:
#             # YYYY-MM-DD í˜•ì‹ì¸ì§€ ê°„ë‹¨íˆ ê²€ì¦
#             if result[4] == '-' and result[7] == '-':
#                 return result
#             else:
#                 logger.warning(f"âš ï¸ ë‚ ì§œ í˜•ì‹ì´ ë¹„ì •ìƒì : {result}")
#                 return "1970-01-01"
#         
#         return result
#         
#     except Exception as e:
#         logger.error(f"âŒ enhanced_safe_strftime_for_json ì‹¤íŒ¨: {e}")
#         return "1970-01-01"

# def validate_dataframe_index_for_json(df):
#     """
#     DataFrame indexì˜ JSON í˜¸í™˜ì„±ì„ ê²€ì¦í•˜ê³  ìˆ˜ì •
#     
#     Args:
#         df: pandas DataFrame
#         
#     Returns:
#         pandas.DataFrame: JSON í˜¸í™˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ DataFrame
#     """
#     if df is None or df.empty:
#         return df
#         
#     try:
#         # ì¸ë±ìŠ¤ê°€ datetime íƒ€ì…ì¸ì§€ í™•ì¸
#         if hasattr(df.index, 'dtype') and 'datetime' in str(df.index.dtype):
#             logger.debug("ğŸ”§ DataFrame ì¸ë±ìŠ¤ë¥¼ JSON í˜¸í™˜ ë¬¸ìì—´ë¡œ ë³€í™˜")
#             # ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
#             df.index = df.index.map(lambda x: enhanced_safe_strftime_for_json(x))
#             
#         # ì¸ë±ìŠ¤ì— pandas Timestampê°€ ìˆëŠ”ì§€ í™•ì¸
#         elif hasattr(df.index, '__iter__'):
#             index_needs_conversion = False
#             for idx_val in df.index[:5]:  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸
#                 if hasattr(idx_val, '__class__') and 'pandas' in str(type(idx_val)):
#                     index_needs_conversion = True
#                     break
#                     
#             if index_needs_conversion:
#                 logger.debug("ğŸ”§ DataFrame ì¸ë±ìŠ¤ì˜ pandas ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜")
#                 df.index = df.index.map(lambda x: enhanced_safe_strftime_for_json(x) if hasattr(x, 'strftime') else str(x))
#         
#         return df
#         
#     except Exception as e:
#         logger.error(f"âŒ DataFrame ì¸ë±ìŠ¤ JSON í˜¸í™˜ì„± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
#         return df

# ë¡œê¹… ê°œì„  ë° ë””ë²„ê¹… ê°•í™” í•¨ìˆ˜ë“¤

# === UNUSED: DataProcessingMonitor í´ë˜ìŠ¤ ===
# class DataProcessingMonitor:
#     """ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤"""
#     
#     def __init__(self):
#         self.stats = {
#             'start_time': datetime.now(),
#             'tickers_processed': 0,
#             'conversion_attempts': 0,
#             'conversion_successes': 0,
#             'conversion_failures': 0,
#             'datetime_detections': 0,
#             'json_serialization_attempts': 0,
#             'json_serialization_successes': 0,
#             'db_query_attempts': 0,
#             'db_query_successes': 0,
#             'total_processing_time': 0.0,
#             'detailed_issues': []
#         }
#     
#     def log_ticker_start(self, ticker):
#         """í‹°ì»¤ ì²˜ë¦¬ ì‹œì‘ ë¡œê·¸"""
#         self.stats['tickers_processed'] += 1
#         logger.info(f"ğŸ“Š [{self.stats['tickers_processed']}] {ticker} ì²˜ë¦¬ ì‹œì‘")
#     
#     def log_conversion_attempt(self, ticker, column, value_type, expected_type):
#         """ë³€í™˜ ì‹œë„ ë¡œê·¸"""
#         self.stats['conversion_attempts'] += 1
#         logger.debug(f"ğŸ”„ {ticker}.{column}: {value_type} â†’ {expected_type} ë³€í™˜ ì‹œë„ (#{self.stats['conversion_attempts']})")
#     
#     def log_conversion_success(self, ticker, column, original, converted):
#         """ë³€í™˜ ì„±ê³µ ë¡œê·¸"""
#         self.stats['conversion_successes'] += 1
#         logger.debug(f"âœ… {ticker}.{column}: ë³€í™˜ ì„±ê³µ - {type(original).__name__}({original}) â†’ {type(converted).__name__}({converted})")
#     
#     def log_conversion_failure(self, ticker, column, original, error, fallback_used):
#         """ë³€í™˜ ì‹¤íŒ¨ ë¡œê·¸"""
#         self.stats['conversion_failures'] += 1
#         issue_detail = {
#             'ticker': ticker,
#             'column': column,
#             'original_value': str(original),
#             'original_type': type(original).__name__,
#             'error': str(error),
#             'fallback_used': fallback_used,
#             'timestamp': datetime.now().isoformat()
#         }
#         self.stats['detailed_issues'].append(issue_detail)
#         logger.warning(f"âš ï¸ {ticker}.{column}: ë³€í™˜ ì‹¤íŒ¨ - {type(original).__name__}({original}) ì˜¤ë¥˜: {error}, ëŒ€ì²´ê°’: {fallback_used}")
#     
#     def log_datetime_detection(self, ticker, column, value, detection_method):
#         """datetime ê°ì§€ ë¡œê·¸"""
#         self.stats['datetime_detections'] += 1
#         logger.debug(f"ğŸ•’ {ticker}.{column}: datetime ê°ì§€ - {detection_method}, ê°’: {value}")
#     
#     def log_json_attempt(self, ticker, data_size):
#         """JSON ì§ë ¬í™” ì‹œë„ ë¡œê·¸"""
#         self.stats['json_serialization_attempts'] += 1
#         logger.debug(f"ğŸ“ {ticker}: JSON ì§ë ¬í™” ì‹œë„ (ë°ì´í„° í¬ê¸°: {data_size} bytes)")
#     
#     def log_json_success(self, ticker, json_size):
#         """JSON ì§ë ¬í™” ì„±ê³µ ë¡œê·¸"""
#         self.stats['json_serialization_successes'] += 1
#         logger.debug(f"âœ… {ticker}: JSON ì§ë ¬í™” ì„±ê³µ (ê²°ê³¼ í¬ê¸°: {json_size} bytes)")
#     
#     def log_db_query_attempt(self, ticker, expected_columns):
#         """DB ì¿¼ë¦¬ ì‹œë„ ë¡œê·¸"""
#         self.stats['db_query_attempts'] += 1
#         logger.debug(f"ğŸ—„ï¸ {ticker}: DB ì¿¼ë¦¬ ì‹œë„ (ì˜ˆìƒ ì»¬ëŸ¼: {expected_columns}ê°œ)")
#     
#     def log_db_query_success(self, ticker, actual_columns, row_count):
#         """DB ì¿¼ë¦¬ ì„±ê³µ ë¡œê·¸"""
#         self.stats['db_query_successes'] += 1
#         logger.debug(f"âœ… {ticker}: DB ì¿¼ë¦¬ ì„±ê³µ (ì‹¤ì œ ì»¬ëŸ¼: {actual_columns}ê°œ, í–‰: {row_count}ê°œ)")
#     
#     def log_processing_time(self, ticker, duration):
#         """ì²˜ë¦¬ ì‹œê°„ ë¡œê·¸"""
#         self.stats['total_processing_time'] += duration
#         logger.debug(f"â±ï¸ {ticker}: ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.3f}ì´ˆ)")
#     
#     def get_summary_report(self):
#         """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
#         runtime = datetime.now() - self.stats['start_time']
#         conversion_success_rate = (self.stats['conversion_successes'] / self.stats['conversion_attempts'] * 100) if self.stats['conversion_attempts'] > 0 else 0
#         json_success_rate = (self.stats['json_serialization_successes'] / self.stats['json_serialization_attempts'] * 100) if self.stats['json_serialization_attempts'] > 0 else 0
#         db_success_rate = (self.stats['db_query_successes'] / self.stats['db_query_attempts'] * 100) if self.stats['db_query_attempts'] > 0 else 0
#         
#         return {
#             'runtime_seconds': runtime.total_seconds(),
#             'tickers_processed': self.stats['tickers_processed'],
#             'conversion_success_rate': f"{conversion_success_rate:.1f}%",
#             'json_success_rate': f"{json_success_rate:.1f}%",
#             'db_success_rate': f"{db_success_rate:.1f}%",
#             'datetime_detections': self.stats['datetime_detections'],
#             'total_issues': len(self.stats['detailed_issues']),
#             'avg_processing_time_per_ticker': self.stats['total_processing_time'] / max(self.stats['tickers_processed'], 1)
#         }
#     
#     def log_summary(self):
#         """ìš”ì•½ ë¡œê·¸ ì¶œë ¥"""
#         summary = self.get_summary_report()
#         logger.info("=" * 50)
#         logger.info("ğŸ“Š ë°ì´í„° ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ìš”ì•½ ë¦¬í¬íŠ¸")
#         logger.info("=" * 50)
#         logger.info(f"â±ï¸ ì´ ì‹¤í–‰ì‹œê°„: {summary['runtime_seconds']:.1f}ì´ˆ")
#         logger.info(f"ğŸ“ˆ ì²˜ë¦¬ëœ í‹°ì»¤: {summary['tickers_processed']}ê°œ")
#         logger.info(f"ğŸ”„ íƒ€ì… ë³€í™˜ ì„±ê³µë¥ : {summary['conversion_success_rate']}")
#         logger.info(f"ğŸ“ JSON ì§ë ¬í™” ì„±ê³µë¥ : {summary['json_success_rate']}")
#         logger.info(f"ğŸ—„ï¸ DB ì¿¼ë¦¬ ì„±ê³µë¥ : {summary['db_success_rate']}")
#         logger.info(f"ğŸ•’ datetime ê°ì§€ íšŸìˆ˜: {summary['datetime_detections']}íšŒ")
#         logger.info(f"âš ï¸ ì´ ì´ìŠˆ ë°œìƒ: {summary['total_issues']}ê±´")
#         logger.info(f"âš¡ í‹°ì»¤ë‹¹ í‰ê·  ì²˜ë¦¬ì‹œê°„: {summary['avg_processing_time_per_ticker']:.3f}ì´ˆ")
#         
#         # ìƒìœ„ ë¬¸ì œì ë“¤ ë¦¬í¬íŠ¸
#         if self.stats['detailed_issues']:
#             issue_types = {}
#             for issue in self.stats['detailed_issues']:
#                 issue_key = f"{issue['column']}_{issue['original_type']}"
#                 issue_types[issue_key] = issue_types.get(issue_key, 0) + 1
#             
#             logger.info("ğŸ” ì£¼ìš” ë¬¸ì œì  TOP 5:")
#             for i, (issue_type, count) in enumerate(sorted(issue_types.items(), key=lambda x: x[1], reverse=True)[:5], 1):
#                 logger.info(f"  {i}. {issue_type}: {count}íšŒ")

# === UNUSED: ë¡œê¹… ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# def log_db_query_structure(query_result, context="", expected_columns=None):
#     """
#     DB ì¿¼ë¦¬ ê²°ê³¼ì˜ êµ¬ì¡°ë¥¼ ìƒì„¸íˆ ë¡œê¹…
#     
#     Args:
#         query_result: DB ì¿¼ë¦¬ ê²°ê³¼ (list of tuples)
#         context: ë¡œê¹… ì»¨í…ìŠ¤íŠ¸
#         expected_columns: ì˜ˆìƒ ì»¬ëŸ¼ ìˆ˜
#     """
#     if not query_result:
#         logger.warning(f"ğŸ” {context}: ë¹ˆ ì¿¼ë¦¬ ê²°ê³¼")
#         return
#     
#     sample_row = query_result[0]
#     actual_columns = len(sample_row)
#     
#     logger.debug(f"ğŸ” {context}: ì¿¼ë¦¬ ê²°ê³¼ êµ¬ì¡° ë¶„ì„")
#     logger.debug(f"  - ì´ í–‰ ìˆ˜: {len(query_result)}")
#     logger.debug(f"  - ì»¬ëŸ¼ ìˆ˜: {actual_columns}")
#     
#     if expected_columns and actual_columns != expected_columns:
#         logger.warning(f"âš ï¸ {context}: ì»¬ëŸ¼ ìˆ˜ ë¶ˆì¼ì¹˜ (ì˜ˆìƒ: {expected_columns}, ì‹¤ì œ: {actual_columns})")
#     
#     # ì²« ë²ˆì§¸ í–‰ì˜ ê° ì»¬ëŸ¼ íƒ€ì… ë¶„ì„
#     logger.debug(f"  - ì²« ë²ˆì§¸ í–‰ íƒ€ì… ë¶„ì„:")
#     for i, value in enumerate(sample_row):
#         value_type = type(value).__name__
#         value_preview = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
#         logger.debug(f"    [{i}] {value_type}: {value_preview}")
#         
#         # datetime ê°ì²´ ê°ì§€ ê²½ê³ 
#         if isinstance(value, (datetime, date)):
#             logger.warning(f"âš ï¸ {context}: ì»¬ëŸ¼ {i}ì—ì„œ datetime ê°ì²´ ê°ì§€ - {value}")

# def enhanced_conversion_logging(original_value, converted_value, conversion_type, context=""):
#     """
#     ìƒì„¸í•œ ë³€í™˜ ë¡œê¹…
#     
#     Args:
#         original_value: ì›ë³¸ ê°’
#         converted_value: ë³€í™˜ëœ ê°’
#         conversion_type: ë³€í™˜ íƒ€ì…
#         context: ì»¨í…ìŠ¤íŠ¸
#     """
#     logger.debug(f"ğŸ”„ {context}: {conversion_type} ë³€í™˜ ìƒì„¸")
#     logger.debug(f"  - ì›ë³¸: {type(original_value).__name__}({original_value})")
#     logger.debug(f"  - ê²°ê³¼: {type(converted_value).__name__}({converted_value})")
#     
#     # íŠ¹ë³„í•œ ê²½ìš°ë“¤ ì²´í¬
#     if hasattr(original_value, '__class__'):
#         class_str = str(type(original_value))
#         if 'pandas' in class_str:
#             logger.debug(f"  - pandas ê°ì²´ ê°ì§€: {class_str}")
#         elif 'numpy' in class_str:
#             logger.debug(f"  - numpy ê°ì²´ ê°ì§€: {class_str}")
#     
#     # ë³€í™˜ ê³¼ì •ì—ì„œ ì •ë³´ ì†ì‹¤ ì²´í¬
#     if conversion_type == 'float' and isinstance(original_value, str):
#         try:
#             if '.' in original_value and len(original_value.split('.')[1]) > 6:
#                 logger.debug(f"  - ì •ë°€ë„ ì†ì‹¤ ê°€ëŠ¥: ì›ë³¸ ì†Œìˆ˜ì  {len(original_value.split('.')[1])}ìë¦¬")
#         except:
#             pass

# === UNUSED: ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ê´€ë ¨ í•¨ìˆ˜ë“¤ ===
# # ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
# _global_monitor = DataProcessingMonitor()

# def get_global_monitor():
#     """ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
#     return _global_monitor

# def reset_global_monitor():
    """ê¸€ë¡œë²Œ ëª¨ë‹ˆí„° ì´ˆê¸°í™”"""
    global _global_monitor
    _global_monitor = DataProcessingMonitor()

# 5ë‹¨ê³„: ì¼ê´€ì„± í™•ë³´ ë° í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ë“¤

# === UNUSED: ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜ ===
# def unified_date_conversion_for_json(date_obj, ticker="", position=0, total_length=1):
#     """
#     ê°„ë‹¨í•œ ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜
#     JSON ì§ë ¬í™”ì— ìµœì í™”ë¨
#     
#     Args:
#         date_obj: ë³€í™˜í•  ë‚ ì§œ ê°ì²´
#         ticker: í‹°ì»¤ëª… (ë¡œê¹…ìš©)
#         position: DataFrameì—ì„œì˜ ìœ„ì¹˜ (fallback ê³„ì‚°ìš©)
#         total_length: DataFrame ì „ì²´ ê¸¸ì´ (fallback ê³„ì‚°ìš©)
#         
#     Returns:
#         str: JSON í˜¸í™˜ ë‚ ì§œ ë¬¸ìì—´ ë˜ëŠ” None
#     """
#     try:
#         # 1ìˆœìœ„: Pandas Timestamp ì§ì ‘ ë³€í™˜ (data_fetcherì™€ ë™ì¼)
#         if isinstance(date_obj, pd.Timestamp) and hasattr(date_obj, 'year') and date_obj.year > 1990:
#             result = date_obj.strftime('%Y-%m-%d')
#             logger.debug(f"âœ… {ticker}: Pandas Timestamp ë³€í™˜ ì„±ê³µ - {result}")
#             return result
#         
#         # 2ìˆœìœ„: datetime/date ê°ì²´ ë³€í™˜ (ê°•í™”ë¨)
#         if isinstance(date_obj, (datetime, date)) and hasattr(date_obj, 'year') and date_obj.year > 1990:
#             result = date_obj.strftime('%Y-%m-%d')
#             logger.debug(f"âœ… {ticker}: datetime ê°ì²´ ë³€í™˜ ì„±ê³µ - {result}")
#             return result
#         
#         # 3ìˆœìœ„: date ì†ì„±ì´ ìˆëŠ” ê°ì²´
#         if hasattr(date_obj, 'date') and hasattr(date_obj.date(), 'year') and date_obj.date().year > 1990:
#             result = date_obj.date().strftime('%Y-%m-%d')
#             logger.debug(f"âœ… {ticker}: date ì†ì„± ë³€í™˜ ì„±ê³µ - {result}")
#             return result
#         
#         # 4ìˆœìœ„: ë¬¸ìì—´ íŒŒì‹± ì‹œë„ (data_fetcherì™€ ë™ì¼)
#         if isinstance(date_obj, str) and date_obj not in ["N/A", "Invalid Date", ""]:
#             try:
#                 parsed_date = pd.to_datetime(date_obj)
#                 if hasattr(parsed_date, 'year') and parsed_date.year > 1990:
#                     result = parsed_date.strftime('%Y-%m-%d')
#                     logger.debug(f"âœ… {ticker}: ë¬¸ìì—´ íŒŒì‹± ì„±ê³µ - {result}")
#                     return result
#             except:
#                 pass
#         
#         # 5ìˆœìœ„: fallback ë©”ì»¤ë‹ˆì¦˜ (data_fetcherì™€ ë™ì¼í•˜ì§€ë§Œ ë” ì•ˆì „í•˜ê²Œ)
#         if total_length > 1 and 0 <= position < total_length:
#             days_offset = total_length - position - 1
#             estimated_date = (datetime.now() - timedelta(days=days_offset)).date()
#             
#             # ì¶”ì • ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦ (10ë…„ ì´ë‚´ë§Œ í—ˆìš©)
#             if (datetime.now().date() - estimated_date).days <= 3650:
#                 result = estimated_date.strftime('%Y-%m-%d')
#                 logger.warning(f"âš ï¸ {ticker}: ë‚ ì§œ ì¶”ì • ì ìš© - ìœ„ì¹˜ {position} â†’ {result}")
#                 return result
#         
#         # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ
#         logger.error(f"âŒ {ticker}: ë‚ ì§œ ë³€í™˜ ì™„ì „ ì‹¤íŒ¨ - ì›ë³¸: {date_obj} (íƒ€ì…: {type(date_obj)})")
#         return None
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker}: ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜ˆì™¸ ë°œìƒ - {e}")
#         return None

# === UNUSED: DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ í•¨ìˆ˜ ===
# def validate_and_fix_db_schema_alignment():
#     """
#     ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì™€ get_safe_ohlcv_columns() ê²°ê³¼ì˜ ì¼ì¹˜ì„±ì„ ê²€ì¦í•˜ê³  ìˆ˜ì •
#     
#     Returns:
#         dict: ê²€ì¦ ê²°ê³¼ {'valid': bool, 'issues': list, 'corrected_mapping': dict}
#     """
#     try:
#         # ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
#         actual_columns = get_safe_ohlcv_columns()
#         
#         # ì½”ë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆìƒ ì»¬ëŸ¼ë“¤
#         expected_core_columns = [
#             'ticker', 'date', 'open', 'high', 'low', 'close', 'volume',  # ê¸°ë³¸ OHLCV
#             'pivot', 'r1', 's1', 'support', 'resistance',  # í”¼ë´‡ í¬ì¸íŠ¸
#             'supertrend', 'macd_hist', 'rsi_14', 'adx', 'atr',  # ê¸°ìˆ  ì§€í‘œ
#             'bb_upper', 'bb_lower', 'volume_20ma', 'ht_trendline',  # ì¶”ê°€ ì§€í‘œ
#             'ma_50', 'ma_200', 'donchian_high', 'donchian_low',  # ì´ë™í‰ê·  ë° ëˆì¹˜ì•ˆ
#             'macd', 'macd_signal', 'macd_histogram', 'plus_di', 'minus_di'  # MACD ë° ADX
#         ]
#         
#         # í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼ë“¤
#         fibo_columns = get_safe_fibo_columns()
#         
#         issues = []
#         missing_columns = []
#         extra_columns = []
#         
#         # ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
#         for col in expected_core_columns:
#             if col not in actual_columns:
#                 missing_columns.append(col)
#                 issues.append(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
#         
#         # ì˜ˆìƒ ì™¸ ì¶”ê°€ ì»¬ëŸ¼ í™•ì¸
#         for col in actual_columns:
#             if col not in expected_core_columns and not col.startswith('fibo_'):
#                 extra_columns.append(col)
#         
#         # ê²€ì¦ ê²°ê³¼
#         is_valid = len(missing_columns) == 0
#         
#         logger.info(f"ğŸ” DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼:")
#         logger.info(f"  - ì‹¤ì œ ì»¬ëŸ¼ ìˆ˜: {len(actual_columns)}")
#         logger.info(f"  - ì˜ˆìƒ í•„ìˆ˜ ì»¬ëŸ¼: {len(expected_core_columns)}")
#         logger.info(f"  - í”¼ë³´ë‚˜ì¹˜ ì»¬ëŸ¼: {len(fibo_columns)}")
#         logger.info(f"  - ëˆ„ë½ ì»¬ëŸ¼: {len(missing_columns)}")
#         logger.info(f"  - ì¶”ê°€ ì»¬ëŸ¼: {len(extra_columns)}")
#         
#         if missing_columns:
#             logger.warning(f"âš ï¸ ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼ë“¤: {missing_columns}")
#         
#         if extra_columns:
#             logger.info(f"â„¹ï¸ ì˜ˆìƒ ì™¸ ì¶”ê°€ ì»¬ëŸ¼ë“¤: {extra_columns[:10]}{'...' if len(extra_columns) > 10 else ''}")
#         
#         # ìˆ˜ì •ëœ ë§¤í•‘ ìƒì„±
#         corrected_mapping = {
#             'available_core_columns': [col for col in expected_core_columns if col in actual_columns],
#             'available_fibo_columns': fibo_columns,
#             'missing_columns': missing_columns,
#             'extra_columns': extra_columns,
#             'total_available': len(actual_columns)
#         }
#         
#         return {
#             'valid': is_valid,
#             'issues': issues,
#             'corrected_mapping': corrected_mapping
#         }
#         
#     except Exception as e:
#         logger.error(f"âŒ DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
#         return {
#             'valid': False,
#             'issues': [f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}"],
#             'corrected_mapping': {}
#         }

# === UNUSED: DataFrame ì¸ë±ìŠ¤ ì²˜ë¦¬ í•¨ìˆ˜ ===
# def standardize_dataframe_index_processing(df, ticker=""):
#     """
#     pandas DataFrameì˜ ì¸ë±ìŠ¤ë¥¼ í‘œì¤€í™”ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
#     - ëª¨ë“  ë‚ ì§œ ì¸ë±ìŠ¤ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
#     - JSON ì§ë ¬í™” í˜¸í™˜ì„± ë³´ì¥
#     
#     Args:
#         df: pandas DataFrame
#         ticker: í‹°ì»¤ëª… (ë¡œê¹…ìš©)
#         
#     Returns:
#         pandas.DataFrame: í‘œì¤€í™”ëœ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ DataFrame
#     """
#     if df is None or df.empty:
#         return df
#     
#     try:
#         logger.debug(f"ğŸ”§ {ticker}: DataFrame ì¸ë±ìŠ¤ í‘œì¤€í™” ì‹œì‘")
#         original_index_type = type(df.index).__name__
#         
#         # í˜„ì¬ ì¸ë±ìŠ¤ íƒ€ì… í™•ì¸
#         if hasattr(df.index, 'dtype') and 'datetime' in str(df.index.dtype):
#             logger.debug(f"ğŸ”§ {ticker}: datetime ì¸ë±ìŠ¤ ê°ì§€ - {original_index_type}")
#             
#             # ì¸ë±ìŠ¤ë¥¼ í‘œì¤€í™”ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
#             new_index = []
#             for i, idx_val in enumerate(df.index):
#                 converted_date = unified_date_conversion_for_json(
#                     idx_val, ticker, i, len(df.index)
#                 )
#                 if converted_date:
#                     new_index.append(converted_date)
#                 else:
#                     # fallback: ê¸°ë³¸ í¬ë§· ì‚¬ìš©
#                     fallback_date = safe_strftime(idx_val, '%Y-%m-%d')
#                     new_index.append(fallback_date)
#             
#             df.index = new_index
#             logger.debug(f"âœ… {ticker}: ì¸ë±ìŠ¤ í‘œì¤€í™” ì™„ë£Œ - {original_index_type} â†’ ë¬¸ìì—´")
#             
#         # pandas Timestampê°€ ì„ì—¬ìˆëŠ” ê²½ìš° ì²˜ë¦¬
#         elif hasattr(df.index, '__iter__'):
#             needs_conversion = False
#             for idx_val in df.index[:5]:  # ìƒ˜í”Œë§ìœ¼ë¡œ í™•ì¸
#                 if hasattr(idx_val, '__class__') and any(keyword in str(type(idx_val)) for keyword in ['pandas', 'datetime', 'Timestamp']):
#                     needs_conversion = True
#                     break
#             
#             if needs_conversion:
#                 logger.debug(f"ğŸ”§ {ticker}: í˜¼í•© ì¸ë±ìŠ¤ íƒ€ì… ê°ì§€, í†µì¼ ì‘ì—… ì‹œì‘")
#                 
#                 new_index = []
#                 for i, idx_val in enumerate(df.index):
#                     if hasattr(idx_val, 'strftime') or isinstance(idx_val, (datetime, date)):
#                         converted_date = unified_date_conversion_for_json(
#                             idx_val, ticker, i, len(df.index)
#                         )
#                         new_index.append(converted_date if converted_date else str(idx_val))
#                     else:
#                         new_index.append(str(idx_val))
#                 
#                 df.index = new_index
#                 logger.debug(f"âœ… {ticker}: í˜¼í•© ì¸ë±ìŠ¤ í†µì¼ ì™„ë£Œ")
#         
#         # ìµœì¢… JSON í˜¸í™˜ì„± ê²€ì¦
#         try:
#             import json
#             json.dumps(df.index.tolist(), ensure_ascii=False)
#             logger.debug(f"âœ… {ticker}: ì¸ë±ìŠ¤ JSON í˜¸í™˜ì„± ê²€ì¦ í†µê³¼")
#         except Exception as json_error:
#             logger.warning(f"âš ï¸ {ticker}: ì¸ë±ìŠ¤ JSON í˜¸í™˜ì„± ë¬¸ì œ - {json_error}")
#             # ê°•ì œë¡œ ë¬¸ìì—´ ë³€í™˜
#             df.index = [str(idx) for idx in df.index]
#         
#         return df
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker}: DataFrame ì¸ë±ìŠ¤ í‘œì¤€í™” ì‹¤íŒ¨ - {e}")
#         return df

# === UNUSED: ì¢…í•© ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ ===
# def comprehensive_data_validation_and_fix(data, data_type="unknown", context=""):
#     """
#     í¬ê´„ì ì¸ ë°ì´í„° ê²€ì¦ ë° ìˆ˜ì • - ëª¨ë“  ë°ì´í„° íƒ€ì…ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
#     
#     Args:
#         data: ê²€ì¦í•  ë°ì´í„° (DataFrame, dict, list, ê¸°ë³¸ íƒ€ì…)
#         data_type: ë°ì´í„° íƒ€ì… íŒíŠ¸
#         context: ë¡œê¹… ì»¨í…ìŠ¤íŠ¸
#         
#     Returns:
#         ê²€ì¦ë˜ê³  ìˆ˜ì •ëœ ë°ì´í„°
#     """
#     monitor = get_global_monitor()
#     
#     try:
#         if data is None:
#             return None
#         
#         # DataFrame ì²˜ë¦¬
#         if isinstance(data, pd.DataFrame):
#             logger.debug(f"ğŸ”§ {context}: DataFrame ì¢…í•© ê²€ì¦ ì‹œì‘")
#             
#             # 1. ì¸ë±ìŠ¤ í‘œì¤€í™”
#             data = standardize_dataframe_index_processing(data, context)
#             
#             # 2. JSON í˜¸í™˜ì„±ì„ ìœ„í•œ ë°ì´í„° íƒ€ì… ìˆ˜ì •
#             data = validate_dataframe_index_for_json(data)
#             
#             # 3. ì»¬ëŸ¼ ê°’ë“¤ì˜ JSON í˜¸í™˜ì„± ê²€ì¦
#             for col in data.columns:
#                 try:
#                     # ìƒ˜í”Œë§ìœ¼ë¡œ ì»¬ëŸ¼ íƒ€ì… í™•ì¸
#                     sample_values = data[col].dropna().head(5)
#                     for val in sample_values:
#                         if isinstance(val, (datetime, date)) or (hasattr(val, '__class__') and 'pandas' in str(type(val))):
#                             logger.debug(f"ğŸ”§ {context}: {col} ì»¬ëŸ¼ì˜ ë¹„í˜¸í™˜ íƒ€ì… ê°ì§€")
#                             # ì „ì²´ ì»¬ëŸ¼ì„ ì•ˆì „í•œ í˜•íƒœë¡œ ë³€í™˜
#                             data[col] = data[col].apply(
#                                 lambda x: enhanced_safe_strftime_for_json(x) 
#                                 if hasattr(x, 'strftime') or isinstance(x, (datetime, date))
#                                 else x
#                             )
#                             break
#                 except Exception as col_error:
#                     logger.warning(f"âš ï¸ {context}: {col} ì»¬ëŸ¼ ê²€ì¦ ì‹¤íŒ¨ - {col_error}")
#             
#             return data
#         
#         # Dictionary ì²˜ë¦¬
#         elif isinstance(data, dict):
#             return safe_json_structure_with_fallback(data, context)
#         
#         # List ì²˜ë¦¬
#         elif isinstance(data, (list, tuple)):
#             corrected_list = []
#             for i, item in enumerate(data):
#                 item_context = f"{context}[{i}]"
#                 corrected_item = comprehensive_data_validation_and_fix(item, "list_item", item_context)
#                 corrected_list.append(corrected_item)
#             return corrected_list
#         
#         # ê¸°ë³¸ íƒ€ì… ì²˜ë¦¬
#         else:
#             validation_result = validate_json_compatibility(data, context)
#             return validation_result['corrected_data']
#     
#     except Exception as e:
#         logger.error(f"âŒ {context}: ì¢…í•© ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ - {e}")
#         monitor.log_conversion_failure(context, "validation", data, str(e), "Error")
#         return "ValidationError"

# === UNUSED: DB ìŠ¤í‚¤ë§ˆ ë° ì—°ê²° ê²€ì¦ í•¨ìˆ˜ ===
# def validate_db_schema_and_connection():
#     """
#     DB ì—°ê²° ë° ìŠ¤í‚¤ë§ˆ ìƒíƒœ ê²€ì¦ í•¨ìˆ˜
#     
#     ê²€ì¦ í•­ëª©:
#     1. .env íŒŒì¼ì˜ DB ì—°ê²° ì •ë³´ í™•ì¸
#     2. ohlcv í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ
#     3. static_indicators í…Œì´ë¸”ì˜ ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ
#     4. pivot, r1, s1 ë“±ì´ ì–´ëŠ í…Œì´ë¸”ì— ìˆëŠ”ì§€ í™•ì¸
#     5. ê²€ì¦ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ë¡œê¹…
#     
#     ë°˜í™˜ê°’: 
#     {
#         'ohlcv_columns': [...],
#         'static_columns': [...],
#         'missing_columns': [...],
#         'status': 'success/error'
#     }
#     """
#     validation_result = {
#         'ohlcv_columns': [],
#         'static_columns': [],
#         'missing_columns': [],
#         'status': 'error',
#         'env_vars': {},
#         'pivot_locations': {}
#     }
#     
#     try:
#         logger.info("ğŸ” DB ì—°ê²° ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹œì‘")
#         
#         # 1. í™˜ê²½ ë³€ìˆ˜ í™•ì¸
#         logger.info("ğŸ“‹ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦:")
#         required_env_vars = ['PG_HOST', 'PG_PORT', 'PG_DATABASE', 'PG_USER', 'PG_PASSWORD']
#         for var in required_env_vars:
#             value = os.getenv(var)
#             if value:
#                 validation_result['env_vars'][var] = "âœ… ì„¤ì •ë¨" if var != 'PG_PASSWORD' else "âœ… ì„¤ì •ë¨ (ë¹„ë°€ë²ˆí˜¸)"
#                 logger.info(f"  - {var}: âœ… ì„¤ì •ë¨")
#             else:
#                 validation_result['env_vars'][var] = "âŒ ëˆ„ë½"
#                 logger.error(f"  - {var}: âŒ ëˆ„ë½")
#         
#         # 2. DB ì—°ê²° í…ŒìŠ¤íŠ¸
#         logger.info("ğŸ”Œ DB ì—°ê²° í…ŒìŠ¤íŠ¸:")
#         conn = get_db_connection()
#         cursor = conn.cursor()
#         
#         # 3. ohlcv í…Œì´ë¸” ì»¬ëŸ¼ ì¡°íšŒ
#         logger.info("ğŸ” ohlcv í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ:")
#         cursor.execute("""
#             SELECT column_name, data_type, is_nullable
#             FROM information_schema.columns 
#             WHERE table_name = 'ohlcv' 
#             ORDER BY ordinal_position
#         """)
#         ohlcv_columns_info = cursor.fetchall()
#         validation_result['ohlcv_columns'] = [col[0] for col in ohlcv_columns_info]
#         
#         logger.info(f"  - ì´ {len(ohlcv_columns_info)}ê°œ ì»¬ëŸ¼ ë°œê²¬")
#         for col_name, data_type, nullable in ohlcv_columns_info[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
#             logger.info(f"    {col_name} ({data_type})")
#         if len(ohlcv_columns_info) > 10:
#             logger.info(f"    ... ì™¸ {len(ohlcv_columns_info) - 10}ê°œ ì»¬ëŸ¼")
#         
#         # 4. static_indicators í…Œì´ë¸” ì»¬ëŸ¼ ì¡°íšŒ
#         logger.info("ğŸ” static_indicators í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ:")
#         cursor.execute("""
#             SELECT column_name, data_type, is_nullable
#             FROM information_schema.columns 
#             WHERE table_name = 'static_indicators' 
#             ORDER BY ordinal_position
#         """)
#         static_columns_info = cursor.fetchall()
#         validation_result['static_columns'] = [col[0] for col in static_columns_info]
#         
#         logger.info(f"  - ì´ {len(static_columns_info)}ê°œ ì»¬ëŸ¼ ë°œê²¬")
#         for col_name, data_type, nullable in static_columns_info:
#             logger.info(f"    {col_name} ({data_type})")
#         
#         # 5. í•µì‹¬ ì§€í‘œ ì»¬ëŸ¼ë“¤ì˜ ìœ„ì¹˜ í™•ì¸
#         logger.info("ğŸ¯ í•µì‹¬ ì§€í‘œ ì»¬ëŸ¼ ìœ„ì¹˜ í™•ì¸:")
#         critical_columns = ['pivot', 'r1', 's1', 'support', 'resistance', 'atr', 'adx']
#         
#         for col in critical_columns:
#             locations = []
#             if col in validation_result['ohlcv_columns']:
#                 locations.append('ohlcv')
#             if col in validation_result['static_columns']:
#                 locations.append('static_indicators')
#             
#             validation_result['pivot_locations'][col] = locations
#             
#             if locations:
#                 location_str = ', '.join(locations)
#                 logger.info(f"  - {col}: {location_str}")
#             else:
#                 logger.warning(f"  - {col}: âŒ ì–´ëŠ í…Œì´ë¸”ì—ë„ ì—†ìŒ")
#                 validation_result['missing_columns'].append(col)
#         
#         # 6. ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
#         logger.info("ğŸ“Š ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸:")
#         
#         # ohlcv í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜
#         cursor.execute("SELECT COUNT(*) FROM ohlcv")
#         ohlcv_count = cursor.fetchone()[0]
#         logger.info(f"  - ohlcv í…Œì´ë¸”: {ohlcv_count:,}ê°œ ë ˆì½”ë“œ")
#         
#         # static_indicators í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜
#         cursor.execute("SELECT COUNT(*) FROM static_indicators")
#         static_count = cursor.fetchone()[0]
#         logger.info(f"  - static_indicators í…Œì´ë¸”: {static_count:,}ê°œ ë ˆì½”ë“œ")
#         
#         # ìµœê·¼ ë°ì´í„° í™•ì¸
#         cursor.execute("SELECT MAX(date) FROM ohlcv")
#         latest_ohlcv_date = cursor.fetchone()[0]
#         if latest_ohlcv_date:
#             logger.info(f"  - ohlcv ìµœì‹  ë°ì´í„°: {latest_ohlcv_date}")
#         
#         cursor.close()
#         conn.close()
#         
#         validation_result['status'] = 'success'
#         logger.info("âœ… DB ì—°ê²° ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ")
#         
#         return validation_result
#         
#     except Exception as e:
#         logger.error(f"âŒ DB ì—°ê²° ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
#         validation_result['status'] = 'error'
#         validation_result['error'] = str(e)
#         return validation_result

# === UNUSED: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ ===
# def monitor_data_processing_performance(
#     ticker: str,
#     current_progress: int,
#     total_records: int,
#     start_time: float,
#     operation_name: str = "ë°ì´í„° ì²˜ë¦¬",
#     log_level: str = "INFO"
# ) -> dict:
#     """
#     ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” í•¨ìˆ˜
#     
#     Args:
#         ticker: ì²˜ë¦¬ ì¤‘ì¸ í‹°ì»¤
#         current_progress: í˜„ì¬ ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜
#         total_records: ì „ì²´ ë ˆì½”ë“œ ìˆ˜
#         start_time: ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ (time.time())
#         operation_name: ì‘ì—… ì´ë¦„
#         log_level: ë¡œê·¸ ë ˆë²¨ (INFO, DEBUG)
#     
#     Returns:
#         dict: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
#         {
#             'progress_percentage': float,
#             'elapsed_time': float,
#             'estimated_remaining_time': float,
#             'processing_speed_per_sec': float,
#             'memory_usage_mb': float,
#             'eta_formatted': str
#         }
#     """
#     try:
#         current_time = time.time()
#         elapsed_time = current_time - start_time
#         
#         # ì§„í–‰ë¥  ê³„ì‚°
#         progress_percentage = (current_progress / total_records) * 100 if total_records > 0 else 0
#         
#         # ì²˜ë¦¬ ì†ë„ ê³„ì‚° (records/sec)
#         processing_speed = current_progress / elapsed_time if elapsed_time > 0 else 0
#         
#         # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
#         if processing_speed > 0:
#             remaining_records = total_records - current_progress
#             estimated_remaining_time = remaining_records / processing_speed
#         else:
#             estimated_remaining_time = 0
#         
#         # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
#         try:
#             process = psutil.Process()
#             memory_usage_mb = process.memory_info().rss / 1024 / 1024
#         except:
#             memory_usage_mb = 0
#         
#         # ETA í¬ë§·íŒ…
#         eta_minutes = int(estimated_remaining_time // 60)
#         eta_seconds = int(estimated_remaining_time % 60)
#         eta_formatted = f"{eta_minutes}ë¶„ {eta_seconds}ì´ˆ" if eta_minutes > 0 else f"{eta_seconds}ì´ˆ"
#         
#         # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
#         performance_metrics = {
#             'progress_percentage': round(progress_percentage, 1),
#             'elapsed_time': round(elapsed_time, 1),
#             'estimated_remaining_time': round(estimated_remaining_time, 1),
#             'processing_speed_per_sec': round(processing_speed, 2),
#             'memory_usage_mb': round(memory_usage_mb, 1),
#             'eta_formatted': eta_formatted
#         }
#         
#         # ë¡œê¹…
#         logger = logging.getLogger(__name__)
#         
#         if log_level == "INFO":
#             logger.info(
#                 f"ğŸ”„ {ticker} {operation_name}: "
#                 f"{current_progress:,}/{total_records:,} ({progress_percentage:.1f}%) | "
#                 f"ì†ë„: {processing_speed:.1f} records/sec | "
#                 f"ETA: {eta_formatted} | "
#                 f"ë©”ëª¨ë¦¬: {memory_usage_mb:.1f}MB"
#             )
#         elif log_level == "DEBUG":
#             logger.debug(
#                 f"ğŸ“Š {ticker} ìƒì„¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­:\n"
#                 f"   - ì§„í–‰ë¥ : {progress_percentage:.1f}% ({current_progress:,}/{total_records:,})\n"
#                 f"   - ê²½ê³¼ ì‹œê°„: {elapsed_time:.1f}ì´ˆ\n"
#                 f"   - ì²˜ë¦¬ ì†ë„: {processing_speed:.2f} records/sec\n"
#                 f"   - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time:.1f}ì´ˆ ({eta_formatted})\n"
#                 f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage_mb:.1f}MB"
#             )
#         
#         return performance_metrics
#         
#     except Exception as e:
#         logger = logging.getLogger(__name__)
#         logger.error(f"âŒ {ticker} ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")
#         
#         # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë°˜í™˜
#         return {
#             'progress_percentage': 0,
#             'elapsed_time': 0,
#             'estimated_remaining_time': 0,
#             'processing_speed_per_sec': 0,
#             'memory_usage_mb': 0,
#             'eta_formatted': "ì•Œ ìˆ˜ ì—†ìŒ"
#         }

def validate_db_schema_and_indicators():
    """
    ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì™€ ì§€í‘œ ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    
    ì‘ì—… ë‚´ìš©:
    1. .env íŒŒì¼ì˜ DB ì—°ê²° ì •ë³´ í™•ì¸
    2. ohlcv í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ì¡°íšŒí•˜ì—¬ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ë§Œ í™•ì¸
    3. static_indicators í…Œì´ë¸”ì— ìˆëŠ” ì •ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ í™•ì¸
    4. ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œì„ ëŒ€ìƒìœ¼ë¡œ NULL ê°’ ë¹„ìœ¨ í™•ì¸
    5. ê²€ì¦ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ë¡œê¹…
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ ì •ë³´
    """
    logger = setup_logger()
    
    try:
        logger.info("ğŸ” DB ìŠ¤í‚¤ë§ˆ ë° ì§€í‘œ êµ¬ì¡° ê²€ì¦ ì‹œì‘")
        
        # 1. DB ì—°ê²° ì •ë³´ í™•ì¸
        required_env_vars = ['PG_HOST', 'PG_PORT', 'PG_DATABASE', 'PG_USER', 'PG_PASSWORD']
        missing_vars = []
        
        for var in required_env_vars:
            value = os.getenv(var)
            if not value:
                missing_vars.append(var)
            else:
                # ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹
                if var == 'PG_PASSWORD':
                    logger.info(f"âœ… {var}: {'*' * len(value)}")
                else:
                    logger.info(f"âœ… {var}: {value}")
        
        if missing_vars:
            logger.error(f"âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {missing_vars}")
            return {'status': 'error', 'error': f'Missing environment variables: {missing_vars}'}
        
        # 2. DB ì—°ê²°
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # 3. ohlcv í…Œì´ë¸” ì‹¤ì œ ì»¬ëŸ¼ ì¡°íšŒ
            logger.info("ğŸ” ohlcv í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ ì¤‘...")
            cursor.execute("""
                SELECT column_name, data_type, is_nullable 
                FROM information_schema.columns 
                WHERE table_name = 'ohlcv' 
                AND table_schema = 'public'
                ORDER BY ordinal_position
            """)
            
            ohlcv_columns_info = cursor.fetchall()
            actual_ohlcv_columns = []
            
            for col_name, data_type, is_nullable in ohlcv_columns_info:
                actual_ohlcv_columns.append(col_name)
                null_status = "NOT NULL" if is_nullable == "NO" else "NULL"
                logger.info(f"   ğŸ“Š {col_name}: {data_type} ({null_status})")
            
            # 4. static_indicators í…Œì´ë¸” ì‹¤ì œ ì»¬ëŸ¼ ì¡°íšŒ
            logger.info("ğŸ” static_indicators í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ ì¤‘...")
            cursor.execute("""
                SELECT column_name, data_type, is_nullable 
                FROM information_schema.columns 
                WHERE table_name = 'static_indicators' 
                AND table_schema = 'public'
                ORDER BY ordinal_position
            """)
            
            static_columns_info = cursor.fetchall()
            actual_static_columns = []
            
            for col_name, data_type, is_nullable in static_columns_info:
                actual_static_columns.append(col_name)
                null_status = "NOT NULL" if is_nullable == "NO" else "NULL"
                logger.info(f"   ğŸ“Š {col_name}: {data_type} ({null_status})")
            
            # 5. ë™ì  ì§€í‘œ ì»¬ëŸ¼ ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ í•„í„°ë§
            expected_dynamic_indicators = [
                'fibo_618', 'fibo_382', 'ht_trendline', 'ma_50', 'ma_200', 
                'bb_upper', 'bb_lower', 'donchian_high', 'donchian_low', 
                'macd_histogram', 'rsi_14', 'volume_20ma', 'stoch_k', 'stoch_d', 'cci'
            ]
            
            existing_dynamic_indicators = [col for col in expected_dynamic_indicators if col in actual_ohlcv_columns]
            missing_dynamic_indicators = [col for col in expected_dynamic_indicators if col not in actual_ohlcv_columns]
            
            logger.info(f"ğŸ“Š ë™ì  ì§€í‘œ í˜„í™©:")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ: {len(existing_dynamic_indicators)}ê°œ")
            for col in existing_dynamic_indicators:
                logger.info(f"     âœ… {col}")
            
            if missing_dynamic_indicators:
                logger.warning(f"   â€¢ ëˆ„ë½ëœ ë™ì  ì§€í‘œ: {len(missing_dynamic_indicators)}ê°œ")
                for col in missing_dynamic_indicators:
                    logger.warning(f"     âŒ {col}")
            
            # 6. ê° ì§€í‘œì˜ NULL ë¹„ìœ¨ í™•ì¸ (KRW-BTC ê¸°ì¤€)
            logger.info("ğŸ” ë™ì  ì§€í‘œ NULL ë¹„ìœ¨ ê²€ì¦ ì¤‘...")
            null_analysis = {}
            
            for indicator in existing_dynamic_indicators:
                try:
                    cursor.execute(f"""
                        SELECT 
                            COUNT(*) as total, 
                            COUNT({indicator}) as non_null,
                            COUNT(*) - COUNT({indicator}) as null_count
                        FROM ohlcv 
                        WHERE ticker = 'KRW-BTC'
                        AND date >= CURRENT_DATE - INTERVAL '30 days'
                    """)
                    
                    result = cursor.fetchone()
                    if result and result[0] > 0:
                        total, non_null, null_count = result
                        null_ratio = (null_count / total) * 100 if total > 0 else 100
                        
                        null_analysis[indicator] = {
                            'total': total,
                            'non_null': non_null, 
                            'null_count': null_count,
                            'null_ratio': null_ratio
                        }
                        
                        if null_ratio > 80:
                            logger.warning(f"  âš ï¸ {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                        elif null_ratio > 50:
                            logger.info(f"  ğŸ”¶ {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                        else:
                            logger.info(f"  âœ… {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                    else:
                        logger.warning(f"  âš ï¸ {indicator}: ë°ì´í„° ì—†ìŒ")
                        null_analysis[indicator] = {'error': 'no_data'}
                        
                except Exception as e:
                    logger.error(f"  âŒ {indicator} NULL ë¹„ìœ¨ í™•ì¸ ì‹¤íŒ¨: {e}")
                    null_analysis[indicator] = {'error': str(e)}
            
            # 7. static_indicators ì»¬ëŸ¼ ê²€ì¦
            expected_static_indicators = [
                'ma200_slope', 'nvt_relative', 'volume_change_7_30', 'price', 
                'high_60', 'low_60', 'pivot', 's1', 'r1', 'resistance', 
                'support', 'atr', 'adx', 'supertrend_signal'
            ]
            
            existing_static_indicators = [col for col in expected_static_indicators if col in actual_static_columns]
            missing_static_indicators = [col for col in expected_static_indicators if col not in actual_static_columns]
            
            logger.info(f"ğŸ“Š ì •ì  ì§€í‘œ í˜„í™©:")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ì •ì  ì§€í‘œ: {len(existing_static_indicators)}ê°œ")
            for col in existing_static_indicators:
                logger.info(f"     âœ… {col}")
            
            if missing_static_indicators:
                logger.warning(f"   â€¢ ëˆ„ë½ëœ ì •ì  ì§€í‘œ: {len(missing_static_indicators)}ê°œ")
                for col in missing_static_indicators:
                    logger.warning(f"     âŒ {col}")
            
            # 8. ê²€ì¦ ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ“Š DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"   â€¢ ohlcv í…Œì´ë¸” ì»¬ëŸ¼ ìˆ˜: {len(actual_ohlcv_columns)}")
            logger.info(f"   â€¢ static_indicators í…Œì´ë¸” ì»¬ëŸ¼ ìˆ˜: {len(actual_static_columns)}")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ: {len(existing_dynamic_indicators)}/{len(expected_dynamic_indicators)}")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ì •ì  ì§€í‘œ: {len(existing_static_indicators)}/{len(expected_static_indicators)}")
            
            total_missing = len(missing_dynamic_indicators) + len(missing_static_indicators)
            if total_missing > 0:
                logger.warning(f"   â€¢ ì´ ëˆ„ë½ ì»¬ëŸ¼ ìˆ˜: {total_missing}")
            
            return {
                'status': 'success',
                'ohlcv_columns': actual_ohlcv_columns,
                'static_columns': actual_static_columns,
                'existing_dynamic_indicators': existing_dynamic_indicators,
                'missing_dynamic_indicators': missing_dynamic_indicators,
                'existing_static_indicators': existing_static_indicators,
                'missing_static_indicators': missing_static_indicators,
                'null_analysis': null_analysis,
                'total_missing_columns': total_missing
            }
            
        finally:
            cursor.close()
            conn.close()
            
    except Exception as e:
        logger.error(f"âŒ DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {'status': 'error', 'error': str(e)}